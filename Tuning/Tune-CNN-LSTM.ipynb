{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ahUZuAfz8Jje"
   },
   "source": [
    "# Sklearn Tuner for CNN - LSTM Activity Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Re0ecmVI8Jjk"
   },
   "source": [
    "In this notebook, we use SKlearn  to tune a CNN-LSTM neural net to classify PE activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ixsGb9tY8Jjm"
   },
   "source": [
    "#### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5At1PKQp8Jjp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 11633662699428622942\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 4993672144390965880\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4022219571\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 10180279242237958329\n",
      "physical_device_desc: \"device: 0, name: Quadro P2000, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 13379853376503830196\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n",
      "# GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv1D, GlobalMaxPooling1D, LSTM\n",
    "from tensorflow.keras.layers import AveragePooling1D, LeakyReLU , MaxPool1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint \n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "import tensorflow as tf\n",
    "print(\"# GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "import sklearn\n",
    "# bug in sklearn wrapper\n",
    "#https://stackoverflow.com/questions/59746974/cannot-clone-object-tensorflow-python-keras-wrappers-scikit-learn-kerasclassifi\n",
    "if sklearn.__version__ != '0.21.2':\n",
    "    print(\"updating sklearn ...\")\n",
    "    !pip install --user scikit-learn==0.21.2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hg5wX2Dd8Jjz"
   },
   "source": [
    "#### Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lYLOM99-8Jj2"
   },
   "outputs": [],
   "source": [
    "# output directory name:\n",
    "output_dir = 'model_output/tune-cnn-lstm'\n",
    "input_dir =  'Z:/Research/dfuller/Walkabilly/studies/smarphone_accel/data/Ethica_Jaeger_Merged/pocket/'\n",
    "input_file_name = 'pocket-NN-data.npz'\n",
    "\n",
    "# from the data preparation section we have:\n",
    "window_size_second = 3\n",
    "frequency = 30\n",
    "lenght_of_each_seq = window_size_second * frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn hyperparams\n",
    "params = {\n",
    "    \n",
    "    # Conv layers\n",
    "    'n_conv_1':[512, 768], # filters, a.k.a. kernels\n",
    "    'k_conv_1':[2, 3], # kernel length\n",
    "    'n_conv_2':[256, 512], # filters, a.k.a. kernels\n",
    "    'k_conv_2':[2, 3], # kernel length\n",
    "    'n_conv_3':[256, 512], # filters, a.k.a. kernels\n",
    "    'k_conv_3':[2, 3], # kernel length\n",
    "    'maxpooling_pool_size':[2, 3],\n",
    "    'avepooling_pool_size':[2, 3],\n",
    "    \n",
    "    # LSTM layers\n",
    "    'n_lstm_1' : [128, 256],\n",
    "    'n_lstm_2' : [128, 256],\n",
    "    'drop_lstm_1' : [0.02,0.1],\n",
    "    'drop_lstm_2' : [0.02,0.1],\n",
    "\n",
    "    \n",
    "    # Dense layers\n",
    "    'n_dense_1':[256, 384, 512],\n",
    "    'dropout_1':[0.2, 0.3],\n",
    "    'n_dense_2':[256, 384, 512],\n",
    "    'dropout_2':[0.2, 0.3],\n",
    "    'activation_conv':['elu', 'relu', LeakyReLU()],\n",
    "    'activation_dense':['elu', 'relu', LeakyReLU()]\n",
    "}\n",
    "\n",
    "# training:\n",
    "n_tune_iter = 20\n",
    "cv = 3\n",
    "epochs = 60\n",
    "batch_size = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to get reproducable results\n",
    "from numpy.random import seed\n",
    "seed(85)\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DaLx4yZ48Jj9"
   },
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CD9X0yJg8Jj_"
   },
   "source": [
    "##### For this notebook we use the acceleration data gathered from the pocket location. It was prepared in the DataPrep-Deep notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6r44JKj-8JkA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acceleration_data\n",
      "metadata\n",
      "labels\n"
     ]
    }
   ],
   "source": [
    "# read the raw file and get the keys:\n",
    "raw_data = np.load(file=input_dir+input_file_name,allow_pickle=True)\n",
    "for k in raw_data.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "\n",
    "accel_array = raw_data['acceleration_data']\n",
    "meta_array = raw_data['metadata']\n",
    "labels_array = raw_data['labels']\n",
    "input_shape = list(accel_array.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WWq3ipbu8JlG"
   },
   "source": [
    "#### Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the  labels to integer.\n",
    "In the raw data format of the labels is String and there are 6 classes. 'Lying', 'Sitting', 'Self Pace walk', 'Running 3 METs',\n",
    "       'Running 5 METs', 'Running 7 METs' <br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LyReiequ8Jln"
   },
   "outputs": [],
   "source": [
    "n_class = len(np.unique(labels_array))\n",
    "class_list, labels_array_int = np.unique(labels_array,return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels_array_int, num_classes=n_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "40x3s0KY8Jlv",
    "outputId": "d3ac3e31-3c47-4b01-c2f9-ada0684b31b7"
   },
   "source": [
    "### Splitting and shuffeling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f5grc2H78Jlz",
    "outputId": "3f7eeb55-f851-4796-de4e-0c68eb9df88d"
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "     accel_array, y, test_size=0.1, random_state=65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "GscSnCpk8Jlm"
   },
   "source": [
    "\n",
    "#### Design neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_conv_1': [512, 768],\n",
       " 'k_conv_1': [2, 3],\n",
       " 'n_conv_2': [256, 512],\n",
       " 'k_conv_2': [2, 3],\n",
       " 'n_conv_3': [256, 512],\n",
       " 'k_conv_3': [2, 3],\n",
       " 'maxpooling_pool_size': [2, 3],\n",
       " 'avepooling_pool_size': [2, 3],\n",
       " 'n_lstm_1': [128, 256],\n",
       " 'n_lstm_2': [128, 256],\n",
       " 'drop_lstm_1': [0.02, 0.1],\n",
       " 'drop_lstm_2': [0.02, 0.1],\n",
       " 'n_dense_1': [256, 384, 512],\n",
       " 'dropout_1': [0.2, 0.3],\n",
       " 'n_dense_2': [256, 384, 512],\n",
       " 'dropout_2': [0.2, 0.3],\n",
       " 'activation_conv': ['elu',\n",
       "  'relu',\n",
       "  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU at 0x2527e6dd448>],\n",
       " 'activation_dense': ['elu',\n",
       "  'relu',\n",
       "  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU at 0x252062f3d48>]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_conv_1=256, k_conv_1=3, n_conv_2=256, k_conv_2=3, n_conv_3=256, k_conv_3=3,\n",
    "                 maxpooling_pool_size = 2, avepooling_pool_size = 2, n_lstm_1=128, n_lstm_2=128,\n",
    "                 drop_lstm_1=0.02,drop_lstm_2=0.02, n_dense_1=256, dropout_1=0.2,\n",
    "                 n_dense_2=256, dropout_2=0.2, activation_conv= 'relu', activation_dense= 'elu'\n",
    "                ):\n",
    "    model = Sequential()\n",
    "    # Conv layers\n",
    "    model.add(Conv1D(n_conv_1, k_conv_1, activation=activation_conv, input_shape=input_shape[1:]))\n",
    "    model.add(MaxPool1D(pool_size = maxpooling_pool_size))\n",
    "    model.add(Conv1D(n_conv_2, k_conv_2, activation=activation_conv))\n",
    "    model.add(AveragePooling1D(pool_size = avepooling_pool_size))\n",
    "    model.add(Conv1D(n_conv_3, k_conv_3, activation=activation_conv))\n",
    "#     model.add(GlobalAveragePooling1D())\n",
    "\n",
    "    # LSTM layers\n",
    "    model.add(LSTM(n_lstm_1, dropout=drop_lstm_1, \n",
    "                             return_sequences=True)) \n",
    "    model.add(LSTM(n_lstm_2, dropout=drop_lstm_2, \n",
    "                             return_sequences=True))\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(n_dense_1, activation=activation_dense))\n",
    "    model.add(Dropout(dropout_1))\n",
    "    model.add(Dense(n_dense_2, activation=activation_dense))\n",
    "    model.add(Dropout(dropout_2))\n",
    "    model.add(Dense(n_class, activation='softmax'))\n",
    "    model.summary()\n",
    "    print(f\"\"\"n_conv_1 = {n_conv_1}, k_conv_1 = {k_conv_1},  n_conv_2 = {n_conv_2}, k_conv_2 = {k_conv_2}, \n",
    "          n_conv_3 = {n_conv_3},  k_conv_3 = {k_conv_3},  maxpooling_pool_size = {maxpooling_pool_size},\n",
    "          avepooling_pool_size = {avepooling_pool_size},\n",
    "          n_lstm_1 = {n_lstm_1}, n_lstm_2 = {n_lstm_2},\n",
    "          drop_lstm_1 = {drop_lstm_1}, drop_lstm_2 = {drop_lstm_2},\n",
    "          n_dense_1 = {n_dense_1}, dropout_1 = {dropout_1},\n",
    "          n_dense_2 = {n_dense_2}, dropout_2 = {dropout_2}, activation_conv=  {activation_conv},\n",
    "          activation_dense=  {activation_dense}\"\"\")\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_9 (Conv1D)            (None, 88, 256)           2560      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 44, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 42, 256)           196864    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_3 (Average (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 19, 256)           196864    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 19, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 19, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2432)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               622848    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 1,415,174\n",
      "Trainable params: 1,415,174\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 256, k_conv_1 = 3,  n_conv_2 = 256, k_conv_2 = 3, \n",
      "          n_conv_3 = 256,  k_conv_3 = 3,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 128,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 256, dropout_2 = 0.2, activation_conv=  relu,\n",
      "          activation_dense=  elu\n"
     ]
    }
   ],
   "source": [
    "model_default = create_model()\n",
    "# model_default.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 88, 512)           5120      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 29, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 28, 256)           262400    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_4 (Average (None, 9, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 7, 512)            393728    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 7, 256)            787456    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 7, 256)            525312    \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1792)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 384)               688512    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 2,862,726\n",
      "Trainable params: 2,862,726\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 512, k_conv_1 = 3,  n_conv_2 = 256, k_conv_2 = 2, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 3,\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 384, dropout_1 = 0.3,\n",
      "          n_dense_2 = 512, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x00000252062F3D48>\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 6s 37ms/step - loss: 1.0695 - accuracy: 0.5395\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.7041 - accuracy: 0.7201\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.5314 - accuracy: 0.7974\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.4232 - accuracy: 0.8376\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.3896 - accuracy: 0.8515\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.3464 - accuracy: 0.8691\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.3305 - accuracy: 0.8740\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.3112 - accuracy: 0.8813\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.3025 - accuracy: 0.8849\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.2829 - accuracy: 0.8927\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.2780 - accuracy: 0.8959\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.2604 - accuracy: 0.9017\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.2502 - accuracy: 0.9058\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 5s 36ms/step - loss: 0.2363 - accuracy: 0.9088\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.2355 - accuracy: 0.9120\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2277 - accuracy: 0.9134\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2139 - accuracy: 0.9195\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2075 - accuracy: 0.9210\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2015 - accuracy: 0.9225\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1956 - accuracy: 0.9251\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1986 - accuracy: 0.9244\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1840 - accuracy: 0.9298\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1806 - accuracy: 0.9314\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1828 - accuracy: 0.9291\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1675 - accuracy: 0.9349\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1616 - accuracy: 0.9379\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1690 - accuracy: 0.9348\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1644 - accuracy: 0.9370\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1592 - accuracy: 0.9400\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1502 - accuracy: 0.9419\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1413 - accuracy: 0.9446\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1474 - accuracy: 0.9438\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1299 - accuracy: 0.9490\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1484 - accuracy: 0.9423\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1300 - accuracy: 0.9494\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1220 - accuracy: 0.9519\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1265 - accuracy: 0.9516\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1174 - accuracy: 0.9532\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1224 - accuracy: 0.9523\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1205 - accuracy: 0.9532\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1104 - accuracy: 0.9567\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1243 - accuracy: 0.9501\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1127 - accuracy: 0.9568\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1150 - accuracy: 0.9564\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1114 - accuracy: 0.9573\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1022 - accuracy: 0.9609\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1079 - accuracy: 0.9592\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1082 - accuracy: 0.9588\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0929 - accuracy: 0.9638\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0953 - accuracy: 0.9634\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0953 - accuracy: 0.9633\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1006 - accuracy: 0.9635\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0886 - accuracy: 0.9663\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0895 - accuracy: 0.9665\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0876 - accuracy: 0.9671\n",
      "Epoch 56/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0841 - accuracy: 0.9664\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0852 - accuracy: 0.9680 0s - los\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0926 - accuracy: 0.9655\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0790 - accuracy: 0.9697\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0807 - accuracy: 0.9693\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 0.2801 - accuracy: 0.9407\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 88, 512)           5120      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 29, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 28, 256)           262400    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_5 (Average (None, 9, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 7, 512)            393728    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 7, 256)            787456    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 7, 256)            525312    \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1792)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 384)               688512    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 2,862,726\n",
      "Trainable params: 2,862,726\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 512, k_conv_1 = 3,  n_conv_2 = 256, k_conv_2 = 2, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 3,\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 384, dropout_1 = 0.3,\n",
      "          n_dense_2 = 512, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x00000252062F3D48>\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 1.1414 - accuracy: 0.5105\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.7491 - accuracy: 0.6939\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.5441 - accuracy: 0.7937\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.4396 - accuracy: 0.8345\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.3866 - accuracy: 0.8527\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.3478 - accuracy: 0.8702\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.3292 - accuracy: 0.8753\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.3132 - accuracy: 0.8814\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2911 - accuracy: 0.8896\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.2786 - accuracy: 0.8931\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2655 - accuracy: 0.8980\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.2567 - accuracy: 0.9021\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.2495 - accuracy: 0.9057\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2424 - accuracy: 0.9074\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2340 - accuracy: 0.9134\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2170 - accuracy: 0.9192\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2208 - accuracy: 0.9185\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2078 - accuracy: 0.9234\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2001 - accuracy: 0.9244\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1944 - accuracy: 0.9276\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1936 - accuracy: 0.9271\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1898 - accuracy: 0.9280\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1814 - accuracy: 0.9313\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1788 - accuracy: 0.9320\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1702 - accuracy: 0.9363\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1581 - accuracy: 0.9386\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1661 - accuracy: 0.9375\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1591 - accuracy: 0.9390\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1569 - accuracy: 0.9398\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1422 - accuracy: 0.9436\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1437 - accuracy: 0.9452\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1451 - accuracy: 0.9445\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1335 - accuracy: 0.9481\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1298 - accuracy: 0.9511\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1297 - accuracy: 0.9502\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1239 - accuracy: 0.9528\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1263 - accuracy: 0.9514\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1281 - accuracy: 0.9497\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1197 - accuracy: 0.9534\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1207 - accuracy: 0.9533\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1138 - accuracy: 0.9558\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1136 - accuracy: 0.9561\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1047 - accuracy: 0.9597\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1028 - accuracy: 0.9609\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1097 - accuracy: 0.9583\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0968 - accuracy: 0.9633\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.0955 - accuracy: 0.9635\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0966 - accuracy: 0.9632\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1151 - accuracy: 0.9568\n",
      "Epoch 50/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0885 - accuracy: 0.9658\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.0882 - accuracy: 0.9660\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.0920 - accuracy: 0.9672\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.0910 - accuracy: 0.9655\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.0855 - accuracy: 0.9675\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.0843 - accuracy: 0.9681\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0858 - accuracy: 0.9677\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.0784 - accuracy: 0.9707\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.0812 - accuracy: 0.9701\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.0723 - accuracy: 0.9735\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.0820 - accuracy: 0.9690\n",
      "76/76 [==============================] - 1s 13ms/step - loss: 0.2786 - accuracy: 0.9406\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_18 (Conv1D)           (None, 88, 512)           5120      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 29, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 28, 256)           262400    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_6 (Average (None, 9, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 7, 512)            393728    \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 7, 256)            787456    \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 7, 256)            525312    \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1792)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 384)               688512    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 512)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 2,862,726\n",
      "Trainable params: 2,862,726\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 512, k_conv_1 = 3,  n_conv_2 = 256, k_conv_2 = 2, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 3,\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 384, dropout_1 = 0.3,\n",
      "          n_dense_2 = 512, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x00000252062F3D48>\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 1.0723 - accuracy: 0.5452\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.7246 - accuracy: 0.7081\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.5271 - accuracy: 0.7999\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.4269 - accuracy: 0.8377\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.3711 - accuracy: 0.8586\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.3486 - accuracy: 0.8680\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.3238 - accuracy: 0.8769\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.3155 - accuracy: 0.8803\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.2909 - accuracy: 0.8924\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.2916 - accuracy: 0.8919\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.2681 - accuracy: 0.9003\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.2663 - accuracy: 0.9014\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.2524 - accuracy: 0.9061\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.2423 - accuracy: 0.9102\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.2367 - accuracy: 0.9121\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2186 - accuracy: 0.9187\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.2209 - accuracy: 0.9180\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2093 - accuracy: 0.9233\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.2070 - accuracy: 0.9237\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.2040 - accuracy: 0.9242\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1882 - accuracy: 0.9307\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1919 - accuracy: 0.9286\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1986 - accuracy: 0.9269\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1766 - accuracy: 0.9330\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1683 - accuracy: 0.9357\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1690 - accuracy: 0.9365\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1686 - accuracy: 0.9378\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1474 - accuracy: 0.9447\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1579 - accuracy: 0.9393\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1468 - accuracy: 0.9435\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1419 - accuracy: 0.9456\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1448 - accuracy: 0.9439\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1343 - accuracy: 0.9485\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1336 - accuracy: 0.9473\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1310 - accuracy: 0.9511\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1214 - accuracy: 0.9535\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1170 - accuracy: 0.9545\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1160 - accuracy: 0.9560\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1265 - accuracy: 0.9530\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1137 - accuracy: 0.9566\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1198 - accuracy: 0.9559\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1178 - accuracy: 0.9552\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1281 - accuracy: 0.9520\n",
      "Epoch 44/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1041 - accuracy: 0.9605\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1067 - accuracy: 0.9605\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1075 - accuracy: 0.9590\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1088 - accuracy: 0.9592\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.0930 - accuracy: 0.9661\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.0943 - accuracy: 0.9644\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.0959 - accuracy: 0.9654\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.0874 - accuracy: 0.9684\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.0945 - accuracy: 0.9653\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0870 - accuracy: 0.9676\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.0929 - accuracy: 0.9652\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.0861 - accuracy: 0.9671\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.0786 - accuracy: 0.9703\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.0797 - accuracy: 0.9701\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.0797 - accuracy: 0.9697\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.0861 - accuracy: 0.9683\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.0796 - accuracy: 0.9705\n",
      "76/76 [==============================] - 1s 13ms/step - loss: 0.3011 - accuracy: 0.9336\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 89, 768)           5376      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 44, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 42, 512)           1180160   \n",
      "_________________________________________________________________\n",
      "average_pooling1d_7 (Average (None, 21, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 19, 512)           786944    \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 19, 256)           787456    \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 19, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4864)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 384)               1868160   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               98560     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 5,253,510\n",
      "Trainable params: 5,253,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 2,  n_conv_2 = 512, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.1,\n",
      "          n_dense_1 = 384, dropout_1 = 0.3,\n",
      "          n_dense_2 = 256, dropout_2 = 0.2, activation_conv=  elu,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 15s 101ms/step - loss: 1.1527 - accuracy: 0.5062\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.8590 - accuracy: 0.6337\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.6999 - accuracy: 0.7111\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.6011 - accuracy: 0.7586\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.5200 - accuracy: 0.7951\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.4682 - accuracy: 0.8197\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.4280 - accuracy: 0.8335\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.3844 - accuracy: 0.8506\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.3662 - accuracy: 0.8600\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.3301 - accuracy: 0.8737\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.3243 - accuracy: 0.8757\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.3108 - accuracy: 0.8822\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2985 - accuracy: 0.8832\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2856 - accuracy: 0.8899\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2754 - accuracy: 0.8939\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2687 - accuracy: 0.8970\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2608 - accuracy: 0.9006\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2636 - accuracy: 0.8974\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2408 - accuracy: 0.9067\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2353 - accuracy: 0.9118\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2422 - accuracy: 0.9067\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2393 - accuracy: 0.9073\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2215 - accuracy: 0.9150\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2190 - accuracy: 0.9169\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2077 - accuracy: 0.9210\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2099 - accuracy: 0.9199\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2018 - accuracy: 0.9230\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2138 - accuracy: 0.9196\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2068 - accuracy: 0.9217\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2003 - accuracy: 0.9237\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1962 - accuracy: 0.9240\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1802 - accuracy: 0.9305\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1840 - accuracy: 0.9305\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1804 - accuracy: 0.9309\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1763 - accuracy: 0.9322\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1755 - accuracy: 0.9323\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1748 - accuracy: 0.9343\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1633 - accuracy: 0.9386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1607 - accuracy: 0.9387\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1682 - accuracy: 0.9354\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1657 - accuracy: 0.9392\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1550 - accuracy: 0.9418\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1731 - accuracy: 0.9360\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1647 - accuracy: 0.9374\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1581 - accuracy: 0.9403\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1544 - accuracy: 0.9415\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1526 - accuracy: 0.9423\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1523 - accuracy: 0.9436\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1404 - accuracy: 0.9464\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1465 - accuracy: 0.9455\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1420 - accuracy: 0.9466\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1358 - accuracy: 0.9501\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1303 - accuracy: 0.9529\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1287 - accuracy: 0.9519\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1339 - accuracy: 0.95021s - l\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1325 - accuracy: 0.9517\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1430 - accuracy: 0.9480\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1316 - accuracy: 0.95170s - loss: 0.1324 - ac\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1303 - accuracy: 0.9509\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1282 - accuracy: 0.9521\n",
      "76/76 [==============================] - 3s 37ms/step - loss: 0.2774 - accuracy: 0.9229\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_24 (Conv1D)           (None, 89, 768)           5376      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 44, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 42, 512)           1180160   \n",
      "_________________________________________________________________\n",
      "average_pooling1d_8 (Average (None, 21, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 19, 512)           786944    \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 19, 256)           787456    \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 19, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 4864)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 384)               1868160   \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 256)               98560     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 5,253,510\n",
      "Trainable params: 5,253,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 2,  n_conv_2 = 512, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.1,\n",
      "          n_dense_1 = 384, dropout_1 = 0.3,\n",
      "          n_dense_2 = 256, dropout_2 = 0.2, activation_conv=  elu,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 1.1510 - accuracy: 0.5100\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.8542 - accuracy: 0.6336\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.6903 - accuracy: 0.71471s -\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.5917 - accuracy: 0.7630\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.5026 - accuracy: 0.8051\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.4571 - accuracy: 0.8247\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.4138 - accuracy: 0.8395\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.3984 - accuracy: 0.8463\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.3747 - accuracy: 0.8550\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.3262 - accuracy: 0.8726\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.3204 - accuracy: 0.8756\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.3473 - accuracy: 0.8666\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2940 - accuracy: 0.8871\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2906 - accuracy: 0.8875\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2626 - accuracy: 0.8979\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2655 - accuracy: 0.8965\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2550 - accuracy: 0.9013\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2498 - accuracy: 0.9016\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2470 - accuracy: 0.9054\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2404 - accuracy: 0.9078\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2298 - accuracy: 0.9111\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2389 - accuracy: 0.9073\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2270 - accuracy: 0.91310s - loss: 0.228\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2157 - accuracy: 0.9163\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2146 - accuracy: 0.9173\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1985 - accuracy: 0.92501s -\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2072 - accuracy: 0.9214\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1894 - accuracy: 0.9269\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1861 - accuracy: 0.9298\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1873 - accuracy: 0.9282\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1868 - accuracy: 0.9291\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1814 - accuracy: 0.9317\n",
      "Epoch 33/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1869 - accuracy: 0.9305\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1703 - accuracy: 0.9368\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1742 - accuracy: 0.9334\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1740 - accuracy: 0.9343\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1691 - accuracy: 0.93801s - loss: 0.1692 - accuracy: 0.93 - ETA: 1s - los\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1787 - accuracy: 0.9323\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1650 - accuracy: 0.9371\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1628 - accuracy: 0.9395\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1504 - accuracy: 0.94301s - los\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1661 - accuracy: 0.9386\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1566 - accuracy: 0.9422\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1565 - accuracy: 0.9403\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1501 - accuracy: 0.9435\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1533 - accuracy: 0.9421\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1349 - accuracy: 0.9502\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1457 - accuracy: 0.9473\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1445 - accuracy: 0.9474\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1417 - accuracy: 0.9458\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1446 - accuracy: 0.9464\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1393 - accuracy: 0.9474\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1332 - accuracy: 0.9494\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1382 - accuracy: 0.9491\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1374 - accuracy: 0.9487\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1347 - accuracy: 0.9483\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1304 - accuracy: 0.9515\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1248 - accuracy: 0.9527\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1292 - accuracy: 0.9531\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1233 - accuracy: 0.9541\n",
      "76/76 [==============================] - 3s 34ms/step - loss: 0.3036 - accuracy: 0.9233\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_27 (Conv1D)           (None, 89, 768)           5376      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 44, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 42, 512)           1180160   \n",
      "_________________________________________________________________\n",
      "average_pooling1d_9 (Average (None, 21, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 19, 512)           786944    \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 19, 256)           787456    \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 19, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 4864)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 384)               1868160   \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 256)               98560     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 5,253,510\n",
      "Trainable params: 5,253,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 2,  n_conv_2 = 512, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.1,\n",
      "          n_dense_1 = 384, dropout_1 = 0.3,\n",
      "          n_dense_2 = 256, dropout_2 = 0.2, activation_conv=  elu,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 1.1511 - accuracy: 0.5130\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.8437 - accuracy: 0.6409\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.6929 - accuracy: 0.71800s - loss: 0.6945 - accu\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.5942 - accuracy: 0.7642\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.5128 - accuracy: 0.7967\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.4690 - accuracy: 0.8176\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.4367 - accuracy: 0.8316\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.3946 - accuracy: 0.8482\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.3788 - accuracy: 0.8552\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.3568 - accuracy: 0.8627\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.3429 - accuracy: 0.8681\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.3202 - accuracy: 0.8773\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.3120 - accuracy: 0.8801\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2932 - accuracy: 0.8880\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2905 - accuracy: 0.8896\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2778 - accuracy: 0.8909\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2754 - accuracy: 0.8943\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2592 - accuracy: 0.8983\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2569 - accuracy: 0.8989\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2548 - accuracy: 0.9024\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2503 - accuracy: 0.9030\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2383 - accuracy: 0.9082\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2316 - accuracy: 0.9114\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2220 - accuracy: 0.9141\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2207 - accuracy: 0.9158\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2150 - accuracy: 0.9175\n",
      "Epoch 27/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2060 - accuracy: 0.9198\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2087 - accuracy: 0.9189\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2171 - accuracy: 0.9179\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1973 - accuracy: 0.9241\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.2094 - accuracy: 0.9205\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1940 - accuracy: 0.9263\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1985 - accuracy: 0.9248\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1877 - accuracy: 0.9288\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1835 - accuracy: 0.9322\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1845 - accuracy: 0.9298\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1749 - accuracy: 0.9337\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1797 - accuracy: 0.9325\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1724 - accuracy: 0.9352\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1694 - accuracy: 0.9343\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1714 - accuracy: 0.9348\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1628 - accuracy: 0.94020s - loss: 0.161\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1574 - accuracy: 0.9414\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1723 - accuracy: 0.9344\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1617 - accuracy: 0.9401\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1627 - accuracy: 0.93831s - loss:\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1582 - accuracy: 0.9400\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1554 - accuracy: 0.9424\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1518 - accuracy: 0.9429\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1485 - accuracy: 0.9440\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1496 - accuracy: 0.9446\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1456 - accuracy: 0.9459\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1402 - accuracy: 0.9475\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1500 - accuracy: 0.9450\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1467 - accuracy: 0.9445\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1387 - accuracy: 0.9475\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1249 - accuracy: 0.9530\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1287 - accuracy: 0.9515\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1387 - accuracy: 0.9489\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 15s 98ms/step - loss: 0.1557 - accuracy: 0.9429\n",
      "76/76 [==============================] - 3s 34ms/step - loss: 0.2886 - accuracy: 0.9192\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 88, 768)           7680      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 29, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 28, 512)           786944    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_10 (Averag (None, 9, 512)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 7, 512)            786944    \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 7, 256)            787456    \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 7, 128)            197120    \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 512)               459264    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 3,158,278\n",
      "Trainable params: 3,158,278\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 3,  n_conv_2 = 512, k_conv_2 = 2, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 3,\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 512, dropout_1 = 0.3,\n",
      "          n_dense_2 = 256, dropout_2 = 0.2, activation_conv=  elu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x00000252062F3D48>\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 9s 56ms/step - loss: 1.0580 - accuracy: 0.5465\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.7969 - accuracy: 0.6631\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.6783 - accuracy: 0.7209\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.5771 - accuracy: 0.7721\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.4949 - accuracy: 0.8099\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.4718 - accuracy: 0.8173\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.4319 - accuracy: 0.8343\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.3860 - accuracy: 0.8532\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.3751 - accuracy: 0.8577\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.3728 - accuracy: 0.8585\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.3702 - accuracy: 0.8601\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.3288 - accuracy: 0.8743\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.3221 - accuracy: 0.8771\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.3166 - accuracy: 0.8818\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.3020 - accuracy: 0.8841\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2987 - accuracy: 0.8871\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2861 - accuracy: 0.8939\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2879 - accuracy: 0.8913\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2858 - accuracy: 0.8904\n",
      "Epoch 20/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2666 - accuracy: 0.9018\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2640 - accuracy: 0.8995\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2540 - accuracy: 0.9049\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2437 - accuracy: 0.9076\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2403 - accuracy: 0.9096\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2421 - accuracy: 0.9101\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2335 - accuracy: 0.9120\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2395 - accuracy: 0.9098\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2346 - accuracy: 0.9105\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2327 - accuracy: 0.9128\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2257 - accuracy: 0.9160\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2185 - accuracy: 0.9164\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2154 - accuracy: 0.9180\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2743 - accuracy: 0.9006\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2185 - accuracy: 0.9168\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2112 - accuracy: 0.9205\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2110 - accuracy: 0.9190\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2067 - accuracy: 0.9223\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2054 - accuracy: 0.9231\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2048 - accuracy: 0.9231\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2025 - accuracy: 0.9237\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2084 - accuracy: 0.9211\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1998 - accuracy: 0.9231\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1962 - accuracy: 0.9257\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2003 - accuracy: 0.9236\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1937 - accuracy: 0.9248\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1935 - accuracy: 0.9247\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1888 - accuracy: 0.9291\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1983 - accuracy: 0.9254\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1926 - accuracy: 0.9257\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1832 - accuracy: 0.9292\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1931 - accuracy: 0.9262\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1896 - accuracy: 0.9278\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1941 - accuracy: 0.9262\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1923 - accuracy: 0.9262\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1812 - accuracy: 0.9305\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1860 - accuracy: 0.9288\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1935 - accuracy: 0.9252\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1845 - accuracy: 0.9300\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1781 - accuracy: 0.9328\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1743 - accuracy: 0.9333\n",
      "76/76 [==============================] - 2s 21ms/step - loss: 0.2735 - accuracy: 0.9174\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_33 (Conv1D)           (None, 88, 768)           7680      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 29, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 28, 512)           786944    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_11 (Averag (None, 9, 512)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 7, 512)            786944    \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 7, 256)            787456    \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 7, 128)            197120    \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 512)               459264    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 3,158,278\n",
      "Trainable params: 3,158,278\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 3,  n_conv_2 = 512, k_conv_2 = 2, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 3,\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 512, dropout_1 = 0.3,\n",
      "          n_dense_2 = 256, dropout_2 = 0.2, activation_conv=  elu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x00000252062F3D48>\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 1.0598 - accuracy: 0.5451\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.8043 - accuracy: 0.6652\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.6837 - accuracy: 0.7211\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.6067 - accuracy: 0.7600\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.5488 - accuracy: 0.7854\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.4810 - accuracy: 0.8169\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.4501 - accuracy: 0.8291\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.4094 - accuracy: 0.8423\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.3722 - accuracy: 0.8585\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.3619 - accuracy: 0.8638\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.3362 - accuracy: 0.8738\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.3473 - accuracy: 0.8695\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.3386 - accuracy: 0.8707\n",
      "Epoch 14/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 8s 53ms/step - loss: 0.3152 - accuracy: 0.8801\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2976 - accuracy: 0.8881\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.3023 - accuracy: 0.8862\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2902 - accuracy: 0.8901\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2908 - accuracy: 0.8877\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2952 - accuracy: 0.8882\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2719 - accuracy: 0.8951\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2639 - accuracy: 0.8997\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2557 - accuracy: 0.9039\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2529 - accuracy: 0.9051\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2432 - accuracy: 0.9084\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2392 - accuracy: 0.9089\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2391 - accuracy: 0.9094\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2367 - accuracy: 0.9090\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2467 - accuracy: 0.9048\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2377 - accuracy: 0.9093\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2254 - accuracy: 0.9141\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2334 - accuracy: 0.9113\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2193 - accuracy: 0.9166\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2074 - accuracy: 0.9217\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2157 - accuracy: 0.9179\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2127 - accuracy: 0.9164\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2329 - accuracy: 0.9127\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2048 - accuracy: 0.9220\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2017 - accuracy: 0.9234\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2059 - accuracy: 0.9227\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2105 - accuracy: 0.9180\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1978 - accuracy: 0.9245\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1963 - accuracy: 0.9255\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1966 - accuracy: 0.9251\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1926 - accuracy: 0.9271\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1890 - accuracy: 0.9287\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1894 - accuracy: 0.9293\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1895 - accuracy: 0.9273\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1934 - accuracy: 0.9269\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1874 - accuracy: 0.9285\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1906 - accuracy: 0.9281\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1922 - accuracy: 0.9272\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1757 - accuracy: 0.9332\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1867 - accuracy: 0.9285\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1736 - accuracy: 0.9339\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1943 - accuracy: 0.9272\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1746 - accuracy: 0.9336\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1724 - accuracy: 0.9345\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1841 - accuracy: 0.9297\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1860 - accuracy: 0.9304\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1729 - accuracy: 0.9331\n",
      "76/76 [==============================] - 1s 19ms/step - loss: 0.2884 - accuracy: 0.9149\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 88, 768)           7680      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 29, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 28, 512)           786944    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_12 (Averag (None, 9, 512)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 7, 512)            786944    \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 7, 256)            787456    \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 7, 128)            197120    \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 512)               459264    \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 3,158,278\n",
      "Trainable params: 3,158,278\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 3,  n_conv_2 = 512, k_conv_2 = 2, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 3,\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 512, dropout_1 = 0.3,\n",
      "          n_dense_2 = 256, dropout_2 = 0.2, activation_conv=  elu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x00000252062F3D48>\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 1.0736 - accuracy: 0.5381\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.8223 - accuracy: 0.6564\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.7078 - accuracy: 0.7111\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.6064 - accuracy: 0.7579\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.5453 - accuracy: 0.7857\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.4996 - accuracy: 0.8071\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.4297 - accuracy: 0.8377\n",
      "Epoch 8/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 8s 53ms/step - loss: 0.4108 - accuracy: 0.8450\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.3719 - accuracy: 0.8615\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.3613 - accuracy: 0.8636\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.3361 - accuracy: 0.8735\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.3375 - accuracy: 0.8730\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.3143 - accuracy: 0.8829\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.3111 - accuracy: 0.8837\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2986 - accuracy: 0.8890\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.3060 - accuracy: 0.8853\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2818 - accuracy: 0.8945\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2756 - accuracy: 0.8980\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2639 - accuracy: 0.9015\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2576 - accuracy: 0.9041\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2641 - accuracy: 0.9020\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2555 - accuracy: 0.9043\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2389 - accuracy: 0.9119\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2462 - accuracy: 0.9073\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2400 - accuracy: 0.9109\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2431 - accuracy: 0.9096\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2237 - accuracy: 0.9171\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2197 - accuracy: 0.9174\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2233 - accuracy: 0.9159\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2297 - accuracy: 0.9145\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2124 - accuracy: 0.9205\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2230 - accuracy: 0.9169\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2173 - accuracy: 0.9182\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2226 - accuracy: 0.9162\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2038 - accuracy: 0.9246\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2096 - accuracy: 0.9201\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2083 - accuracy: 0.9222\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2045 - accuracy: 0.9237\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1953 - accuracy: 0.9264\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1978 - accuracy: 0.9260\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2036 - accuracy: 0.9236\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1987 - accuracy: 0.9250\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1989 - accuracy: 0.9249\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2054 - accuracy: 0.9213\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1898 - accuracy: 0.9279\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1926 - accuracy: 0.9280\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1941 - accuracy: 0.9270\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1969 - accuracy: 0.9265\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.2065 - accuracy: 0.9231\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1854 - accuracy: 0.9292\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1834 - accuracy: 0.9291\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1735 - accuracy: 0.9343\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1929 - accuracy: 0.9274\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1843 - accuracy: 0.9314\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1775 - accuracy: 0.9332\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1772 - accuracy: 0.9327\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1717 - accuracy: 0.9348\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1800 - accuracy: 0.9320\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1593 - accuracy: 0.9399\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 8s 53ms/step - loss: 0.1680 - accuracy: 0.9357\n",
      "76/76 [==============================] - 1s 19ms/step - loss: 0.3019 - accuracy: 0.9116\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_39 (Conv1D)           (None, 89, 512)           3584      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 29, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 27, 512)           786944    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_13 (Averag (None, 13, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 12, 512)           524800    \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, 12, 256)           787456    \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               (None, 12, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 512)               1573376   \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 4,334,342\n",
      "Trainable params: 4,334,342\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 512, k_conv_1 = 2,  n_conv_2 = 512, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 2,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.1,\n",
      "          n_dense_1 = 512, dropout_1 = 0.2,\n",
      "          n_dense_2 = 256, dropout_2 = 0.2, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000002527E6DD448>,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x00000252062F3D48>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 9s 59ms/step - loss: 1.0874 - accuracy: 0.5266\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.8351 - accuracy: 0.6413\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.7123 - accuracy: 0.7012\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.5860 - accuracy: 0.7590\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.5067 - accuracy: 0.7922\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.4478 - accuracy: 0.8181\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.4050 - accuracy: 0.8354\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.3846 - accuracy: 0.8466\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.3607 - accuracy: 0.8526\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.3479 - accuracy: 0.8611\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.3225 - accuracy: 0.8691\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.3094 - accuracy: 0.8727\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.3127 - accuracy: 0.8736\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.2779 - accuracy: 0.8892\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.2599 - accuracy: 0.8949\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.2525 - accuracy: 0.8970\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.2413 - accuracy: 0.9039\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.2332 - accuracy: 0.9080\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.2143 - accuracy: 0.9161\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1983 - accuracy: 0.9236\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.2105 - accuracy: 0.9194\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1872 - accuracy: 0.9272\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1809 - accuracy: 0.9293\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1852 - accuracy: 0.9292\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1626 - accuracy: 0.9388\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1664 - accuracy: 0.9367\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1638 - accuracy: 0.9385\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1543 - accuracy: 0.9405\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1560 - accuracy: 0.9397\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1453 - accuracy: 0.9457\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1453 - accuracy: 0.9459\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1300 - accuracy: 0.9494\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1419 - accuracy: 0.9454\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1327 - accuracy: 0.9504\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1310 - accuracy: 0.9494\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1283 - accuracy: 0.9520\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1320 - accuracy: 0.9500\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1229 - accuracy: 0.9540\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1231 - accuracy: 0.9547\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1248 - accuracy: 0.9519\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1408 - accuracy: 0.9471\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1210 - accuracy: 0.9554\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1198 - accuracy: 0.9562\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1299 - accuracy: 0.9510\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1103 - accuracy: 0.9586\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1092 - accuracy: 0.9596\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1051 - accuracy: 0.9612\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1111 - accuracy: 0.9591\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1105 - accuracy: 0.9585\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1065 - accuracy: 0.9603\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.0941 - accuracy: 0.9649\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.0973 - accuracy: 0.9644\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.0952 - accuracy: 0.9633\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.0939 - accuracy: 0.9637\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.0921 - accuracy: 0.9658\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.0994 - accuracy: 0.9633\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.0931 - accuracy: 0.9649\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.0939 - accuracy: 0.9645\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.0937 - accuracy: 0.9640\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.0879 - accuracy: 0.9674\n",
      "76/76 [==============================] - 2s 23ms/step - loss: 0.2980 - accuracy: 0.9336\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 89, 512)           3584      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 29, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 27, 512)           786944    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_14 (Averag (None, 13, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 12, 512)           524800    \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               (None, 12, 256)           787456    \n",
      "_________________________________________________________________\n",
      "lstm_26 (LSTM)               (None, 12, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 512)               1573376   \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 4,334,342\n",
      "Trainable params: 4,334,342\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 512, k_conv_1 = 2,  n_conv_2 = 512, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 2,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.1,\n",
      "          n_dense_1 = 512, dropout_1 = 0.2,\n",
      "          n_dense_2 = 256, dropout_2 = 0.2, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000002527E6DD448>,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x00000252062F3D48>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 8s 55ms/step - loss: 1.0968 - accuracy: 0.5244\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.8554 - accuracy: 0.6332\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.7352 - accuracy: 0.6884\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.5934 - accuracy: 0.7525\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.4896 - accuracy: 0.7992\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.4329 - accuracy: 0.8241\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.3931 - accuracy: 0.8439\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.3656 - accuracy: 0.8523\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.3547 - accuracy: 0.8559\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.3397 - accuracy: 0.8612\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.3069 - accuracy: 0.8748\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.3063 - accuracy: 0.8742\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.2887 - accuracy: 0.8818\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.2537 - accuracy: 0.8984 \n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.2528 - accuracy: 0.8996\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.2407 - accuracy: 0.9059\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.2270 - accuracy: 0.9111\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.2285 - accuracy: 0.9097\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.2072 - accuracy: 0.9195\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.2061 - accuracy: 0.9201\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1867 - accuracy: 0.9277\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1896 - accuracy: 0.9257\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1863 - accuracy: 0.9284\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1789 - accuracy: 0.9306\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1838 - accuracy: 0.9298\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1569 - accuracy: 0.9396\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1455 - accuracy: 0.9446\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1593 - accuracy: 0.9388\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1460 - accuracy: 0.9436\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1381 - accuracy: 0.9471\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1399 - accuracy: 0.9465\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1334 - accuracy: 0.9490\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1342 - accuracy: 0.9504\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1260 - accuracy: 0.9525\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1251 - accuracy: 0.9516\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1241 - accuracy: 0.9517\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1295 - accuracy: 0.9511\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1164 - accuracy: 0.9564\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1227 - accuracy: 0.9545\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1208 - accuracy: 0.9541\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1187 - accuracy: 0.9554\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1029 - accuracy: 0.9598\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1123 - accuracy: 0.9587\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1133 - accuracy: 0.9591\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1102 - accuracy: 0.9575\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1001 - accuracy: 0.9621\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1002 - accuracy: 0.9616\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1005 - accuracy: 0.9626\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1264 - accuracy: 0.9526\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1032 - accuracy: 0.9611\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1102 - accuracy: 0.9592\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1037 - accuracy: 0.9604\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.0922 - accuracy: 0.9651\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.0970 - accuracy: 0.9638\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.0929 - accuracy: 0.9658\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.0934 - accuracy: 0.9655\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.0891 - accuracy: 0.9666\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.0895 - accuracy: 0.9667\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.0936 - accuracy: 0.9652\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.0975 - accuracy: 0.9638\n",
      "76/76 [==============================] - 2s 20ms/step - loss: 0.3563 - accuracy: 0.9216\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 89, 512)           3584      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 29, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 27, 512)           786944    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_15 (Averag (None, 13, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 12, 512)           524800    \n",
      "_________________________________________________________________\n",
      "lstm_27 (LSTM)               (None, 12, 256)           787456    \n",
      "_________________________________________________________________\n",
      "lstm_28 (LSTM)               (None, 12, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 512)               1573376   \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 4,334,342\n",
      "Trainable params: 4,334,342\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 512, k_conv_1 = 2,  n_conv_2 = 512, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 2,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.1,\n",
      "          n_dense_1 = 512, dropout_1 = 0.2,\n",
      "          n_dense_2 = 256, dropout_2 = 0.2, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000002527E6DD448>,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x00000252062F3D48>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 8s 55ms/step - loss: 1.1040 - accuracy: 0.5176\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.8607 - accuracy: 0.6291\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.7653 - accuracy: 0.6724\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.6338 - accuracy: 0.7367\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.5223 - accuracy: 0.7853\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.4506 - accuracy: 0.8185 0s - loss: 0.4504 - accuracy: \n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.4163 - accuracy: 0.8323\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.3820 - accuracy: 0.8475\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.3588 - accuracy: 0.8577\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.3435 - accuracy: 0.8638\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.3257 - accuracy: 0.8675\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.3143 - accuracy: 0.8736\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.2901 - accuracy: 0.8809\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.2804 - accuracy: 0.8858\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.2602 - accuracy: 0.8962\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.2493 - accuracy: 0.9001\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.2383 - accuracy: 0.9073\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.2352 - accuracy: 0.9071\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.2139 - accuracy: 0.9167\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.2088 - accuracy: 0.9183\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1924 - accuracy: 0.9244\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1894 - accuracy: 0.9286\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.2064 - accuracy: 0.9205\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1734 - accuracy: 0.9337\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1800 - accuracy: 0.9311\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1779 - accuracy: 0.9323\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1585 - accuracy: 0.9385\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1518 - accuracy: 0.9408\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1465 - accuracy: 0.9440\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1490 - accuracy: 0.9432\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1440 - accuracy: 0.9464\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1411 - accuracy: 0.9468\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1344 - accuracy: 0.9494\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1340 - accuracy: 0.9482\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1310 - accuracy: 0.9501\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1234 - accuracy: 0.9517\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1226 - accuracy: 0.9540\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1226 - accuracy: 0.9539\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1230 - accuracy: 0.9533\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1230 - accuracy: 0.9530\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1124 - accuracy: 0.9575\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1087 - accuracy: 0.9585\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1077 - accuracy: 0.9589\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1118 - accuracy: 0.9582\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1140 - accuracy: 0.9574\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.0983 - accuracy: 0.9630\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1069 - accuracy: 0.9605\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1119 - accuracy: 0.9586\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.0948 - accuracy: 0.9636\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1045 - accuracy: 0.9620\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.0985 - accuracy: 0.9635\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1005 - accuracy: 0.9618\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.0967 - accuracy: 0.9629\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.0908 - accuracy: 0.9661\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 8s 55ms/step - loss: 0.1040 - accuracy: 0.9616\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 9s 57ms/step - loss: 0.0928 - accuracy: 0.9661\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 9s 58ms/step - loss: 0.1091 - accuracy: 0.9601\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 9s 57ms/step - loss: 0.0911 - accuracy: 0.9657\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 9s 57ms/step - loss: 0.0969 - accuracy: 0.9634\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 9s 56ms/step - loss: 0.0840 - accuracy: 0.9679\n",
      "76/76 [==============================] - 2s 20ms/step - loss: 0.3264 - accuracy: 0.9342\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_48 (Conv1D)           (None, 88, 512)           5120      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 29, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 28, 256)           262400    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_16 (Averag (None, 14, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 12, 512)           393728    \n",
      "_________________________________________________________________\n",
      "lstm_29 (LSTM)               (None, 12, 128)           328192    \n",
      "_________________________________________________________________\n",
      "lstm_30 (LSTM)               (None, 12, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 2,040,838\n",
      "Trainable params: 2,040,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 512, k_conv_1 = 3,  n_conv_2 = 256, k_conv_2 = 2, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 128,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 512, dropout_1 = 0.2,\n",
      "          n_dense_2 = 256, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x00000252062F3D48>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 5s 36ms/step - loss: 0.9905 - accuracy: 0.5772\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.6285 - accuracy: 0.7501\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.4692 - accuracy: 0.8203\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 6s 36ms/step - loss: 0.3961 - accuracy: 0.8500\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.3696 - accuracy: 0.8599\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.3277 - accuracy: 0.8749\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.3162 - accuracy: 0.8781\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.2953 - accuracy: 0.8868\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.2823 - accuracy: 0.8927\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.2678 - accuracy: 0.8995\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.2593 - accuracy: 0.9002\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.2561 - accuracy: 0.9016\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 5s 36ms/step - loss: 0.2427 - accuracy: 0.9078\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 5s 36ms/step - loss: 0.2369 - accuracy: 0.9099\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 5s 36ms/step - loss: 0.2256 - accuracy: 0.9139\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.2224 - accuracy: 0.9156\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.2156 - accuracy: 0.9193\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1990 - accuracy: 0.9244\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.2015 - accuracy: 0.9224\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1988 - accuracy: 0.9244\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1907 - accuracy: 0.9270\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1870 - accuracy: 0.9273\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1765 - accuracy: 0.9313\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 5s 36ms/step - loss: 0.1770 - accuracy: 0.9333\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 5s 36ms/step - loss: 0.1669 - accuracy: 0.9357\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1753 - accuracy: 0.9330\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1490 - accuracy: 0.9409\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1634 - accuracy: 0.9363\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 5s 35ms/step - loss: 0.1495 - accuracy: 0.9431\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1422 - accuracy: 0.9454\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1368 - accuracy: 0.9462\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1324 - accuracy: 0.9473\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1365 - accuracy: 0.9461\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1285 - accuracy: 0.9490\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1214 - accuracy: 0.9525\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1232 - accuracy: 0.9518\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1231 - accuracy: 0.9520\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1196 - accuracy: 0.9514\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1125 - accuracy: 0.9546\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1063 - accuracy: 0.9578\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1200 - accuracy: 0.9545\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1100 - accuracy: 0.9569\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1027 - accuracy: 0.9598\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1113 - accuracy: 0.9576\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0986 - accuracy: 0.9620\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1017 - accuracy: 0.9613\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0999 - accuracy: 0.9613\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1032 - accuracy: 0.9594\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0918 - accuracy: 0.9643\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0967 - accuracy: 0.9634\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0911 - accuracy: 0.9653\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0888 - accuracy: 0.9668\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0887 - accuracy: 0.9668\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0829 - accuracy: 0.9676\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0887 - accuracy: 0.9661\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0802 - accuracy: 0.9687\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0779 - accuracy: 0.9711\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0766 - accuracy: 0.9706\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0759 - accuracy: 0.9714\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0791 - accuracy: 0.9709\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 0.2896 - accuracy: 0.9372\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 88, 512)           5120      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 29, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 28, 256)           262400    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_17 (Averag (None, 14, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 12, 512)           393728    \n",
      "_________________________________________________________________\n",
      "lstm_31 (LSTM)               (None, 12, 128)           328192    \n",
      "_________________________________________________________________\n",
      "lstm_32 (LSTM)               (None, 12, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 2,040,838\n",
      "Trainable params: 2,040,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 512, k_conv_1 = 3,  n_conv_2 = 256, k_conv_2 = 2, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 128,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 512, dropout_1 = 0.2,\n",
      "          n_dense_2 = 256, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x00000252062F3D48>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 1.0302 - accuracy: 0.5595\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.6452 - accuracy: 0.7456\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.4676 - accuracy: 0.8222\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.3910 - accuracy: 0.8535\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.3647 - accuracy: 0.8606\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.3305 - accuracy: 0.8733\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.3103 - accuracy: 0.8820\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2947 - accuracy: 0.8880\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2935 - accuracy: 0.8881\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2670 - accuracy: 0.8993\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2598 - accuracy: 0.9011\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2466 - accuracy: 0.9063\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2307 - accuracy: 0.9101\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2248 - accuracy: 0.9150\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2303 - accuracy: 0.9132\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2107 - accuracy: 0.9194\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2070 - accuracy: 0.9220\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2020 - accuracy: 0.9244\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1939 - accuracy: 0.9268\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1876 - accuracy: 0.9287\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1792 - accuracy: 0.9313\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1773 - accuracy: 0.9323\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1689 - accuracy: 0.9354\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1734 - accuracy: 0.9329\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1542 - accuracy: 0.9403\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1609 - accuracy: 0.9376\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1471 - accuracy: 0.9432\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1517 - accuracy: 0.9412\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1437 - accuracy: 0.9461\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1395 - accuracy: 0.9464\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1325 - accuracy: 0.9487\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1287 - accuracy: 0.9501\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1269 - accuracy: 0.9520\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1253 - accuracy: 0.9514\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1309 - accuracy: 0.9517\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1119 - accuracy: 0.9571\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1131 - accuracy: 0.9572\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1108 - accuracy: 0.9571\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1140 - accuracy: 0.9563\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1051 - accuracy: 0.9598\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1167 - accuracy: 0.9565\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1051 - accuracy: 0.9597\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1001 - accuracy: 0.9619\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1020 - accuracy: 0.9615\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0968 - accuracy: 0.9622\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0990 - accuracy: 0.9629\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0906 - accuracy: 0.9655\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0945 - accuracy: 0.9646\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0883 - accuracy: 0.9659\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0937 - accuracy: 0.9646\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0861 - accuracy: 0.9670\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0833 - accuracy: 0.9692\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0851 - accuracy: 0.9676\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0833 - accuracy: 0.9692\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0824 - accuracy: 0.9695\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0778 - accuracy: 0.9711\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0780 - accuracy: 0.9718\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0743 - accuracy: 0.9714\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0801 - accuracy: 0.9709\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0763 - accuracy: 0.9717\n",
      "76/76 [==============================] - 1s 13ms/step - loss: 0.3063 - accuracy: 0.9370\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_54 (Conv1D)           (None, 88, 512)           5120      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 29, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 28, 256)           262400    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_18 (Averag (None, 14, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 12, 512)           393728    \n",
      "_________________________________________________________________\n",
      "lstm_33 (LSTM)               (None, 12, 128)           328192    \n",
      "_________________________________________________________________\n",
      "lstm_34 (LSTM)               (None, 12, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 2,040,838\n",
      "Trainable params: 2,040,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 512, k_conv_1 = 3,  n_conv_2 = 256, k_conv_2 = 2, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 128,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 512, dropout_1 = 0.2,\n",
      "          n_dense_2 = 256, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x00000252062F3D48>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/152 [..............................] - ETA: 2s - loss: 1.7914 - accuracy: 0.1250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0124s vs `on_train_batch_end` time: 0.0302s). Check your callbacks.\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 1.0329 - accuracy: 0.5594\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.6515 - accuracy: 0.7389\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.4816 - accuracy: 0.8154\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.4024 - accuracy: 0.8475\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.3659 - accuracy: 0.8608\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.3368 - accuracy: 0.8708\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.3180 - accuracy: 0.8764\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2953 - accuracy: 0.8863\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2787 - accuracy: 0.8946\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2766 - accuracy: 0.8960\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2659 - accuracy: 0.9004\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2455 - accuracy: 0.9078\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2456 - accuracy: 0.9084\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2290 - accuracy: 0.9141\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2355 - accuracy: 0.9119\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2175 - accuracy: 0.9175\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2065 - accuracy: 0.9234\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.2179 - accuracy: 0.9179\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1926 - accuracy: 0.9267\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1928 - accuracy: 0.9284\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1936 - accuracy: 0.9268\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1856 - accuracy: 0.9300\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1723 - accuracy: 0.9344\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1729 - accuracy: 0.9329\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1668 - accuracy: 0.9360\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1562 - accuracy: 0.9398\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1639 - accuracy: 0.9378\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1568 - accuracy: 0.9388\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1451 - accuracy: 0.9448\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1456 - accuracy: 0.9432\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1378 - accuracy: 0.9461\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1491 - accuracy: 0.9414\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1287 - accuracy: 0.9490\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1311 - accuracy: 0.9474\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1240 - accuracy: 0.9519\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1325 - accuracy: 0.9493\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1195 - accuracy: 0.9537\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1212 - accuracy: 0.9531\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1122 - accuracy: 0.9549\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1070 - accuracy: 0.9586\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1073 - accuracy: 0.9576\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1041 - accuracy: 0.9586\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1084 - accuracy: 0.9573\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1015 - accuracy: 0.9604\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1008 - accuracy: 0.9617\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0988 - accuracy: 0.9629\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1019 - accuracy: 0.9612\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.1054 - accuracy: 0.9598\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0877 - accuracy: 0.9653\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0964 - accuracy: 0.9638\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0891 - accuracy: 0.9662\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0845 - accuracy: 0.9682\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0876 - accuracy: 0.9673\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0847 - accuracy: 0.9683\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0916 - accuracy: 0.9659\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0843 - accuracy: 0.9685\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0784 - accuracy: 0.9708\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0795 - accuracy: 0.9698\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0883 - accuracy: 0.9675\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 5s 34ms/step - loss: 0.0719 - accuracy: 0.9726\n",
      "76/76 [==============================] - 1s 13ms/step - loss: 0.2872 - accuracy: 0.9411\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_57 (Conv1D)           (None, 89, 768)           5376      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 29, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 27, 512)           1180160   \n",
      "_________________________________________________________________\n",
      "average_pooling1d_19 (Averag (None, 13, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 11, 512)           786944    \n",
      "_________________________________________________________________\n",
      "lstm_35 (LSTM)               (None, 11, 128)           328192    \n",
      "_________________________________________________________________\n",
      "lstm_36 (LSTM)               (None, 11, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 2816)              0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 256)               721152    \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 3,483,398\n",
      "Trainable params: 3,483,398\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 2,  n_conv_2 = 512, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.1,\n",
      "          n_dense_1 = 256, dropout_1 = 0.3,\n",
      "          n_dense_2 = 256, dropout_2 = 0.3, activation_conv=  elu,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 10s 67ms/step - loss: 1.0480 - accuracy: 0.5544\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.8434 - accuracy: 0.6458\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 10s 65ms/step - loss: 0.7419 - accuracy: 0.6935\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.6873 - accuracy: 0.7189\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.6328 - accuracy: 0.7459\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.5879 - accuracy: 0.7675\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.5440 - accuracy: 0.7864\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.5429 - accuracy: 0.7867\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.5097 - accuracy: 0.80530s - loss: 0.5104 - accuracy: \n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.4710 - accuracy: 0.8227\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.4584 - accuracy: 0.8244\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.4452 - accuracy: 0.8321\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.4234 - accuracy: 0.8414\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.4216 - accuracy: 0.8395\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.4030 - accuracy: 0.8470\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3954 - accuracy: 0.8495\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3974 - accuracy: 0.8456\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3833 - accuracy: 0.8521\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3819 - accuracy: 0.8525\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3645 - accuracy: 0.8611\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3649 - accuracy: 0.8572\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3562 - accuracy: 0.8598\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3413 - accuracy: 0.8666\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3261 - accuracy: 0.8713\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3274 - accuracy: 0.8724\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3454 - accuracy: 0.8686\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3255 - accuracy: 0.8720\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3202 - accuracy: 0.8766\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3299 - accuracy: 0.8725\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3137 - accuracy: 0.8784\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3021 - accuracy: 0.8826\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3110 - accuracy: 0.8797\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3005 - accuracy: 0.8831\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3002 - accuracy: 0.8815\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3038 - accuracy: 0.88170s - loss: 0.3026 - ac\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2869 - accuracy: 0.8902\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2876 - accuracy: 0.8872\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2837 - accuracy: 0.8875\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2796 - accuracy: 0.8917\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2830 - accuracy: 0.8907\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2682 - accuracy: 0.8962\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2625 - accuracy: 0.9021\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2726 - accuracy: 0.8953\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2748 - accuracy: 0.8926\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2635 - accuracy: 0.8991\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2735 - accuracy: 0.8946\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2704 - accuracy: 0.8954\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2688 - accuracy: 0.8977\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2626 - accuracy: 0.8992\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2539 - accuracy: 0.9037\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2636 - accuracy: 0.8992\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2564 - accuracy: 0.9021\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2636 - accuracy: 0.8984\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2519 - accuracy: 0.9018\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2483 - accuracy: 0.9029\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2333 - accuracy: 0.9091\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2594 - accuracy: 0.8986\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2433 - accuracy: 0.9034\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2514 - accuracy: 0.9046\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2521 - accuracy: 0.9021\n",
      "76/76 [==============================] - 2s 24ms/step - loss: 0.3595 - accuracy: 0.8803\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 89, 768)           5376      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 29, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 27, 512)           1180160   \n",
      "_________________________________________________________________\n",
      "average_pooling1d_20 (Averag (None, 13, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 11, 512)           786944    \n",
      "_________________________________________________________________\n",
      "lstm_37 (LSTM)               (None, 11, 128)           328192    \n",
      "_________________________________________________________________\n",
      "lstm_38 (LSTM)               (None, 11, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 2816)              0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 256)               721152    \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 3,483,398\n",
      "Trainable params: 3,483,398\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 2,  n_conv_2 = 512, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.1,\n",
      "          n_dense_1 = 256, dropout_1 = 0.3,\n",
      "          n_dense_2 = 256, dropout_2 = 0.3, activation_conv=  elu,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 10s 64ms/step - loss: 1.0630 - accuracy: 0.5522\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 10s 65ms/step - loss: 0.8311 - accuracy: 0.6517\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 10s 65ms/step - loss: 0.7315 - accuracy: 0.7027\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 10s 65ms/step - loss: 0.6734 - accuracy: 0.7294\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.6338 - accuracy: 0.7473\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 10s 65ms/step - loss: 0.5865 - accuracy: 0.7687\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 10s 65ms/step - loss: 0.5595 - accuracy: 0.7818\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.5252 - accuracy: 0.7925\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.4919 - accuracy: 0.8093\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.4673 - accuracy: 0.8211\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.4561 - accuracy: 0.8253\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.4328 - accuracy: 0.8358\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.4303 - accuracy: 0.8370\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.4009 - accuracy: 0.8489\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 10s 65ms/step - loss: 0.3839 - accuracy: 0.8551\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3850 - accuracy: 0.8543\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3750 - accuracy: 0.85782s - loss: 0\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3810 - accuracy: 0.8562\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3602 - accuracy: 0.8612\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3470 - accuracy: 0.8656\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3564 - accuracy: 0.8644\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3479 - accuracy: 0.8657\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3434 - accuracy: 0.8686\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3258 - accuracy: 0.8741\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3265 - accuracy: 0.8734\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 10s 65ms/step - loss: 0.3096 - accuracy: 0.8799\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3106 - accuracy: 0.8802\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 10s 65ms/step - loss: 0.3008 - accuracy: 0.8835\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3145 - accuracy: 0.8781\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2972 - accuracy: 0.8852\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 10s 65ms/step - loss: 0.3306 - accuracy: 0.8757\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3075 - accuracy: 0.8811\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2995 - accuracy: 0.8827\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2877 - accuracy: 0.8869\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2863 - accuracy: 0.8890\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2736 - accuracy: 0.8952\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2954 - accuracy: 0.8858\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2813 - accuracy: 0.8933\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2762 - accuracy: 0.8952\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2753 - accuracy: 0.8936\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2771 - accuracy: 0.8931\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2722 - accuracy: 0.8968\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2699 - accuracy: 0.8978\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2743 - accuracy: 0.8939\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2621 - accuracy: 0.8986\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2576 - accuracy: 0.8999\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2766 - accuracy: 0.8931\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2492 - accuracy: 0.9046\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2611 - accuracy: 0.9000\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2494 - accuracy: 0.9033\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2726 - accuracy: 0.8942\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2657 - accuracy: 0.8957\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2637 - accuracy: 0.8974\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2588 - accuracy: 0.8981\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2581 - accuracy: 0.8972\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2612 - accuracy: 0.8964\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2459 - accuracy: 0.9038\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2436 - accuracy: 0.9081\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2550 - accuracy: 0.9036\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2481 - accuracy: 0.9060\n",
      "76/76 [==============================] - 2s 23ms/step - loss: 0.3110 - accuracy: 0.8981\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_63 (Conv1D)           (None, 89, 768)           5376      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 29, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 27, 512)           1180160   \n",
      "_________________________________________________________________\n",
      "average_pooling1d_21 (Averag (None, 13, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 11, 512)           786944    \n",
      "_________________________________________________________________\n",
      "lstm_39 (LSTM)               (None, 11, 128)           328192    \n",
      "_________________________________________________________________\n",
      "lstm_40 (LSTM)               (None, 11, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 2816)              0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 256)               721152    \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 3,483,398\n",
      "Trainable params: 3,483,398\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 2,  n_conv_2 = 512, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.1,\n",
      "          n_dense_1 = 256, dropout_1 = 0.3,\n",
      "          n_dense_2 = 256, dropout_2 = 0.3, activation_conv=  elu,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 10s 64ms/step - loss: 1.0569 - accuracy: 0.5497\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.8555 - accuracy: 0.63660s - loss: 0.8559 - accuracy: 0.\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.7460 - accuracy: 0.6914\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.6644 - accuracy: 0.7339\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.6263 - accuracy: 0.7536\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.5710 - accuracy: 0.7794\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.5404 - accuracy: 0.7914\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.5167 - accuracy: 0.7983\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.4913 - accuracy: 0.8107\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.4810 - accuracy: 0.8195\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.4473 - accuracy: 0.8306\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.4197 - accuracy: 0.8414\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.4162 - accuracy: 0.8407\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3892 - accuracy: 0.8535\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3876 - accuracy: 0.8544\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3630 - accuracy: 0.8648\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3789 - accuracy: 0.8548\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3634 - accuracy: 0.86080s - loss: 0.364\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3607 - accuracy: 0.8594\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3531 - accuracy: 0.8659\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3389 - accuracy: 0.8675\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3365 - accuracy: 0.8720\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3309 - accuracy: 0.8731\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3427 - accuracy: 0.8679\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3202 - accuracy: 0.8787\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3191 - accuracy: 0.8813\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3106 - accuracy: 0.8821\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2990 - accuracy: 0.8845\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2939 - accuracy: 0.8890\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.3066 - accuracy: 0.8808\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2928 - accuracy: 0.8870\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2902 - accuracy: 0.8880\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2950 - accuracy: 0.8863\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2920 - accuracy: 0.8865\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 10s 65ms/step - loss: 0.2834 - accuracy: 0.8899\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2860 - accuracy: 0.8895\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2891 - accuracy: 0.8874\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2768 - accuracy: 0.8941\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2798 - accuracy: 0.8916\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2733 - accuracy: 0.8960\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2800 - accuracy: 0.8934\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2631 - accuracy: 0.9000\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2699 - accuracy: 0.8942\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2696 - accuracy: 0.8950\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2621 - accuracy: 0.8956\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2532 - accuracy: 0.9005\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2467 - accuracy: 0.9032\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2611 - accuracy: 0.8983\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2638 - accuracy: 0.8967\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2553 - accuracy: 0.9004\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2428 - accuracy: 0.9063\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2543 - accuracy: 0.9042\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2544 - accuracy: 0.9010\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2458 - accuracy: 0.90750s - loss: 0.2452 \n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2483 - accuracy: 0.9070\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2552 - accuracy: 0.9032\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2409 - accuracy: 0.9076\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2309 - accuracy: 0.9102\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2541 - accuracy: 0.9028\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 10s 64ms/step - loss: 0.2547 - accuracy: 0.9009\n",
      "76/76 [==============================] - 2s 23ms/step - loss: 0.3187 - accuracy: 0.8946\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_66 (Conv1D)           (None, 89, 768)           5376      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 29, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 27, 256)           590080    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_22 (Averag (None, 13, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 12, 512)           262656    \n",
      "_________________________________________________________________\n",
      "lstm_41 (LSTM)               (None, 12, 128)           328192    \n",
      "_________________________________________________________________\n",
      "lstm_42 (LSTM)               (None, 12, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 256)               786688    \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 384)               98688     \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 2,468,230\n",
      "Trainable params: 2,468,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 2,  n_conv_2 = 256, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 2,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.3,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  elu,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 8s 52ms/step - loss: 1.0808 - accuracy: 0.5404\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.8322 - accuracy: 0.6452\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.7149 - accuracy: 0.6999\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.6472 - accuracy: 0.7346\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.5523 - accuracy: 0.7776\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 8s 50ms/step - loss: 0.5076 - accuracy: 0.8031\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 8s 50ms/step - loss: 0.4655 - accuracy: 0.8193\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 8s 50ms/step - loss: 0.4363 - accuracy: 0.8334\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.4039 - accuracy: 0.8464\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.3688 - accuracy: 0.8597\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.3600 - accuracy: 0.8640\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.3470 - accuracy: 0.8695\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.3328 - accuracy: 0.8721\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.3261 - accuracy: 0.8792\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.3293 - accuracy: 0.8738\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2899 - accuracy: 0.8906\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 8s 50ms/step - loss: 0.2885 - accuracy: 0.8917\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2847 - accuracy: 0.8939\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 8s 50ms/step - loss: 0.2771 - accuracy: 0.8955\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2667 - accuracy: 0.8994\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.2584 - accuracy: 0.9041\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 8s 50ms/step - loss: 0.2463 - accuracy: 0.9090\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2423 - accuracy: 0.9088\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 8s 50ms/step - loss: 0.2376 - accuracy: 0.9106\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2413 - accuracy: 0.9082\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.2251 - accuracy: 0.9150\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2210 - accuracy: 0.9183 0s - loss: 0.2207 - ac\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2202 - accuracy: 0.9173\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.2168 - accuracy: 0.9178\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2169 - accuracy: 0.9197\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.2035 - accuracy: 0.9229\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.1993 - accuracy: 0.9251\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.1973 - accuracy: 0.9268\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1935 - accuracy: 0.9277\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.1996 - accuracy: 0.9250\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.1920 - accuracy: 0.9295\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.2044 - accuracy: 0.9251\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 8s 50ms/step - loss: 0.1823 - accuracy: 0.9328\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.1847 - accuracy: 0.9318\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.1797 - accuracy: 0.9332\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1830 - accuracy: 0.9332\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.1876 - accuracy: 0.9314\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1829 - accuracy: 0.9317\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.1849 - accuracy: 0.9308\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.1720 - accuracy: 0.9345\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.1741 - accuracy: 0.9355\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1859 - accuracy: 0.9317\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1655 - accuracy: 0.9398\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1726 - accuracy: 0.9372\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.1728 - accuracy: 0.9359\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1622 - accuracy: 0.9402\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1686 - accuracy: 0.9367\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.1574 - accuracy: 0.9427\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1481 - accuracy: 0.9449\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1606 - accuracy: 0.9407\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1420 - accuracy: 0.9468\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1576 - accuracy: 0.9420\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1430 - accuracy: 0.9481\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1482 - accuracy: 0.9461\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1419 - accuracy: 0.9471\n",
      "76/76 [==============================] - 2s 20ms/step - loss: 0.3004 - accuracy: 0.9123\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_69 (Conv1D)           (None, 89, 768)           5376      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 29, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 27, 256)           590080    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_23 (Averag (None, 13, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 12, 512)           262656    \n",
      "_________________________________________________________________\n",
      "lstm_43 (LSTM)               (None, 12, 128)           328192    \n",
      "_________________________________________________________________\n",
      "lstm_44 (LSTM)               (None, 12, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 256)               786688    \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 384)               98688     \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 2,468,230\n",
      "Trainable params: 2,468,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 2,  n_conv_2 = 256, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 2,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.3,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  elu,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 7s 49ms/step - loss: 1.0890 - accuracy: 0.5391\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.8366 - accuracy: 0.6494\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.7057 - accuracy: 0.7073\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.6131 - accuracy: 0.7532\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.5337 - accuracy: 0.7920\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.4964 - accuracy: 0.8081\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.4451 - accuracy: 0.8270\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.4091 - accuracy: 0.8454\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.4087 - accuracy: 0.8445\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.3795 - accuracy: 0.8546\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.3612 - accuracy: 0.8651\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.3434 - accuracy: 0.8696\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.3262 - accuracy: 0.8743\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.3241 - accuracy: 0.8757\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.2993 - accuracy: 0.8847\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.2952 - accuracy: 0.8886\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2926 - accuracy: 0.8895\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.2730 - accuracy: 0.8951\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2779 - accuracy: 0.8956 0s - loss: 0.2771 - accura\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 8s 50ms/step - loss: 0.2753 - accuracy: 0.8942\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.2603 - accuracy: 0.8982\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.2647 - accuracy: 0.8968\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.2427 - accuracy: 0.9063\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2540 - accuracy: 0.9009\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2295 - accuracy: 0.9121\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2422 - accuracy: 0.9038\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2271 - accuracy: 0.9122\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.2366 - accuracy: 0.9097\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2232 - accuracy: 0.9143\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.2116 - accuracy: 0.9179\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.2326 - accuracy: 0.9090\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2062 - accuracy: 0.9191\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2083 - accuracy: 0.9202\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.2069 - accuracy: 0.9206\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1991 - accuracy: 0.9246\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.1937 - accuracy: 0.9271\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1990 - accuracy: 0.9233\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.1870 - accuracy: 0.9278\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1924 - accuracy: 0.9256\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.1935 - accuracy: 0.9250\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.1889 - accuracy: 0.9297\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1937 - accuracy: 0.9271\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1833 - accuracy: 0.9309\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1598 - accuracy: 0.9391\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1710 - accuracy: 0.9362\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1793 - accuracy: 0.9342\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1764 - accuracy: 0.9343\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.1797 - accuracy: 0.9333\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.1694 - accuracy: 0.9362\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1744 - accuracy: 0.9342\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.1557 - accuracy: 0.9409\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1669 - accuracy: 0.9366\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1666 - accuracy: 0.9368\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1553 - accuracy: 0.9407\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1505 - accuracy: 0.9443\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.1595 - accuracy: 0.9403\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1525 - accuracy: 0.9411\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1433 - accuracy: 0.9458\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1828 - accuracy: 0.9310\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1495 - accuracy: 0.9437\n",
      "76/76 [==============================] - 1s 18ms/step - loss: 0.3090 - accuracy: 0.9156\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 89, 768)           5376      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 29, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 27, 256)           590080    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_24 (Averag (None, 13, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 12, 512)           262656    \n",
      "_________________________________________________________________\n",
      "lstm_45 (LSTM)               (None, 12, 128)           328192    \n",
      "_________________________________________________________________\n",
      "lstm_46 (LSTM)               (None, 12, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 256)               786688    \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 384)               98688     \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 2,468,230\n",
      "Trainable params: 2,468,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 2,  n_conv_2 = 256, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 2,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.3,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  elu,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 7s 49ms/step - loss: 1.0881 - accuracy: 0.5391\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.8472 - accuracy: 0.6436\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.7146 - accuracy: 0.7056\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.6084 - accuracy: 0.7543\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.5540 - accuracy: 0.7842\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.5072 - accuracy: 0.8025\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 8s 50ms/step - loss: 0.4586 - accuracy: 0.8250\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.4281 - accuracy: 0.8371\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.4166 - accuracy: 0.8422\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.3709 - accuracy: 0.8563\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.3517 - accuracy: 0.8666\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 8s 50ms/step - loss: 0.3490 - accuracy: 0.8670\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.3458 - accuracy: 0.8671\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.3183 - accuracy: 0.8777\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.3041 - accuracy: 0.8845\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2979 - accuracy: 0.8878\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.2897 - accuracy: 0.8915\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.2897 - accuracy: 0.8891\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2643 - accuracy: 0.8985\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2643 - accuracy: 0.9002\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2600 - accuracy: 0.9000\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2594 - accuracy: 0.8986\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2456 - accuracy: 0.9076\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2407 - accuracy: 0.9068\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2585 - accuracy: 0.9020\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2386 - accuracy: 0.9098\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.2339 - accuracy: 0.9103\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2258 - accuracy: 0.9120\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.2326 - accuracy: 0.9128\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2246 - accuracy: 0.9140\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2460 - accuracy: 0.9047\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.2277 - accuracy: 0.9107\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.2066 - accuracy: 0.9204\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 8s 50ms/step - loss: 0.2110 - accuracy: 0.9168\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.2140 - accuracy: 0.9191\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.2092 - accuracy: 0.9182\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2142 - accuracy: 0.9162\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.2206 - accuracy: 0.9147\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.2061 - accuracy: 0.9214 \n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1899 - accuracy: 0.9287\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1811 - accuracy: 0.9308\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1930 - accuracy: 0.9278\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1828 - accuracy: 0.9280\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 8s 50ms/step - loss: 0.2029 - accuracy: 0.9229\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.1907 - accuracy: 0.9253\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1768 - accuracy: 0.9317\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1744 - accuracy: 0.9327\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1706 - accuracy: 0.9349\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1731 - accuracy: 0.9347\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.1761 - accuracy: 0.9315\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1766 - accuracy: 0.9324\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1681 - accuracy: 0.9374\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.1659 - accuracy: 0.9379\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1711 - accuracy: 0.9377\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1837 - accuracy: 0.9302\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1725 - accuracy: 0.9342\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1740 - accuracy: 0.9348\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 8s 49ms/step - loss: 0.1553 - accuracy: 0.9400\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1651 - accuracy: 0.9366\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 7s 49ms/step - loss: 0.1661 - accuracy: 0.9378\n",
      "76/76 [==============================] - 1s 18ms/step - loss: 0.2958 - accuracy: 0.9159\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_75 (Conv1D)           (None, 89, 768)           5376      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 29, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 27, 256)           590080    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_25 (Averag (None, 13, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 11, 256)           196864    \n",
      "_________________________________________________________________\n",
      "lstm_47 (LSTM)               (None, 11, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_48 (LSTM)               (None, 11, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 2816)              0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 256)               721152    \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 2,172,166\n",
      "Trainable params: 2,172,166\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 2,  n_conv_2 = 256, k_conv_2 = 3, \n",
      "          n_conv_3 = 256,  k_conv_3 = 3,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.3,\n",
      "          n_dense_2 = 256, dropout_2 = 0.3, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000002527E6DD448>,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x00000252062F3D48>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 7s 46ms/step - loss: 1.1045 - accuracy: 0.5158\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.8521 - accuracy: 0.6292\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.7348 - accuracy: 0.6804\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.6207 - accuracy: 0.7416\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.5375 - accuracy: 0.7793\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.4748 - accuracy: 0.8094\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.4448 - accuracy: 0.8238\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.4007 - accuracy: 0.8403\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.3743 - accuracy: 0.8517\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.3608 - accuracy: 0.8562\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.3354 - accuracy: 0.8678\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.3172 - accuracy: 0.8729\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.3077 - accuracy: 0.8740\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2983 - accuracy: 0.8836\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2807 - accuracy: 0.8870\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2760 - accuracy: 0.8892\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2656 - accuracy: 0.8943\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2508 - accuracy: 0.8986\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2340 - accuracy: 0.9063\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2267 - accuracy: 0.9130\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2317 - accuracy: 0.9094\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2163 - accuracy: 0.9159\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1990 - accuracy: 0.9246\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2181 - accuracy: 0.9168\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1974 - accuracy: 0.9256\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1969 - accuracy: 0.9233\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1803 - accuracy: 0.9330\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1688 - accuracy: 0.9360\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1704 - accuracy: 0.9358\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1570 - accuracy: 0.9400\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1551 - accuracy: 0.9412\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1597 - accuracy: 0.9403\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1494 - accuracy: 0.9440\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1509 - accuracy: 0.9436\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1467 - accuracy: 0.9442\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1423 - accuracy: 0.9458\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1578 - accuracy: 0.9425\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1361 - accuracy: 0.9479\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1364 - accuracy: 0.9493\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1250 - accuracy: 0.9533\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1290 - accuracy: 0.9512\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1289 - accuracy: 0.9515\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1239 - accuracy: 0.9535\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1159 - accuracy: 0.9570\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1137 - accuracy: 0.9571\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1258 - accuracy: 0.9531\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1164 - accuracy: 0.9562\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1074 - accuracy: 0.9593\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1087 - accuracy: 0.9601\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.0996 - accuracy: 0.9624\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1166 - accuracy: 0.9576\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1076 - accuracy: 0.9599\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1025 - accuracy: 0.9621\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1121 - accuracy: 0.9582\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.0989 - accuracy: 0.9626\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.0970 - accuracy: 0.9636\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1022 - accuracy: 0.9629\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.0949 - accuracy: 0.9642\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1015 - accuracy: 0.9627\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1033 - accuracy: 0.9624\n",
      "76/76 [==============================] - 1s 18ms/step - loss: 0.3224 - accuracy: 0.9297\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_78 (Conv1D)           (None, 89, 768)           5376      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 29, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 27, 256)           590080    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_26 (Averag (None, 13, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 11, 256)           196864    \n",
      "_________________________________________________________________\n",
      "lstm_49 (LSTM)               (None, 11, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_50 (LSTM)               (None, 11, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_24 (Flatten)         (None, 2816)              0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 256)               721152    \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 2,172,166\n",
      "Trainable params: 2,172,166\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 2,  n_conv_2 = 256, k_conv_2 = 3, \n",
      "          n_conv_3 = 256,  k_conv_3 = 3,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.3,\n",
      "          n_dense_2 = 256, dropout_2 = 0.3, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000002527E6DD448>,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x00000252062F3D48>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 7s 46ms/step - loss: 1.0925 - accuracy: 0.5254\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.8617 - accuracy: 0.6281\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.7458 - accuracy: 0.6820\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.6076 - accuracy: 0.7488\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.5224 - accuracy: 0.7885\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.4634 - accuracy: 0.8122\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.4180 - accuracy: 0.8348\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.3896 - accuracy: 0.8452\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.3631 - accuracy: 0.8539\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.3728 - accuracy: 0.8533\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.3341 - accuracy: 0.8656\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.3168 - accuracy: 0.8748\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.3045 - accuracy: 0.8757\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2867 - accuracy: 0.8838\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2918 - accuracy: 0.8856\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2673 - accuracy: 0.8920\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2578 - accuracy: 0.8972\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2363 - accuracy: 0.9081\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2418 - accuracy: 0.9035\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2258 - accuracy: 0.9120\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2183 - accuracy: 0.9151\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2094 - accuracy: 0.9201\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2006 - accuracy: 0.9224\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1842 - accuracy: 0.9289\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1897 - accuracy: 0.9283\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1781 - accuracy: 0.9323\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1677 - accuracy: 0.9363\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1675 - accuracy: 0.9352\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1698 - accuracy: 0.9350\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1480 - accuracy: 0.9443\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1468 - accuracy: 0.9447\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1485 - accuracy: 0.9438\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1463 - accuracy: 0.9444\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1506 - accuracy: 0.9426\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1414 - accuracy: 0.9452\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1267 - accuracy: 0.9510\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1329 - accuracy: 0.9501\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1242 - accuracy: 0.9535\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1182 - accuracy: 0.9557\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1198 - accuracy: 0.9560\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1297 - accuracy: 0.9513\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1334 - accuracy: 0.9498\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1301 - accuracy: 0.9517\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1178 - accuracy: 0.9554\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1214 - accuracy: 0.9550\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1208 - accuracy: 0.9561\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1142 - accuracy: 0.9568\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1081 - accuracy: 0.9589\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1075 - accuracy: 0.9596\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1154 - accuracy: 0.9569\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1102 - accuracy: 0.9587\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1009 - accuracy: 0.9616\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.0943 - accuracy: 0.9642\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1034 - accuracy: 0.9613\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1169 - accuracy: 0.9563\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1021 - accuracy: 0.9605\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.0944 - accuracy: 0.9645\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.0910 - accuracy: 0.9666\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.0912 - accuracy: 0.9649\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.0991 - accuracy: 0.9628\n",
      "76/76 [==============================] - 1s 17ms/step - loss: 0.3713 - accuracy: 0.9191\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_81 (Conv1D)           (None, 89, 768)           5376      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 29, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 27, 256)           590080    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_27 (Averag (None, 13, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 11, 256)           196864    \n",
      "_________________________________________________________________\n",
      "lstm_51 (LSTM)               (None, 11, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_52 (LSTM)               (None, 11, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_25 (Flatten)         (None, 2816)              0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 256)               721152    \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 2,172,166\n",
      "Trainable params: 2,172,166\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 2,  n_conv_2 = 256, k_conv_2 = 3, \n",
      "          n_conv_3 = 256,  k_conv_3 = 3,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.3,\n",
      "          n_dense_2 = 256, dropout_2 = 0.3, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000002527E6DD448>,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x00000252062F3D48>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 7s 46ms/step - loss: 1.0969 - accuracy: 0.5250\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.8703 - accuracy: 0.6255\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.7589 - accuracy: 0.6749\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.6110 - accuracy: 0.7480\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.5256 - accuracy: 0.7867\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.4712 - accuracy: 0.8117\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.4406 - accuracy: 0.8234\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.4026 - accuracy: 0.8404\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.3727 - accuracy: 0.8548\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.3588 - accuracy: 0.8600\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.3390 - accuracy: 0.8647\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.3238 - accuracy: 0.8733\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.3124 - accuracy: 0.8758\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2868 - accuracy: 0.8877\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2799 - accuracy: 0.8876\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2762 - accuracy: 0.8903\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2571 - accuracy: 0.8972\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2596 - accuracy: 0.8949\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2391 - accuracy: 0.9067\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2343 - accuracy: 0.9087\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2388 - accuracy: 0.9081\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2121 - accuracy: 0.9190\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.2037 - accuracy: 0.9245\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1812 - accuracy: 0.9317\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1994 - accuracy: 0.9239\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1801 - accuracy: 0.9323\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1714 - accuracy: 0.9355\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1776 - accuracy: 0.9334\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1593 - accuracy: 0.9397\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1720 - accuracy: 0.9359\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1571 - accuracy: 0.9407\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1558 - accuracy: 0.9408\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1578 - accuracy: 0.9388\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1579 - accuracy: 0.9403\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1362 - accuracy: 0.9501\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1389 - accuracy: 0.9468\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1272 - accuracy: 0.9512\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1389 - accuracy: 0.9474\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1408 - accuracy: 0.9484\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1270 - accuracy: 0.9519\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1179 - accuracy: 0.9557\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1184 - accuracy: 0.9559\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1174 - accuracy: 0.9554\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1231 - accuracy: 0.9545\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1135 - accuracy: 0.9568\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1157 - accuracy: 0.9567\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1429 - accuracy: 0.9466\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1173 - accuracy: 0.9565\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1068 - accuracy: 0.9592\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1223 - accuracy: 0.9537\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1081 - accuracy: 0.9598\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.0976 - accuracy: 0.9627\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1018 - accuracy: 0.9622\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1017 - accuracy: 0.9627\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1019 - accuracy: 0.9621\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1128 - accuracy: 0.9577\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.0986 - accuracy: 0.9639\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.0890 - accuracy: 0.9653\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.0985 - accuracy: 0.9628\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 0.1090 - accuracy: 0.9595\n",
      "76/76 [==============================] - 1s 17ms/step - loss: 0.3337 - accuracy: 0.9317\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_84 (Conv1D)           (None, 89, 768)           5376      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 29, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 28, 256)           393472    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_28 (Averag (None, 9, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 8, 512)            262656    \n",
      "_________________________________________________________________\n",
      "lstm_53 (LSTM)               (None, 8, 256)            787456    \n",
      "_________________________________________________________________\n",
      "lstm_54 (LSTM)               (None, 8, 128)            197120    \n",
      "_________________________________________________________________\n",
      "flatten_26 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 384)               393600    \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 384)               147840    \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 2,189,830\n",
      "Trainable params: 2,189,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 2,  n_conv_2 = 256, k_conv_2 = 2, \n",
      "          n_conv_3 = 512,  k_conv_3 = 2,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 3,\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.1,\n",
      "          n_dense_1 = 384, dropout_1 = 0.2,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000002527E6DD448>,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 7s 46ms/step - loss: 1.0992 - accuracy: 0.5187\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.8932 - accuracy: 0.6101\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.7599 - accuracy: 0.6759\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.6520 - accuracy: 0.7300\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.5692 - accuracy: 0.7666\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.4971 - accuracy: 0.8002\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.4481 - accuracy: 0.8218\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.4018 - accuracy: 0.8396\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.3757 - accuracy: 0.8510\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.3580 - accuracy: 0.8591\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.3427 - accuracy: 0.8620\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.3260 - accuracy: 0.8699\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2959 - accuracy: 0.8821\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2817 - accuracy: 0.8891\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2767 - accuracy: 0.8879\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2587 - accuracy: 0.8958\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2555 - accuracy: 0.8966\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2374 - accuracy: 0.9052\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2254 - accuracy: 0.9124\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2118 - accuracy: 0.9178\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2094 - accuracy: 0.9185\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2015 - accuracy: 0.9230\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1814 - accuracy: 0.9309\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1864 - accuracy: 0.9287\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1818 - accuracy: 0.9301\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1569 - accuracy: 0.9420\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1572 - accuracy: 0.9405\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1654 - accuracy: 0.9380\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1520 - accuracy: 0.9437\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1530 - accuracy: 0.9435\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1464 - accuracy: 0.9439\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1418 - accuracy: 0.9488\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1363 - accuracy: 0.9499\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1451 - accuracy: 0.9463\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1442 - accuracy: 0.9454\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1239 - accuracy: 0.9532\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1317 - accuracy: 0.9511\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1297 - accuracy: 0.9515\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1162 - accuracy: 0.9562\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1262 - accuracy: 0.9528\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1167 - accuracy: 0.9570\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1320 - accuracy: 0.9496\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1110 - accuracy: 0.9577\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1254 - accuracy: 0.9533\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1209 - accuracy: 0.9539\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1053 - accuracy: 0.9610\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1069 - accuracy: 0.9603\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1044 - accuracy: 0.9604\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1099 - accuracy: 0.9591\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.0919 - accuracy: 0.9656\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.0935 - accuracy: 0.9649\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1041 - accuracy: 0.9615\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1067 - accuracy: 0.9606\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.0973 - accuracy: 0.9645\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.0890 - accuracy: 0.9657\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1008 - accuracy: 0.9634\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.0932 - accuracy: 0.9650\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.0919 - accuracy: 0.9658\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.0988 - accuracy: 0.9647\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.0906 - accuracy: 0.9664\n",
      "76/76 [==============================] - 1s 19ms/step - loss: 0.3309 - accuracy: 0.9281\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_87 (Conv1D)           (None, 89, 768)           5376      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 29, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 28, 256)           393472    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_29 (Averag (None, 9, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 8, 512)            262656    \n",
      "_________________________________________________________________\n",
      "lstm_55 (LSTM)               (None, 8, 256)            787456    \n",
      "_________________________________________________________________\n",
      "lstm_56 (LSTM)               (None, 8, 128)            197120    \n",
      "_________________________________________________________________\n",
      "flatten_27 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 384)               393600    \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 384)               147840    \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 2,189,830\n",
      "Trainable params: 2,189,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 2,  n_conv_2 = 256, k_conv_2 = 2, \n",
      "          n_conv_3 = 512,  k_conv_3 = 2,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 3,\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.1,\n",
      "          n_dense_1 = 384, dropout_1 = 0.2,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000002527E6DD448>,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 7s 43ms/step - loss: 1.1060 - accuracy: 0.5203\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.9056 - accuracy: 0.6018\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.7736 - accuracy: 0.6695\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.6302 - accuracy: 0.7351\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.5541 - accuracy: 0.7730\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.4648 - accuracy: 0.8117\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.4202 - accuracy: 0.8309\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.3923 - accuracy: 0.8426\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.3690 - accuracy: 0.8533\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.3506 - accuracy: 0.8587\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.3184 - accuracy: 0.8725\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.3087 - accuracy: 0.8775\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2987 - accuracy: 0.8800\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2720 - accuracy: 0.8898\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2618 - accuracy: 0.8963\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2585 - accuracy: 0.8949\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2486 - accuracy: 0.9005\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2332 - accuracy: 0.9072\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2336 - accuracy: 0.9067\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2189 - accuracy: 0.9152\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2059 - accuracy: 0.9203\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2099 - accuracy: 0.9183\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1933 - accuracy: 0.9256\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1796 - accuracy: 0.9301\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1708 - accuracy: 0.9344\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1733 - accuracy: 0.9334\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1677 - accuracy: 0.9349\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1441 - accuracy: 0.9443\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1620 - accuracy: 0.9397\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1444 - accuracy: 0.9445\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1424 - accuracy: 0.9459\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1368 - accuracy: 0.9500\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1410 - accuracy: 0.9478\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1295 - accuracy: 0.9517\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1319 - accuracy: 0.9505\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1242 - accuracy: 0.9540\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1317 - accuracy: 0.9507\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1177 - accuracy: 0.9550\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1122 - accuracy: 0.9585\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1277 - accuracy: 0.9534\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1457 - accuracy: 0.9452\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1096 - accuracy: 0.9581\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1216 - accuracy: 0.9548\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1130 - accuracy: 0.9581\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1149 - accuracy: 0.9579\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1163 - accuracy: 0.9567\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1116 - accuracy: 0.9583\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.0964 - accuracy: 0.9640\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1210 - accuracy: 0.9549\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1024 - accuracy: 0.9610\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1062 - accuracy: 0.9597\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.0942 - accuracy: 0.9646\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.0986 - accuracy: 0.9622\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1091 - accuracy: 0.9601\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.0902 - accuracy: 0.9656\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.0997 - accuracy: 0.9636\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.0978 - accuracy: 0.9644\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.0951 - accuracy: 0.9650\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.0912 - accuracy: 0.9657\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.0819 - accuracy: 0.9689\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 0.3717 - accuracy: 0.9243\n",
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 89, 768)           5376      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 29, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 28, 256)           393472    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_30 (Averag (None, 9, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 8, 512)            262656    \n",
      "_________________________________________________________________\n",
      "lstm_57 (LSTM)               (None, 8, 256)            787456    \n",
      "_________________________________________________________________\n",
      "lstm_58 (LSTM)               (None, 8, 128)            197120    \n",
      "_________________________________________________________________\n",
      "flatten_28 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 384)               393600    \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 384)               147840    \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 2,189,830\n",
      "Trainable params: 2,189,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 2,  n_conv_2 = 256, k_conv_2 = 2, \n",
      "          n_conv_3 = 512,  k_conv_3 = 2,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 3,\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.1,\n",
      "          n_dense_1 = 384, dropout_1 = 0.2,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000002527E6DD448>,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 7s 43ms/step - loss: 1.0906 - accuracy: 0.5274\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.8948 - accuracy: 0.6099\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.7854 - accuracy: 0.6629\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.6539 - accuracy: 0.7268\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.5527 - accuracy: 0.7772\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.4826 - accuracy: 0.8088\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.4464 - accuracy: 0.8218\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.4092 - accuracy: 0.8360\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.3815 - accuracy: 0.8478\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.3554 - accuracy: 0.8597\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.3383 - accuracy: 0.8644\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.3185 - accuracy: 0.8722\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.3071 - accuracy: 0.8793\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2988 - accuracy: 0.8794\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2735 - accuracy: 0.8918\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2727 - accuracy: 0.8916\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2598 - accuracy: 0.8953\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2482 - accuracy: 0.8998\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2327 - accuracy: 0.9056\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2224 - accuracy: 0.9107\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2263 - accuracy: 0.9103\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2058 - accuracy: 0.9191\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.2005 - accuracy: 0.9224\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1938 - accuracy: 0.9252\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1887 - accuracy: 0.9283\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1794 - accuracy: 0.9302\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1812 - accuracy: 0.9311\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1677 - accuracy: 0.9357\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1639 - accuracy: 0.9378\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1598 - accuracy: 0.9400\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1503 - accuracy: 0.9435\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1472 - accuracy: 0.9459\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1436 - accuracy: 0.9459\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1404 - accuracy: 0.9484\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1376 - accuracy: 0.9481\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1365 - accuracy: 0.9489\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1315 - accuracy: 0.9498\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1282 - accuracy: 0.9520\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1392 - accuracy: 0.9477\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1333 - accuracy: 0.9509\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1246 - accuracy: 0.9534\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1224 - accuracy: 0.9538\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1227 - accuracy: 0.9542\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1193 - accuracy: 0.9561\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1209 - accuracy: 0.9551\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1155 - accuracy: 0.9571\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1058 - accuracy: 0.9602\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1091 - accuracy: 0.9597\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1136 - accuracy: 0.9578\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1161 - accuracy: 0.9572\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1055 - accuracy: 0.9601\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1110 - accuracy: 0.9580\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1067 - accuracy: 0.9606\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1111 - accuracy: 0.9595\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1029 - accuracy: 0.9616\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1055 - accuracy: 0.9600\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.0982 - accuracy: 0.9626\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1043 - accuracy: 0.9617\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.0885 - accuracy: 0.9672\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.0894 - accuracy: 0.9666\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 0.3252 - accuracy: 0.9273\n",
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_93 (Conv1D)           (None, 89, 768)           5376      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 44, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 42, 512)           1180160   \n",
      "_________________________________________________________________\n",
      "average_pooling1d_31 (Averag (None, 21, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 19, 512)           786944    \n",
      "_________________________________________________________________\n",
      "lstm_59 (LSTM)               (None, 19, 256)           787456    \n",
      "_________________________________________________________________\n",
      "lstm_60 (LSTM)               (None, 19, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_29 (Flatten)         (None, 4864)              0         \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 256)               1245440   \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 4,598,022\n",
      "Trainable params: 4,598,022\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 2,  n_conv_2 = 512, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.1,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 256, dropout_2 = 0.3, activation_conv=  elu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x00000252062F3D48>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 15s 97ms/step - loss: 1.0731 - accuracy: 0.5370\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.8097 - accuracy: 0.6553\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.6660 - accuracy: 0.7232\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.5710 - accuracy: 0.7725\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.5072 - accuracy: 0.7997\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.4536 - accuracy: 0.8248\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.4398 - accuracy: 0.8299\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.3933 - accuracy: 0.8478\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.3682 - accuracy: 0.8593\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.3535 - accuracy: 0.8633\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.3280 - accuracy: 0.8721\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.3268 - accuracy: 0.8719\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.3059 - accuracy: 0.8805\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2882 - accuracy: 0.8881\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2874 - accuracy: 0.8875\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2949 - accuracy: 0.8834\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2694 - accuracy: 0.8955\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2494 - accuracy: 0.9036\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2627 - accuracy: 0.8989\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2410 - accuracy: 0.9058\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2514 - accuracy: 0.9012\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2351 - accuracy: 0.9087\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2249 - accuracy: 0.9130\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2357 - accuracy: 0.9095\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2352 - accuracy: 0.9083\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2117 - accuracy: 0.9182\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2121 - accuracy: 0.9176\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2104 - accuracy: 0.9177\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2016 - accuracy: 0.9230\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2001 - accuracy: 0.9229\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1931 - accuracy: 0.92730s - loss: 0.1924 - accuracy: \n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1909 - accuracy: 0.9285\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1901 - accuracy: 0.9266\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1811 - accuracy: 0.9319\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1915 - accuracy: 0.9282\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1741 - accuracy: 0.9332\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1771 - accuracy: 0.9335\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1863 - accuracy: 0.9307\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1675 - accuracy: 0.9359\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1689 - accuracy: 0.9360\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1623 - accuracy: 0.9383\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1572 - accuracy: 0.9400\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1661 - accuracy: 0.93870s - loss: 0.1661 - accuracy: 0.93\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1675 - accuracy: 0.9363\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1549 - accuracy: 0.94080s - loss: 0.1555 - accu\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1617 - accuracy: 0.93840s - loss: 0.1623 - accura\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1504 - accuracy: 0.9438\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1573 - accuracy: 0.94070s - loss: 0.1574 - accuracy: 0.\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1540 - accuracy: 0.9440\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1478 - accuracy: 0.9451\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1479 - accuracy: 0.9449\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1573 - accuracy: 0.9393\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1453 - accuracy: 0.9461\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1345 - accuracy: 0.9482\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1522 - accuracy: 0.9438\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1433 - accuracy: 0.9484\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1373 - accuracy: 0.9483\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1497 - accuracy: 0.9448\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1451 - accuracy: 0.9461\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1374 - accuracy: 0.9484\n",
      "76/76 [==============================] - 3s 34ms/step - loss: 0.3003 - accuracy: 0.9165\n",
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_96 (Conv1D)           (None, 89, 768)           5376      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 44, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 42, 512)           1180160   \n",
      "_________________________________________________________________\n",
      "average_pooling1d_32 (Averag (None, 21, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 19, 512)           786944    \n",
      "_________________________________________________________________\n",
      "lstm_61 (LSTM)               (None, 19, 256)           787456    \n",
      "_________________________________________________________________\n",
      "lstm_62 (LSTM)               (None, 19, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_30 (Flatten)         (None, 4864)              0         \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 256)               1245440   \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_61 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 4,598,022\n",
      "Trainable params: 4,598,022\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 2,  n_conv_2 = 512, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.1,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 256, dropout_2 = 0.3, activation_conv=  elu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x00000252062F3D48>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 15s 97ms/step - loss: 1.0744 - accuracy: 0.5387\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.7883 - accuracy: 0.6702\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.6606 - accuracy: 0.7280\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.5657 - accuracy: 0.7748\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.5000 - accuracy: 0.8045\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.4580 - accuracy: 0.8237\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.4086 - accuracy: 0.8422\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.3850 - accuracy: 0.8544\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.3605 - accuracy: 0.8617\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.3469 - accuracy: 0.8655\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.3152 - accuracy: 0.8779\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.3076 - accuracy: 0.8796\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2881 - accuracy: 0.8889\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2843 - accuracy: 0.8915\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2755 - accuracy: 0.89131s - l\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2721 - accuracy: 0.8922\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2745 - accuracy: 0.8937\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2580 - accuracy: 0.8997\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2543 - accuracy: 0.9016\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2475 - accuracy: 0.9026\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2342 - accuracy: 0.9077\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2395 - accuracy: 0.9049\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2214 - accuracy: 0.9127\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2127 - accuracy: 0.9171\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2125 - accuracy: 0.9185\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2104 - accuracy: 0.9177\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2178 - accuracy: 0.9173\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2035 - accuracy: 0.9203\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1930 - accuracy: 0.9258\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1922 - accuracy: 0.9273\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1938 - accuracy: 0.9255\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1897 - accuracy: 0.9282\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1920 - accuracy: 0.9274\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1854 - accuracy: 0.9293\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1811 - accuracy: 0.93081s - loss:\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1813 - accuracy: 0.9314\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1786 - accuracy: 0.9324\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1794 - accuracy: 0.9311\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1707 - accuracy: 0.9361\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1875 - accuracy: 0.9298\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1778 - accuracy: 0.9331\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1594 - accuracy: 0.9384\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1578 - accuracy: 0.9411\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1578 - accuracy: 0.93961s - los\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1562 - accuracy: 0.9411\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1660 - accuracy: 0.9386\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1485 - accuracy: 0.9444\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1628 - accuracy: 0.9396\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1655 - accuracy: 0.9383\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1532 - accuracy: 0.9423\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1678 - accuracy: 0.9369\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1551 - accuracy: 0.9414\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1577 - accuracy: 0.9392\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1519 - accuracy: 0.9438\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1452 - accuracy: 0.9458\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1521 - accuracy: 0.9426\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1301 - accuracy: 0.9510\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1420 - accuracy: 0.9473\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1372 - accuracy: 0.9482\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1375 - accuracy: 0.9481\n",
      "76/76 [==============================] - 3s 34ms/step - loss: 0.3045 - accuracy: 0.9174\n",
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_99 (Conv1D)           (None, 89, 768)           5376      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 44, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 42, 512)           1180160   \n",
      "_________________________________________________________________\n",
      "average_pooling1d_33 (Averag (None, 21, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 19, 512)           786944    \n",
      "_________________________________________________________________\n",
      "lstm_63 (LSTM)               (None, 19, 256)           787456    \n",
      "_________________________________________________________________\n",
      "lstm_64 (LSTM)               (None, 19, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_31 (Flatten)         (None, 4864)              0         \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 256)               1245440   \n",
      "_________________________________________________________________\n",
      "dropout_62 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_63 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 4,598,022\n",
      "Trainable params: 4,598,022\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 2,  n_conv_2 = 512, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.1,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 256, dropout_2 = 0.3, activation_conv=  elu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x00000252062F3D48>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 15s 97ms/step - loss: 1.0737 - accuracy: 0.5414\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.7986 - accuracy: 0.6610\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.6642 - accuracy: 0.7286\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.5650 - accuracy: 0.7788\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.5021 - accuracy: 0.8015\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.4721 - accuracy: 0.8170\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.4245 - accuracy: 0.8356\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.4076 - accuracy: 0.8448\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.3759 - accuracy: 0.85680s - loss: 0.375\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.3541 - accuracy: 0.8642\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.3401 - accuracy: 0.8681\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.3234 - accuracy: 0.8790\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.3104 - accuracy: 0.8811\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2897 - accuracy: 0.8879\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2827 - accuracy: 0.8935\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.3009 - accuracy: 0.88250s - loss: 0.3016 \n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2857 - accuracy: 0.8917\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2665 - accuracy: 0.8984\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2520 - accuracy: 0.9039\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2471 - accuracy: 0.9059\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2432 - accuracy: 0.9047\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2475 - accuracy: 0.9059\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2399 - accuracy: 0.9081\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2357 - accuracy: 0.9086\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2313 - accuracy: 0.9092\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2265 - accuracy: 0.91301s - l\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2130 - accuracy: 0.9189\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2117 - accuracy: 0.9182\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2080 - accuracy: 0.9203\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2049 - accuracy: 0.9229\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2056 - accuracy: 0.9208\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1964 - accuracy: 0.9229\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1959 - accuracy: 0.9243\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1896 - accuracy: 0.9276\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2133 - accuracy: 0.9170\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.2090 - accuracy: 0.9178\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1867 - accuracy: 0.9278\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1816 - accuracy: 0.9296\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1829 - accuracy: 0.9306\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1939 - accuracy: 0.9245\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1761 - accuracy: 0.9319\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1862 - accuracy: 0.92730s - loss: 0.1848 \n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1767 - accuracy: 0.9311\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1689 - accuracy: 0.9349\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1697 - accuracy: 0.9350\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1647 - accuracy: 0.9360\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1625 - accuracy: 0.9379\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1637 - accuracy: 0.9391\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1761 - accuracy: 0.9336\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1658 - accuracy: 0.9365\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1681 - accuracy: 0.9372\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1688 - accuracy: 0.9360\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1517 - accuracy: 0.9424\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1627 - accuracy: 0.9386\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1664 - accuracy: 0.9374\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1463 - accuracy: 0.9445\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1410 - accuracy: 0.9476\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1467 - accuracy: 0.9456\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1694 - accuracy: 0.9366\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 15s 97ms/step - loss: 0.1556 - accuracy: 0.9426\n",
      "76/76 [==============================] - 3s 34ms/step - loss: 0.2859 - accuracy: 0.9194\n",
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_102 (Conv1D)          (None, 88, 512)           5120      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 29, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 28, 512)           524800    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_34 (Averag (None, 14, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 13, 256)           262400    \n",
      "_________________________________________________________________\n",
      "lstm_65 (LSTM)               (None, 13, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_66 (LSTM)               (None, 13, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_32 (Flatten)         (None, 3328)              0         \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 256)               852224    \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 2,370,566\n",
      "Trainable params: 2,370,566\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 512, k_conv_1 = 3,  n_conv_2 = 512, k_conv_2 = 2, \n",
      "          n_conv_3 = 256,  k_conv_3 = 2,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.3,\n",
      "          n_dense_2 = 512, dropout_2 = 0.3, activation_conv=  elu,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 7s 44ms/step - loss: 1.0955 - accuracy: 0.5268\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.8279 - accuracy: 0.6511\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.6142 - accuracy: 0.7577\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.5136 - accuracy: 0.8032\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.4586 - accuracy: 0.8255\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.4310 - accuracy: 0.8350\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3879 - accuracy: 0.8527\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3613 - accuracy: 0.8644\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3461 - accuracy: 0.8688\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3230 - accuracy: 0.8765\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3033 - accuracy: 0.8846\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3071 - accuracy: 0.8840\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2768 - accuracy: 0.8939\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2742 - accuracy: 0.8947\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2640 - accuracy: 0.9000\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2461 - accuracy: 0.9068\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2426 - accuracy: 0.9102\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2268 - accuracy: 0.9147\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2190 - accuracy: 0.9188\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2203 - accuracy: 0.9184\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1988 - accuracy: 0.9258\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2055 - accuracy: 0.9226\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2087 - accuracy: 0.9212\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1904 - accuracy: 0.9292\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1868 - accuracy: 0.9309\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2102 - accuracy: 0.9231\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1697 - accuracy: 0.9376\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1800 - accuracy: 0.9330\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1604 - accuracy: 0.9411\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1732 - accuracy: 0.9363\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1662 - accuracy: 0.9396\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1530 - accuracy: 0.9435\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1585 - accuracy: 0.9428\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1504 - accuracy: 0.9460\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1601 - accuracy: 0.9417\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1392 - accuracy: 0.9495\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1391 - accuracy: 0.9502\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1440 - accuracy: 0.9478\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1501 - accuracy: 0.9455\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1364 - accuracy: 0.9503\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1298 - accuracy: 0.9534\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1402 - accuracy: 0.9486\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1419 - accuracy: 0.9488\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1254 - accuracy: 0.9540\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1297 - accuracy: 0.9533\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1268 - accuracy: 0.9534\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1283 - accuracy: 0.9524\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1190 - accuracy: 0.9565\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1051 - accuracy: 0.9617\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1363 - accuracy: 0.9525\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1134 - accuracy: 0.9586\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1169 - accuracy: 0.9594\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1150 - accuracy: 0.9580\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1182 - accuracy: 0.9591\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1127 - accuracy: 0.9589\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1007 - accuracy: 0.9636\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1154 - accuracy: 0.9583\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1105 - accuracy: 0.9610\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1001 - accuracy: 0.9632\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1069 - accuracy: 0.9629\n",
      "76/76 [==============================] - 1s 18ms/step - loss: 0.3105 - accuracy: 0.9271\n",
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_105 (Conv1D)          (None, 88, 512)           5120      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 29, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 28, 512)           524800    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_35 (Averag (None, 14, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 13, 256)           262400    \n",
      "_________________________________________________________________\n",
      "lstm_67 (LSTM)               (None, 13, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_68 (LSTM)               (None, 13, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_33 (Flatten)         (None, 3328)              0         \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 256)               852224    \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 2,370,566\n",
      "Trainable params: 2,370,566\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 512, k_conv_1 = 3,  n_conv_2 = 512, k_conv_2 = 2, \n",
      "          n_conv_3 = 256,  k_conv_3 = 2,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.3,\n",
      "          n_dense_2 = 512, dropout_2 = 0.3, activation_conv=  elu,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 6s 41ms/step - loss: 1.1115 - accuracy: 0.5233\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.8493 - accuracy: 0.6440\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.6183 - accuracy: 0.7567\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.5018 - accuracy: 0.8102\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.4531 - accuracy: 0.8275\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.4256 - accuracy: 0.8405\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3740 - accuracy: 0.8576\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3555 - accuracy: 0.8647\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3485 - accuracy: 0.8673\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3228 - accuracy: 0.8780\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3148 - accuracy: 0.8808\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3079 - accuracy: 0.8829\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2807 - accuracy: 0.8924\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2693 - accuracy: 0.8967\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2707 - accuracy: 0.8965\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2531 - accuracy: 0.9029\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2478 - accuracy: 0.9070\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2380 - accuracy: 0.9089\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2522 - accuracy: 0.9042\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2392 - accuracy: 0.9092\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2174 - accuracy: 0.9165\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2099 - accuracy: 0.9212\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2026 - accuracy: 0.9222\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2059 - accuracy: 0.9228\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1980 - accuracy: 0.9251\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1923 - accuracy: 0.9263\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1798 - accuracy: 0.9319\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1854 - accuracy: 0.9301\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1836 - accuracy: 0.9324\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1711 - accuracy: 0.9367\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1595 - accuracy: 0.9404\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1500 - accuracy: 0.9442\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1812 - accuracy: 0.9326\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1560 - accuracy: 0.9412\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1414 - accuracy: 0.9470\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1422 - accuracy: 0.9478\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1481 - accuracy: 0.9449\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1528 - accuracy: 0.9439\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1425 - accuracy: 0.9469\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1341 - accuracy: 0.9497\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1300 - accuracy: 0.9520\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1339 - accuracy: 0.9504\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1319 - accuracy: 0.9523\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1430 - accuracy: 0.9478\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1271 - accuracy: 0.9526\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1357 - accuracy: 0.9506\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1291 - accuracy: 0.9521\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1283 - accuracy: 0.9540\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1164 - accuracy: 0.9576\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1188 - accuracy: 0.9552\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1147 - accuracy: 0.9580\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1257 - accuracy: 0.9540\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1057 - accuracy: 0.9610\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1112 - accuracy: 0.9594\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1265 - accuracy: 0.9548\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1138 - accuracy: 0.9578\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1140 - accuracy: 0.9590\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.0975 - accuracy: 0.9640\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1128 - accuracy: 0.9596\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1026 - accuracy: 0.9619\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 0.3402 - accuracy: 0.9187\n",
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_108 (Conv1D)          (None, 88, 512)           5120      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 29, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 28, 512)           524800    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_36 (Averag (None, 14, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 13, 256)           262400    \n",
      "_________________________________________________________________\n",
      "lstm_69 (LSTM)               (None, 13, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_70 (LSTM)               (None, 13, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_34 (Flatten)         (None, 3328)              0         \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 256)               852224    \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 2,370,566\n",
      "Trainable params: 2,370,566\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 512, k_conv_1 = 3,  n_conv_2 = 512, k_conv_2 = 2, \n",
      "          n_conv_3 = 256,  k_conv_3 = 2,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.3,\n",
      "          n_dense_2 = 512, dropout_2 = 0.3, activation_conv=  elu,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 6s 41ms/step - loss: 1.1121 - accuracy: 0.5198\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.8312 - accuracy: 0.6593\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.6277 - accuracy: 0.7570\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.5104 - accuracy: 0.8038\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.4705 - accuracy: 0.8183\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.4187 - accuracy: 0.8411\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3999 - accuracy: 0.8477\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3598 - accuracy: 0.8648\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3555 - accuracy: 0.8646\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3280 - accuracy: 0.8747\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3405 - accuracy: 0.8687\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2927 - accuracy: 0.8887\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2992 - accuracy: 0.8847\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2786 - accuracy: 0.8936\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2736 - accuracy: 0.8953\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2674 - accuracy: 0.8964\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2610 - accuracy: 0.9008\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2419 - accuracy: 0.9066\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2344 - accuracy: 0.9103\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2325 - accuracy: 0.9126\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2204 - accuracy: 0.9166\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2124 - accuracy: 0.9200\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2160 - accuracy: 0.9187\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2033 - accuracy: 0.9252\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1912 - accuracy: 0.9274\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2007 - accuracy: 0.9261\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1932 - accuracy: 0.9290\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1760 - accuracy: 0.9347\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1743 - accuracy: 0.9364\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1620 - accuracy: 0.9394\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1611 - accuracy: 0.9416\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1591 - accuracy: 0.9401\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1633 - accuracy: 0.9407\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1515 - accuracy: 0.9445\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1501 - accuracy: 0.9463\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1389 - accuracy: 0.9484\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1530 - accuracy: 0.9439\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1433 - accuracy: 0.9474\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1428 - accuracy: 0.9475\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1384 - accuracy: 0.9485\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1359 - accuracy: 0.9513\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1320 - accuracy: 0.9510\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1276 - accuracy: 0.9526\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1356 - accuracy: 0.9505\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1269 - accuracy: 0.9543\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1151 - accuracy: 0.9571\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1273 - accuracy: 0.9533\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1128 - accuracy: 0.9592\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1316 - accuracy: 0.9525\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1184 - accuracy: 0.9563\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1235 - accuracy: 0.9551\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1152 - accuracy: 0.9577\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1147 - accuracy: 0.9579\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1148 - accuracy: 0.9580\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1054 - accuracy: 0.9613\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1015 - accuracy: 0.9636\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1148 - accuracy: 0.9576\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1043 - accuracy: 0.9611\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1026 - accuracy: 0.9631\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1156 - accuracy: 0.9576\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 0.2811 - accuracy: 0.9340\n",
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_111 (Conv1D)          (None, 89, 512)           3584      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 44, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 43, 256)           262400    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_37 (Averag (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 20, 512)           262656    \n",
      "_________________________________________________________________\n",
      "lstm_71 (LSTM)               (None, 20, 128)           328192    \n",
      "_________________________________________________________________\n",
      "lstm_72 (LSTM)               (None, 20, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_35 (Flatten)         (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 256)               655616    \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 1,778,694\n",
      "Trainable params: 1,778,694\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 512, k_conv_1 = 2,  n_conv_2 = 256, k_conv_2 = 2, \n",
      "          n_conv_3 = 512,  k_conv_3 = 2,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 128,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 512, dropout_2 = 0.2, activation_conv=  relu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x00000252062F3D48>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 7s 45ms/step - loss: 1.0171 - accuracy: 0.5639\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.6871 - accuracy: 0.7211\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.5443 - accuracy: 0.7883\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.4572 - accuracy: 0.8275\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.3987 - accuracy: 0.8456\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.3508 - accuracy: 0.8641\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.3291 - accuracy: 0.8726\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.3101 - accuracy: 0.8792\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2930 - accuracy: 0.8868\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2851 - accuracy: 0.8909\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2674 - accuracy: 0.8983\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2581 - accuracy: 0.9030\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2467 - accuracy: 0.9062\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2339 - accuracy: 0.9122\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.2354 - accuracy: 0.9125\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2165 - accuracy: 0.9178\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2175 - accuracy: 0.9177\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2052 - accuracy: 0.9224\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2035 - accuracy: 0.9241\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1912 - accuracy: 0.9280\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1870 - accuracy: 0.9287\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1873 - accuracy: 0.9295\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1753 - accuracy: 0.9338\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1825 - accuracy: 0.9308\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1644 - accuracy: 0.9371\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1662 - accuracy: 0.9361\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.1601 - accuracy: 0.9398\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1637 - accuracy: 0.9367\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1558 - accuracy: 0.9410\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 0.1454 - accuracy: 0.9440\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1467 - accuracy: 0.9438\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.1401 - accuracy: 0.9459\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.1425 - accuracy: 0.9452\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1264 - accuracy: 0.9514\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1234 - accuracy: 0.9524\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.1273 - accuracy: 0.9503\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1231 - accuracy: 0.9527\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.1200 - accuracy: 0.9519\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1157 - accuracy: 0.9542\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1198 - accuracy: 0.9530\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.1024 - accuracy: 0.9597\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1079 - accuracy: 0.9584\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1044 - accuracy: 0.9595\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.1022 - accuracy: 0.9615\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1050 - accuracy: 0.9603\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1010 - accuracy: 0.9620\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.0982 - accuracy: 0.9622\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.0941 - accuracy: 0.9634\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.0896 - accuracy: 0.9654\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.0860 - accuracy: 0.9672\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.0914 - accuracy: 0.9661\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.0862 - accuracy: 0.9667\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.0766 - accuracy: 0.9715\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.0906 - accuracy: 0.9666\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.0832 - accuracy: 0.9692\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.0930 - accuracy: 0.9662\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.0741 - accuracy: 0.9716\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.0771 - accuracy: 0.9703\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.0802 - accuracy: 0.9703\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.0734 - accuracy: 0.9722\n",
      "76/76 [==============================] - 1s 18ms/step - loss: 0.2683 - accuracy: 0.9427\n",
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_114 (Conv1D)          (None, 89, 512)           3584      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 44, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 43, 256)           262400    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_38 (Averag (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 20, 512)           262656    \n",
      "_________________________________________________________________\n",
      "lstm_73 (LSTM)               (None, 20, 128)           328192    \n",
      "_________________________________________________________________\n",
      "lstm_74 (LSTM)               (None, 20, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_36 (Flatten)         (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense_108 (Dense)            (None, 256)               655616    \n",
      "_________________________________________________________________\n",
      "dropout_72 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_73 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 1,778,694\n",
      "Trainable params: 1,778,694\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 512, k_conv_1 = 2,  n_conv_2 = 256, k_conv_2 = 2, \n",
      "          n_conv_3 = 512,  k_conv_3 = 2,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 128,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 512, dropout_2 = 0.2, activation_conv=  relu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x00000252062F3D48>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 7s 43ms/step - loss: 1.0388 - accuracy: 0.5610\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.7042 - accuracy: 0.7072\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.5270 - accuracy: 0.7930\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.4371 - accuracy: 0.8329\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.3778 - accuracy: 0.8550\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.3455 - accuracy: 0.8683\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.3194 - accuracy: 0.8763\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2982 - accuracy: 0.8846\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.2773 - accuracy: 0.8938\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2671 - accuracy: 0.8983\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.2564 - accuracy: 0.9038\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2523 - accuracy: 0.9046\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2485 - accuracy: 0.9056\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2359 - accuracy: 0.9120\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2276 - accuracy: 0.9143\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2104 - accuracy: 0.9202\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2109 - accuracy: 0.9194\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.2111 - accuracy: 0.9198\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2006 - accuracy: 0.9238\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.1849 - accuracy: 0.9281\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1830 - accuracy: 0.9289\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1863 - accuracy: 0.9284\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.1778 - accuracy: 0.9320\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1703 - accuracy: 0.9347\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1735 - accuracy: 0.9317\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1641 - accuracy: 0.9357\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1598 - accuracy: 0.9374\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1508 - accuracy: 0.9404\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1532 - accuracy: 0.9409\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.1503 - accuracy: 0.9395\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1507 - accuracy: 0.9408\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1330 - accuracy: 0.9450\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1412 - accuracy: 0.9438\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.1358 - accuracy: 0.9467\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1304 - accuracy: 0.9481\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.1307 - accuracy: 0.9482\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1235 - accuracy: 0.9523\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1163 - accuracy: 0.9517\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1231 - accuracy: 0.9512\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1177 - accuracy: 0.9522\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.1107 - accuracy: 0.9567\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1175 - accuracy: 0.9532\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.1117 - accuracy: 0.9553\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.1050 - accuracy: 0.9578\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1027 - accuracy: 0.9584\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.0971 - accuracy: 0.9618\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.1044 - accuracy: 0.9581\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.0943 - accuracy: 0.9628\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.0929 - accuracy: 0.9627\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.0939 - accuracy: 0.9627\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.0994 - accuracy: 0.9606\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.0918 - accuracy: 0.9645\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.0858 - accuracy: 0.9659\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.0867 - accuracy: 0.9656\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.0840 - accuracy: 0.9668\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.0830 - accuracy: 0.9657\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.0823 - accuracy: 0.9677\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.0812 - accuracy: 0.9670\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.0797 - accuracy: 0.9694\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.0877 - accuracy: 0.9666\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 0.2777 - accuracy: 0.9396\n",
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_117 (Conv1D)          (None, 89, 512)           3584      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 44, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 43, 256)           262400    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_39 (Averag (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 20, 512)           262656    \n",
      "_________________________________________________________________\n",
      "lstm_75 (LSTM)               (None, 20, 128)           328192    \n",
      "_________________________________________________________________\n",
      "lstm_76 (LSTM)               (None, 20, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_37 (Flatten)         (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 256)               655616    \n",
      "_________________________________________________________________\n",
      "dropout_74 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 1,778,694\n",
      "Trainable params: 1,778,694\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 512, k_conv_1 = 2,  n_conv_2 = 256, k_conv_2 = 2, \n",
      "          n_conv_3 = 512,  k_conv_3 = 2,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 128,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 512, dropout_2 = 0.2, activation_conv=  relu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x00000252062F3D48>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 6s 43ms/step - loss: 1.0329 - accuracy: 0.5599\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.7038 - accuracy: 0.7096\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.5568 - accuracy: 0.7758\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.4465 - accuracy: 0.8289\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.3876 - accuracy: 0.8500\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.3564 - accuracy: 0.8628\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.3198 - accuracy: 0.8772\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.3030 - accuracy: 0.8843\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2879 - accuracy: 0.8912\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2765 - accuracy: 0.8951\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2672 - accuracy: 0.8993\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2645 - accuracy: 0.8991\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2461 - accuracy: 0.9071\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2452 - accuracy: 0.9084\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2260 - accuracy: 0.9153\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2164 - accuracy: 0.9180\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2166 - accuracy: 0.9188\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2119 - accuracy: 0.9207\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1991 - accuracy: 0.9244\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.2017 - accuracy: 0.9235\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1942 - accuracy: 0.9261\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.1877 - accuracy: 0.9284\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1831 - accuracy: 0.9298\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1749 - accuracy: 0.9330\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1732 - accuracy: 0.9348\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.1828 - accuracy: 0.9320\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1640 - accuracy: 0.9374\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1535 - accuracy: 0.9402\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1561 - accuracy: 0.9400\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1598 - accuracy: 0.9385\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1433 - accuracy: 0.9440\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1482 - accuracy: 0.9417\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1328 - accuracy: 0.9491\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1402 - accuracy: 0.9462\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1245 - accuracy: 0.9525\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1496 - accuracy: 0.9421\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1253 - accuracy: 0.9510\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.1291 - accuracy: 0.9504\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1150 - accuracy: 0.9555\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1200 - accuracy: 0.9542\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.1107 - accuracy: 0.9577\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1139 - accuracy: 0.9557\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1055 - accuracy: 0.9582\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1013 - accuracy: 0.9611\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1108 - accuracy: 0.9576\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.0982 - accuracy: 0.9630\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.1022 - accuracy: 0.9608\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.0933 - accuracy: 0.9646\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.0988 - accuracy: 0.9618\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.0909 - accuracy: 0.9658\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.0930 - accuracy: 0.9648\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.0941 - accuracy: 0.9646\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.0782 - accuracy: 0.9704\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.0885 - accuracy: 0.9672\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.0879 - accuracy: 0.9670\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.0852 - accuracy: 0.9673\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.0836 - accuracy: 0.9676\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.0836 - accuracy: 0.9684\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 6s 43ms/step - loss: 0.0774 - accuracy: 0.9701\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.0858 - accuracy: 0.9675\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 0.2748 - accuracy: 0.9370\n",
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_120 (Conv1D)          (None, 88, 768)           7680      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 44, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 43, 512)           786944    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_40 (Averag (None, 14, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 13, 512)           524800    \n",
      "_________________________________________________________________\n",
      "lstm_77 (LSTM)               (None, 13, 256)           787456    \n",
      "_________________________________________________________________\n",
      "lstm_78 (LSTM)               (None, 13, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_38 (Flatten)         (None, 3328)              0         \n",
      "_________________________________________________________________\n",
      "dense_114 (Dense)            (None, 384)               1278336   \n",
      "_________________________________________________________________\n",
      "dropout_76 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (None, 512)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_116 (Dense)            (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 4,110,726\n",
      "Trainable params: 4,110,726\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 3,  n_conv_2 = 512, k_conv_2 = 2, \n",
      "          n_conv_3 = 512,  k_conv_3 = 2,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 3,\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.1,\n",
      "          n_dense_1 = 384, dropout_1 = 0.2,\n",
      "          n_dense_2 = 512, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 11s 73ms/step - loss: 1.0514 - accuracy: 0.5521\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 11s 72ms/step - loss: 0.6597 - accuracy: 0.7304\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.4961 - accuracy: 0.8024\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 11s 72ms/step - loss: 0.4277 - accuracy: 0.8302\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.3875 - accuracy: 0.8483\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.3564 - accuracy: 0.8608\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.3279 - accuracy: 0.8738\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.3107 - accuracy: 0.8804\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2988 - accuracy: 0.8858\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2824 - accuracy: 0.8923\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2776 - accuracy: 0.8932\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2679 - accuracy: 0.8988\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2610 - accuracy: 0.8989\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2513 - accuracy: 0.9040\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2409 - accuracy: 0.9077\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2311 - accuracy: 0.9102\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2221 - accuracy: 0.9148\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2140 - accuracy: 0.9193\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2150 - accuracy: 0.9166\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1990 - accuracy: 0.9244\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2075 - accuracy: 0.9221\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1964 - accuracy: 0.9242\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1850 - accuracy: 0.9294\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1835 - accuracy: 0.9298\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1766 - accuracy: 0.9318\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1702 - accuracy: 0.9337\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1611 - accuracy: 0.9386\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1701 - accuracy: 0.9363\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1745 - accuracy: 0.9344\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1596 - accuracy: 0.9398\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1439 - accuracy: 0.9452\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1486 - accuracy: 0.9411\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1384 - accuracy: 0.9463\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1325 - accuracy: 0.9480\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1387 - accuracy: 0.9459\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1196 - accuracy: 0.9530\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1313 - accuracy: 0.9494\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1317 - accuracy: 0.9472\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1239 - accuracy: 0.9521\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1236 - accuracy: 0.9522\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1184 - accuracy: 0.9544\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1105 - accuracy: 0.9583\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1100 - accuracy: 0.9563\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1141 - accuracy: 0.9567\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1072 - accuracy: 0.9588\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1118 - accuracy: 0.9573\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1037 - accuracy: 0.9598\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0980 - accuracy: 0.9623\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1047 - accuracy: 0.9597\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0942 - accuracy: 0.9640\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0946 - accuracy: 0.9648\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0981 - accuracy: 0.9626\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0904 - accuracy: 0.9659\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0919 - accuracy: 0.9665\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0918 - accuracy: 0.9670\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0919 - accuracy: 0.9658\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0916 - accuracy: 0.9668\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0832 - accuracy: 0.9688\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0721 - accuracy: 0.9729\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0781 - accuracy: 0.9702\n",
      "76/76 [==============================] - 2s 28ms/step - loss: 0.2678 - accuracy: 0.9432\n",
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_123 (Conv1D)          (None, 88, 768)           7680      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 44, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 43, 512)           786944    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_41 (Averag (None, 14, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 13, 512)           524800    \n",
      "_________________________________________________________________\n",
      "lstm_79 (LSTM)               (None, 13, 256)           787456    \n",
      "_________________________________________________________________\n",
      "lstm_80 (LSTM)               (None, 13, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_39 (Flatten)         (None, 3328)              0         \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (None, 384)               1278336   \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_118 (Dense)            (None, 512)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_79 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 4,110,726\n",
      "Trainable params: 4,110,726\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 3,  n_conv_2 = 512, k_conv_2 = 2, \n",
      "          n_conv_3 = 512,  k_conv_3 = 2,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 3,\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.1,\n",
      "          n_dense_1 = 384, dropout_1 = 0.2,\n",
      "          n_dense_2 = 512, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 11s 72ms/step - loss: 1.0891 - accuracy: 0.5329\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.6844 - accuracy: 0.7177\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.5143 - accuracy: 0.7925\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.4362 - accuracy: 0.8260\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.3900 - accuracy: 0.8463\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.3555 - accuracy: 0.8618\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.3267 - accuracy: 0.8719\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.3140 - accuracy: 0.8770\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2953 - accuracy: 0.8859\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2858 - accuracy: 0.8899\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2752 - accuracy: 0.8937\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2893 - accuracy: 0.8881\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2527 - accuracy: 0.9017\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2390 - accuracy: 0.9080\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2266 - accuracy: 0.9131\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2218 - accuracy: 0.9143\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2219 - accuracy: 0.9161\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2127 - accuracy: 0.9193\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2106 - accuracy: 0.9206\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2045 - accuracy: 0.9210\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1945 - accuracy: 0.9253\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2220 - accuracy: 0.9183\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1824 - accuracy: 0.9322\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1759 - accuracy: 0.9329\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1706 - accuracy: 0.9351\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1794 - accuracy: 0.9332\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1594 - accuracy: 0.9401\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1636 - accuracy: 0.9375\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1495 - accuracy: 0.94240s - loss: 0.1484 - ac\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1520 - accuracy: 0.9414\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1448 - accuracy: 0.9441\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1397 - accuracy: 0.9453\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1377 - accuracy: 0.9470\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1351 - accuracy: 0.9481\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1459 - accuracy: 0.9425\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1238 - accuracy: 0.9519\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1316 - accuracy: 0.9500\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1179 - accuracy: 0.9544\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1223 - accuracy: 0.9527\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1230 - accuracy: 0.9518\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1194 - accuracy: 0.9530\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 11s 72ms/step - loss: 0.1107 - accuracy: 0.9569\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1076 - accuracy: 0.9596\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1017 - accuracy: 0.9607\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1067 - accuracy: 0.9607\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1021 - accuracy: 0.9602\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1106 - accuracy: 0.95830s - loss: 0.1110 - accuracy\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0949 - accuracy: 0.9637\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1013 - accuracy: 0.9632\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0990 - accuracy: 0.9616\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0896 - accuracy: 0.9666\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0863 - accuracy: 0.9671\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0903 - accuracy: 0.9658\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0835 - accuracy: 0.9691\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0797 - accuracy: 0.9703\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0814 - accuracy: 0.9703\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0812 - accuracy: 0.9693\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0840 - accuracy: 0.9683\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0789 - accuracy: 0.9702\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0782 - accuracy: 0.9705\n",
      "76/76 [==============================] - 2s 26ms/step - loss: 0.2652 - accuracy: 0.9442\n",
      "Model: \"sequential_42\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_126 (Conv1D)          (None, 88, 768)           7680      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 44, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 43, 512)           786944    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_42 (Averag (None, 14, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 13, 512)           524800    \n",
      "_________________________________________________________________\n",
      "lstm_81 (LSTM)               (None, 13, 256)           787456    \n",
      "_________________________________________________________________\n",
      "lstm_82 (LSTM)               (None, 13, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_40 (Flatten)         (None, 3328)              0         \n",
      "_________________________________________________________________\n",
      "dense_120 (Dense)            (None, 384)               1278336   \n",
      "_________________________________________________________________\n",
      "dropout_80 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_121 (Dense)            (None, 512)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_81 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_122 (Dense)            (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 4,110,726\n",
      "Trainable params: 4,110,726\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 3,  n_conv_2 = 512, k_conv_2 = 2, \n",
      "          n_conv_3 = 512,  k_conv_3 = 2,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 3,\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.1,\n",
      "          n_dense_1 = 384, dropout_1 = 0.2,\n",
      "          n_dense_2 = 512, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 11s 72ms/step - loss: 1.0413 - accuracy: 0.5621\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 11s 72ms/step - loss: 0.6854 - accuracy: 0.7180\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 11s 72ms/step - loss: 0.5314 - accuracy: 0.7893\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.4292 - accuracy: 0.8338\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.3889 - accuracy: 0.8488\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.3647 - accuracy: 0.8597\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.3347 - accuracy: 0.8717\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.3290 - accuracy: 0.8714\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.3089 - accuracy: 0.8803\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2881 - accuracy: 0.8931\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2835 - accuracy: 0.8931\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2635 - accuracy: 0.9000\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2587 - accuracy: 0.9032\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2536 - accuracy: 0.9037\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2383 - accuracy: 0.9096\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2297 - accuracy: 0.9144\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2270 - accuracy: 0.9149\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2201 - accuracy: 0.9175\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2132 - accuracy: 0.9206\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2066 - accuracy: 0.9216\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.2128 - accuracy: 0.9199\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1933 - accuracy: 0.9265\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1887 - accuracy: 0.9268\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1818 - accuracy: 0.9290\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1833 - accuracy: 0.9299\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1785 - accuracy: 0.93090s - loss:\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1687 - accuracy: 0.9342\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1691 - accuracy: 0.9352\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1569 - accuracy: 0.9388\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1693 - accuracy: 0.9353\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1553 - accuracy: 0.9408\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1412 - accuracy: 0.9454\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1421 - accuracy: 0.9445\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1365 - accuracy: 0.9466\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1355 - accuracy: 0.9464\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1296 - accuracy: 0.9496\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1300 - accuracy: 0.9500\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1346 - accuracy: 0.9476\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1249 - accuracy: 0.9515\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1207 - accuracy: 0.9546\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1229 - accuracy: 0.9544\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1165 - accuracy: 0.9557\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1113 - accuracy: 0.9575\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1118 - accuracy: 0.9575\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1081 - accuracy: 0.9582\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1028 - accuracy: 0.9609\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.1165 - accuracy: 0.9569\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0982 - accuracy: 0.9633\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0946 - accuracy: 0.9641\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0975 - accuracy: 0.9634\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0926 - accuracy: 0.9655\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0914 - accuracy: 0.9670\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0886 - accuracy: 0.9676\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0873 - accuracy: 0.9669\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0939 - accuracy: 0.9657\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0855 - accuracy: 0.9691\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0926 - accuracy: 0.9663\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0875 - accuracy: 0.9670\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0885 - accuracy: 0.9682\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 11s 71ms/step - loss: 0.0795 - accuracy: 0.9701\n",
      "76/76 [==============================] - 2s 26ms/step - loss: 0.2516 - accuracy: 0.9454\n",
      "Model: \"sequential_43\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_129 (Conv1D)          (None, 89, 512)           3584      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 44, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 42, 256)           393472    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_43 (Averag (None, 14, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 13, 512)           262656    \n",
      "_________________________________________________________________\n",
      "lstm_83 (LSTM)               (None, 13, 128)           328192    \n",
      "_________________________________________________________________\n",
      "lstm_84 (LSTM)               (None, 13, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_41 (Flatten)         (None, 1664)              0         \n",
      "_________________________________________________________________\n",
      "dense_123 (Dense)            (None, 256)               426240    \n",
      "_________________________________________________________________\n",
      "dropout_82 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_124 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_83 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_125 (Dense)            (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 1,680,390\n",
      "Trainable params: 1,680,390\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 512, k_conv_1 = 2,  n_conv_2 = 256, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 2,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 3,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 128,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 512, dropout_2 = 0.3, activation_conv=  elu,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 6s 42ms/step - loss: 1.1027 - accuracy: 0.5219\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.8758 - accuracy: 0.6251\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.6996 - accuracy: 0.7074\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.5925 - accuracy: 0.7606\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.5221 - accuracy: 0.7943\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.4665 - accuracy: 0.8171\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.4207 - accuracy: 0.8357\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.4065 - accuracy: 0.8446\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3691 - accuracy: 0.8596\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3538 - accuracy: 0.8637\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3276 - accuracy: 0.8745\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3195 - accuracy: 0.8785\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3033 - accuracy: 0.8843\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2923 - accuracy: 0.8885\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2735 - accuracy: 0.8964\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2668 - accuracy: 0.8988\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2592 - accuracy: 0.9019 \n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2490 - accuracy: 0.9029\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2487 - accuracy: 0.9038\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2421 - accuracy: 0.9068\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2344 - accuracy: 0.9109\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2315 - accuracy: 0.9113\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2287 - accuracy: 0.9129\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2200 - accuracy: 0.9165\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2077 - accuracy: 0.9205\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2043 - accuracy: 0.9212\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2098 - accuracy: 0.9218\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2004 - accuracy: 0.9243\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1966 - accuracy: 0.9271\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1867 - accuracy: 0.9305\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1916 - accuracy: 0.9300\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1765 - accuracy: 0.9344\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1846 - accuracy: 0.9297\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1745 - accuracy: 0.9339\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1688 - accuracy: 0.9365\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1701 - accuracy: 0.9362\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1564 - accuracy: 0.9413\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1645 - accuracy: 0.9386\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1680 - accuracy: 0.9378\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1688 - accuracy: 0.9375\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1532 - accuracy: 0.9434\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1570 - accuracy: 0.9421\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1694 - accuracy: 0.9371\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1467 - accuracy: 0.9456\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1549 - accuracy: 0.9423\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1509 - accuracy: 0.9438\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1513 - accuracy: 0.9438\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1437 - accuracy: 0.9475\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1494 - accuracy: 0.9443\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1485 - accuracy: 0.9444\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1324 - accuracy: 0.9504\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1459 - accuracy: 0.9453\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1328 - accuracy: 0.9505\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1408 - accuracy: 0.9483\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1270 - accuracy: 0.9523\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1404 - accuracy: 0.9467\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1328 - accuracy: 0.9517\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1229 - accuracy: 0.9550\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1226 - accuracy: 0.9547\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1371 - accuracy: 0.9506\n",
      "76/76 [==============================] - 1s 17ms/step - loss: 0.2716 - accuracy: 0.9252\n",
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_132 (Conv1D)          (None, 89, 512)           3584      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 44, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 42, 256)           393472    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_44 (Averag (None, 14, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 13, 512)           262656    \n",
      "_________________________________________________________________\n",
      "lstm_85 (LSTM)               (None, 13, 128)           328192    \n",
      "_________________________________________________________________\n",
      "lstm_86 (LSTM)               (None, 13, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_42 (Flatten)         (None, 1664)              0         \n",
      "_________________________________________________________________\n",
      "dense_126 (Dense)            (None, 256)               426240    \n",
      "_________________________________________________________________\n",
      "dropout_84 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_127 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_85 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_128 (Dense)            (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 1,680,390\n",
      "Trainable params: 1,680,390\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 512, k_conv_1 = 2,  n_conv_2 = 256, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 2,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 3,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 128,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 512, dropout_2 = 0.3, activation_conv=  elu,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 6s 41ms/step - loss: 1.1115 - accuracy: 0.5214\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.8766 - accuracy: 0.6249\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.7126 - accuracy: 0.7081\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.6035 - accuracy: 0.7601\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.5263 - accuracy: 0.7984\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.4714 - accuracy: 0.8200\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.4188 - accuracy: 0.8409\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3907 - accuracy: 0.8528\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3577 - accuracy: 0.8620\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3459 - accuracy: 0.8654\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3172 - accuracy: 0.8792\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3278 - accuracy: 0.8747\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2994 - accuracy: 0.8859\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2913 - accuracy: 0.8871\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2870 - accuracy: 0.8903\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.2853 - accuracy: 0.8905\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2805 - accuracy: 0.8921\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2553 - accuracy: 0.9004\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2473 - accuracy: 0.9028\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2439 - accuracy: 0.9069\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2278 - accuracy: 0.9126\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2315 - accuracy: 0.9111\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2127 - accuracy: 0.9202\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2221 - accuracy: 0.9139\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2184 - accuracy: 0.9151\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2107 - accuracy: 0.9195\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2032 - accuracy: 0.9224\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2032 - accuracy: 0.9212\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1920 - accuracy: 0.9269\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1867 - accuracy: 0.9273\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1957 - accuracy: 0.9237\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.2021 - accuracy: 0.9238\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1907 - accuracy: 0.9260\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1872 - accuracy: 0.9290\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1762 - accuracy: 0.9337\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1850 - accuracy: 0.9302\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1741 - accuracy: 0.9345\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1681 - accuracy: 0.9360\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1769 - accuracy: 0.9319\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1639 - accuracy: 0.9382\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.1701 - accuracy: 0.9362\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1618 - accuracy: 0.9407\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1594 - accuracy: 0.9399\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1675 - accuracy: 0.9382\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1554 - accuracy: 0.9430\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1546 - accuracy: 0.9419\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1491 - accuracy: 0.9446\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1391 - accuracy: 0.9482\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1613 - accuracy: 0.9402\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1532 - accuracy: 0.9436\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1454 - accuracy: 0.9457\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1358 - accuracy: 0.9500\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1457 - accuracy: 0.9462\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1429 - accuracy: 0.9462\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1379 - accuracy: 0.9492\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1370 - accuracy: 0.9499\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1411 - accuracy: 0.9479\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1281 - accuracy: 0.9527\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1259 - accuracy: 0.9535\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1258 - accuracy: 0.9537\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 0.3060 - accuracy: 0.9196\n",
      "Model: \"sequential_45\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_135 (Conv1D)          (None, 89, 512)           3584      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 44, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 42, 256)           393472    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_45 (Averag (None, 14, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 13, 512)           262656    \n",
      "_________________________________________________________________\n",
      "lstm_87 (LSTM)               (None, 13, 128)           328192    \n",
      "_________________________________________________________________\n",
      "lstm_88 (LSTM)               (None, 13, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_43 (Flatten)         (None, 1664)              0         \n",
      "_________________________________________________________________\n",
      "dense_129 (Dense)            (None, 256)               426240    \n",
      "_________________________________________________________________\n",
      "dropout_86 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_130 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_87 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_131 (Dense)            (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 1,680,390\n",
      "Trainable params: 1,680,390\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 512, k_conv_1 = 2,  n_conv_2 = 256, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 2,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 3,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 128,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 512, dropout_2 = 0.3, activation_conv=  elu,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/152 [..............................] - ETA: 4s - loss: 1.7965 - accuracy: 0.1484WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0132s vs `on_train_batch_end` time: 0.0285s). Check your callbacks.\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 1.1112 - accuracy: 0.5198\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.8620 - accuracy: 0.6321\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.7044 - accuracy: 0.7112\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.6095 - accuracy: 0.7569\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.5253 - accuracy: 0.7953\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.4695 - accuracy: 0.8182\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.4370 - accuracy: 0.8307\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3937 - accuracy: 0.8485\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3692 - accuracy: 0.8589\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3673 - accuracy: 0.8587\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3320 - accuracy: 0.8722\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3217 - accuracy: 0.8756\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3034 - accuracy: 0.8829\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.3009 - accuracy: 0.8836\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2825 - accuracy: 0.8907\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2679 - accuracy: 0.8956\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2595 - accuracy: 0.8998\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2612 - accuracy: 0.8997\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2530 - accuracy: 0.9033\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2468 - accuracy: 0.9034\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2394 - accuracy: 0.9075\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2376 - accuracy: 0.9088\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2232 - accuracy: 0.9135\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2139 - accuracy: 0.9171\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2186 - accuracy: 0.9196\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2241 - accuracy: 0.9131\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2233 - accuracy: 0.9131\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1973 - accuracy: 0.9239\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1937 - accuracy: 0.9255\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1896 - accuracy: 0.9269\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2023 - accuracy: 0.9251\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1999 - accuracy: 0.9236\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1810 - accuracy: 0.9314\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1804 - accuracy: 0.9314\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1754 - accuracy: 0.9342\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.2079 - accuracy: 0.9227\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1727 - accuracy: 0.9356\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1703 - accuracy: 0.9368\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1747 - accuracy: 0.9352\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1581 - accuracy: 0.9406\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1560 - accuracy: 0.9424\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1776 - accuracy: 0.9328\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1638 - accuracy: 0.9391\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1654 - accuracy: 0.9379\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1563 - accuracy: 0.9414\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1594 - accuracy: 0.9405\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1600 - accuracy: 0.9409\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1457 - accuracy: 0.9441\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1621 - accuracy: 0.9391\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1464 - accuracy: 0.9458\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1431 - accuracy: 0.9464\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1410 - accuracy: 0.9473\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1454 - accuracy: 0.9459\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1354 - accuracy: 0.9502\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1355 - accuracy: 0.9503\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1485 - accuracy: 0.9449\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1368 - accuracy: 0.9496\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1352 - accuracy: 0.9493\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1328 - accuracy: 0.9507\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 6s 41ms/step - loss: 0.1253 - accuracy: 0.9535\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 0.3164 - accuracy: 0.9171\n",
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_138 (Conv1D)          (None, 88, 768)           7680      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 29, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 27, 256)           590080    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_46 (Averag (None, 13, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 12, 256)           131328    \n",
      "_________________________________________________________________\n",
      "lstm_89 (LSTM)               (None, 12, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_90 (LSTM)               (None, 12, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_44 (Flatten)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_132 (Dense)            (None, 384)               1180032   \n",
      "_________________________________________________________________\n",
      "dropout_88 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_133 (Dense)            (None, 256)               98560     \n",
      "_________________________________________________________________\n",
      "dropout_89 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_134 (Dense)            (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 2,600,582\n",
      "Trainable params: 2,600,582\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 3,  n_conv_2 = 256, k_conv_2 = 3, \n",
      "          n_conv_3 = 256,  k_conv_3 = 2,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.1,\n",
      "          n_dense_1 = 384, dropout_1 = 0.2,\n",
      "          n_dense_2 = 256, dropout_2 = 0.2, activation_conv=  relu,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 7s 46ms/step - loss: 1.0351 - accuracy: 0.5564\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.6460 - accuracy: 0.7445\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.4752 - accuracy: 0.8231\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.3978 - accuracy: 0.8530\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.3794 - accuracy: 0.8574\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.3328 - accuracy: 0.8761\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.3112 - accuracy: 0.8829\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.3006 - accuracy: 0.8902\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2869 - accuracy: 0.8928\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2730 - accuracy: 0.8996\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2587 - accuracy: 0.9045\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2536 - accuracy: 0.9073\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2400 - accuracy: 0.9122\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2337 - accuracy: 0.9149\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2271 - accuracy: 0.9158\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2259 - accuracy: 0.9166\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2100 - accuracy: 0.9212\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2063 - accuracy: 0.9228\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2028 - accuracy: 0.9228\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1909 - accuracy: 0.9292\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1948 - accuracy: 0.9274\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1831 - accuracy: 0.9309\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1878 - accuracy: 0.9298\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1764 - accuracy: 0.9353\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1706 - accuracy: 0.9366\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1745 - accuracy: 0.9342\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1597 - accuracy: 0.9410\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1655 - accuracy: 0.9375\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1607 - accuracy: 0.9403\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1442 - accuracy: 0.9459\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1491 - accuracy: 0.9447\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1439 - accuracy: 0.9451\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1367 - accuracy: 0.9491\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1396 - accuracy: 0.9479\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1385 - accuracy: 0.9469\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1351 - accuracy: 0.9510\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1282 - accuracy: 0.9520\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1277 - accuracy: 0.9524\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1218 - accuracy: 0.9550\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1175 - accuracy: 0.9558\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1228 - accuracy: 0.9552\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1161 - accuracy: 0.9565\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1084 - accuracy: 0.9602\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1099 - accuracy: 0.9587\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1181 - accuracy: 0.9586\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1128 - accuracy: 0.9591\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1024 - accuracy: 0.9622\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1011 - accuracy: 0.9621\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1063 - accuracy: 0.9611\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.0974 - accuracy: 0.9638\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.0921 - accuracy: 0.9657\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.0948 - accuracy: 0.9646\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.0969 - accuracy: 0.9648\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.0955 - accuracy: 0.9642\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.0958 - accuracy: 0.9654\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.0904 - accuracy: 0.9668\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.0868 - accuracy: 0.9676\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.0983 - accuracy: 0.9640\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.0855 - accuracy: 0.9678\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.0821 - accuracy: 0.9694\n",
      "76/76 [==============================] - 1s 17ms/step - loss: 0.2384 - accuracy: 0.9371\n",
      "Model: \"sequential_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_141 (Conv1D)          (None, 88, 768)           7680      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 29, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 27, 256)           590080    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_47 (Averag (None, 13, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 12, 256)           131328    \n",
      "_________________________________________________________________\n",
      "lstm_91 (LSTM)               (None, 12, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_92 (LSTM)               (None, 12, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_45 (Flatten)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_135 (Dense)            (None, 384)               1180032   \n",
      "_________________________________________________________________\n",
      "dropout_90 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_136 (Dense)            (None, 256)               98560     \n",
      "_________________________________________________________________\n",
      "dropout_91 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_137 (Dense)            (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 2,600,582\n",
      "Trainable params: 2,600,582\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 3,  n_conv_2 = 256, k_conv_2 = 3, \n",
      "          n_conv_3 = 256,  k_conv_3 = 2,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.1,\n",
      "          n_dense_1 = 384, dropout_1 = 0.2,\n",
      "          n_dense_2 = 256, dropout_2 = 0.2, activation_conv=  relu,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 7s 45ms/step - loss: 1.0473 - accuracy: 0.5547\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.6655 - accuracy: 0.7360\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.4893 - accuracy: 0.8137\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.3964 - accuracy: 0.8515\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.3593 - accuracy: 0.8642\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.3383 - accuracy: 0.8733\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.3090 - accuracy: 0.8836\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.3024 - accuracy: 0.8864\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2823 - accuracy: 0.8946\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2798 - accuracy: 0.8958\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2659 - accuracy: 0.9004\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2601 - accuracy: 0.9025\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2413 - accuracy: 0.9073\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2360 - accuracy: 0.9125\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2358 - accuracy: 0.9120\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2242 - accuracy: 0.9162\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2147 - accuracy: 0.9211\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2100 - accuracy: 0.9214\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2069 - accuracy: 0.9213\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2036 - accuracy: 0.9239\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1942 - accuracy: 0.9279\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1859 - accuracy: 0.9316\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1839 - accuracy: 0.9319\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1817 - accuracy: 0.9330\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1750 - accuracy: 0.9339\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1653 - accuracy: 0.9395\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1661 - accuracy: 0.9386\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1609 - accuracy: 0.9391\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1554 - accuracy: 0.9412\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1519 - accuracy: 0.9428\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1444 - accuracy: 0.9462\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1452 - accuracy: 0.9454\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1438 - accuracy: 0.9474\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1398 - accuracy: 0.9490\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1343 - accuracy: 0.9497\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1290 - accuracy: 0.9526\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1285 - accuracy: 0.9524\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1232 - accuracy: 0.9540\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1240 - accuracy: 0.9534\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1164 - accuracy: 0.9573\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1213 - accuracy: 0.9554\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1194 - accuracy: 0.9566\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1087 - accuracy: 0.9596\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1072 - accuracy: 0.9610\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1101 - accuracy: 0.9585\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1009 - accuracy: 0.9627\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1091 - accuracy: 0.9603\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1011 - accuracy: 0.9622\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.0961 - accuracy: 0.9637\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1016 - accuracy: 0.9625 0s - loss: 0.1016 - accuracy: 0.96\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.0966 - accuracy: 0.9649 0s - loss: 0.0\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.0995 - accuracy: 0.9632\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.0885 - accuracy: 0.9668\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.0917 - accuracy: 0.9665\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.0889 - accuracy: 0.9673\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.0853 - accuracy: 0.9686\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.0780 - accuracy: 0.9708\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.0926 - accuracy: 0.9673\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.0864 - accuracy: 0.9678\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.0833 - accuracy: 0.9696\n",
      "76/76 [==============================] - 1s 17ms/step - loss: 0.2544 - accuracy: 0.9397\n",
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_144 (Conv1D)          (None, 88, 768)           7680      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 29, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 27, 256)           590080    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_48 (Averag (None, 13, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 12, 256)           131328    \n",
      "_________________________________________________________________\n",
      "lstm_93 (LSTM)               (None, 12, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_94 (LSTM)               (None, 12, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_46 (Flatten)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_138 (Dense)            (None, 384)               1180032   \n",
      "_________________________________________________________________\n",
      "dropout_92 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_139 (Dense)            (None, 256)               98560     \n",
      "_________________________________________________________________\n",
      "dropout_93 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_140 (Dense)            (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 2,600,582\n",
      "Trainable params: 2,600,582\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 3,  n_conv_2 = 256, k_conv_2 = 3, \n",
      "          n_conv_3 = 256,  k_conv_3 = 2,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.1,\n",
      "          n_dense_1 = 384, dropout_1 = 0.2,\n",
      "          n_dense_2 = 256, dropout_2 = 0.2, activation_conv=  relu,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 7s 45ms/step - loss: 1.0507 - accuracy: 0.5490\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.6677 - accuracy: 0.7357\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.4943 - accuracy: 0.8124\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.4164 - accuracy: 0.8444\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.3613 - accuracy: 0.8660\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.3368 - accuracy: 0.8748\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.3108 - accuracy: 0.8849\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.3077 - accuracy: 0.8851\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2828 - accuracy: 0.8950\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2737 - accuracy: 0.8989\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2644 - accuracy: 0.9010\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2604 - accuracy: 0.9047\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2458 - accuracy: 0.9081\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2428 - accuracy: 0.9103\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2329 - accuracy: 0.9142\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2243 - accuracy: 0.9175\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2203 - accuracy: 0.9177\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2125 - accuracy: 0.9210\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2079 - accuracy: 0.9229\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2028 - accuracy: 0.9249\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1933 - accuracy: 0.9283\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.2013 - accuracy: 0.9253\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1959 - accuracy: 0.9270\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1878 - accuracy: 0.9307\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1762 - accuracy: 0.9352\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1789 - accuracy: 0.9346\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1656 - accuracy: 0.9370\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1680 - accuracy: 0.9368\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1768 - accuracy: 0.9340\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1562 - accuracy: 0.9397\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1642 - accuracy: 0.9386\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1519 - accuracy: 0.9431\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1433 - accuracy: 0.9471\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1472 - accuracy: 0.9439\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1449 - accuracy: 0.9461\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1437 - accuracy: 0.9453\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1348 - accuracy: 0.9498\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1349 - accuracy: 0.9492\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1369 - accuracy: 0.9493\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1253 - accuracy: 0.9525\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1241 - accuracy: 0.9525\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1185 - accuracy: 0.9545\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1144 - accuracy: 0.9563\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1142 - accuracy: 0.9574\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1147 - accuracy: 0.9572\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1076 - accuracy: 0.9594\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1095 - accuracy: 0.9596\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1075 - accuracy: 0.9604\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1053 - accuracy: 0.9599\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1006 - accuracy: 0.9615\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1045 - accuracy: 0.9612\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1026 - accuracy: 0.9620\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.0990 - accuracy: 0.9624\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1016 - accuracy: 0.9620\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.1024 - accuracy: 0.9631\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.0900 - accuracy: 0.9674\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.0848 - accuracy: 0.9687\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.0936 - accuracy: 0.9655\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.0982 - accuracy: 0.9642\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.0817 - accuracy: 0.9697\n",
      "76/76 [==============================] - 1s 17ms/step - loss: 0.2715 - accuracy: 0.9376\n",
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_147 (Conv1D)          (None, 89, 768)           5376      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 44, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 42, 256)           590080    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_49 (Averag (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 19, 256)           196864    \n",
      "_________________________________________________________________\n",
      "lstm_95 (LSTM)               (None, 19, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_96 (LSTM)               (None, 19, 128)           197120    \n",
      "_________________________________________________________________\n",
      "flatten_47 (Flatten)         (None, 2432)              0         \n",
      "_________________________________________________________________\n",
      "dense_141 (Dense)            (None, 384)               934272    \n",
      "_________________________________________________________________\n",
      "dropout_94 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_142 (Dense)            (None, 384)               147840    \n",
      "_________________________________________________________________\n",
      "dropout_95 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_143 (Dense)            (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 2,599,174\n",
      "Trainable params: 2,599,174\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 2,  n_conv_2 = 256, k_conv_2 = 3, \n",
      "          n_conv_3 = 256,  k_conv_3 = 3,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.1,\n",
      "          n_dense_1 = 384, dropout_1 = 0.2,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x00000252062F3D48>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 10s 63ms/step - loss: 1.0321 - accuracy: 0.5579\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.6954 - accuracy: 0.7142\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.5325 - accuracy: 0.7910 0s - loss: 0.5322 - accuracy: 0.\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 9s 62ms/step - loss: 0.4389 - accuracy: 0.8291\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.3884 - accuracy: 0.8498\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.3409 - accuracy: 0.8692 0s - loss: 0.3409 - accuracy\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.3181 - accuracy: 0.8779\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2990 - accuracy: 0.8861\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2941 - accuracy: 0.8885\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2793 - accuracy: 0.8936\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2657 - accuracy: 0.8986\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2526 - accuracy: 0.9044\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2489 - accuracy: 0.9057\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2456 - accuracy: 0.9077\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2357 - accuracy: 0.9120\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2216 - accuracy: 0.9159\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2170 - accuracy: 0.9158\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2047 - accuracy: 0.9198\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2076 - accuracy: 0.9227\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1957 - accuracy: 0.9260\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1941 - accuracy: 0.9265\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1889 - accuracy: 0.9275\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1857 - accuracy: 0.9283\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1785 - accuracy: 0.9322\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1675 - accuracy: 0.9347\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1624 - accuracy: 0.9370\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1630 - accuracy: 0.9374\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1555 - accuracy: 0.9393\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1544 - accuracy: 0.9406\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1446 - accuracy: 0.9450\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1360 - accuracy: 0.9469\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1428 - accuracy: 0.9437\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1354 - accuracy: 0.9478\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1297 - accuracy: 0.9494\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1308 - accuracy: 0.9505\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1279 - accuracy: 0.9501\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1196 - accuracy: 0.9517\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1223 - accuracy: 0.9514\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1127 - accuracy: 0.9556\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1134 - accuracy: 0.9555\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1129 - accuracy: 0.9555\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1100 - accuracy: 0.9563\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1070 - accuracy: 0.9573\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1051 - accuracy: 0.9577\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1013 - accuracy: 0.9600\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1023 - accuracy: 0.9596\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0976 - accuracy: 0.9616\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0992 - accuracy: 0.9609\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1006 - accuracy: 0.9607\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0982 - accuracy: 0.9631\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0878 - accuracy: 0.9662\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0937 - accuracy: 0.9640\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0860 - accuracy: 0.9665\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0826 - accuracy: 0.9682\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0800 - accuracy: 0.9689\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0839 - accuracy: 0.9689\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0818 - accuracy: 0.9685\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0775 - accuracy: 0.9708\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0803 - accuracy: 0.9693\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0743 - accuracy: 0.9718\n",
      "76/76 [==============================] - 2s 23ms/step - loss: 0.3001 - accuracy: 0.9331\n",
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_150 (Conv1D)          (None, 89, 768)           5376      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 44, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 42, 256)           590080    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_50 (Averag (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 19, 256)           196864    \n",
      "_________________________________________________________________\n",
      "lstm_97 (LSTM)               (None, 19, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_98 (LSTM)               (None, 19, 128)           197120    \n",
      "_________________________________________________________________\n",
      "flatten_48 (Flatten)         (None, 2432)              0         \n",
      "_________________________________________________________________\n",
      "dense_144 (Dense)            (None, 384)               934272    \n",
      "_________________________________________________________________\n",
      "dropout_96 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_145 (Dense)            (None, 384)               147840    \n",
      "_________________________________________________________________\n",
      "dropout_97 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_146 (Dense)            (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 2,599,174\n",
      "Trainable params: 2,599,174\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 2,  n_conv_2 = 256, k_conv_2 = 3, \n",
      "          n_conv_3 = 256,  k_conv_3 = 3,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.1,\n",
      "          n_dense_1 = 384, dropout_1 = 0.2,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x00000252062F3D48>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 9s 62ms/step - loss: 1.0499 - accuracy: 0.5547\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.6840 - accuracy: 0.7190\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.5213 - accuracy: 0.7941\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.4222 - accuracy: 0.8374\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.3868 - accuracy: 0.8487\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.3432 - accuracy: 0.8678\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.3179 - accuracy: 0.8781\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 9s 62ms/step - loss: 0.3083 - accuracy: 0.8800\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2882 - accuracy: 0.8898\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2820 - accuracy: 0.8908\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 9s 62ms/step - loss: 0.2714 - accuracy: 0.8943\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2591 - accuracy: 0.8987 0s - loss: 0.260\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2485 - accuracy: 0.9039\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2343 - accuracy: 0.9080\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2282 - accuracy: 0.9125\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2147 - accuracy: 0.9154\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2163 - accuracy: 0.9177\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2025 - accuracy: 0.9221\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1992 - accuracy: 0.9237\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2004 - accuracy: 0.9245\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1874 - accuracy: 0.9293\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1786 - accuracy: 0.9301\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1729 - accuracy: 0.9343\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1812 - accuracy: 0.9312\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1650 - accuracy: 0.9353\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1559 - accuracy: 0.9391\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1555 - accuracy: 0.9386\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1530 - accuracy: 0.9409\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1543 - accuracy: 0.9396\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1424 - accuracy: 0.9443\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1477 - accuracy: 0.9420\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1456 - accuracy: 0.9446\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 9s 62ms/step - loss: 0.1300 - accuracy: 0.9491\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1270 - accuracy: 0.9513\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1241 - accuracy: 0.9520\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1166 - accuracy: 0.9545\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1149 - accuracy: 0.9547\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1141 - accuracy: 0.9547\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1127 - accuracy: 0.9556\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1186 - accuracy: 0.9540\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1091 - accuracy: 0.9573\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1091 - accuracy: 0.9592\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1003 - accuracy: 0.9620\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1055 - accuracy: 0.9602\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1006 - accuracy: 0.9616\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0949 - accuracy: 0.9638\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0894 - accuracy: 0.9655\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0933 - accuracy: 0.9647\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0963 - accuracy: 0.9646\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0878 - accuracy: 0.9663\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0835 - accuracy: 0.9681\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0895 - accuracy: 0.9651\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0782 - accuracy: 0.9711\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0775 - accuracy: 0.9701\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0789 - accuracy: 0.9705\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0729 - accuracy: 0.9714\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0803 - accuracy: 0.9698\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0767 - accuracy: 0.9708\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0688 - accuracy: 0.9738\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0692 - accuracy: 0.9749\n",
      "76/76 [==============================] - 2s 22ms/step - loss: 0.3167 - accuracy: 0.9377\n",
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_153 (Conv1D)          (None, 89, 768)           5376      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 44, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 42, 256)           590080    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_51 (Averag (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 19, 256)           196864    \n",
      "_________________________________________________________________\n",
      "lstm_99 (LSTM)               (None, 19, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_100 (LSTM)              (None, 19, 128)           197120    \n",
      "_________________________________________________________________\n",
      "flatten_49 (Flatten)         (None, 2432)              0         \n",
      "_________________________________________________________________\n",
      "dense_147 (Dense)            (None, 384)               934272    \n",
      "_________________________________________________________________\n",
      "dropout_98 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_148 (Dense)            (None, 384)               147840    \n",
      "_________________________________________________________________\n",
      "dropout_99 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_149 (Dense)            (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 2,599,174\n",
      "Trainable params: 2,599,174\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 2,  n_conv_2 = 256, k_conv_2 = 3, \n",
      "          n_conv_3 = 256,  k_conv_3 = 3,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.1,\n",
      "          n_dense_1 = 384, dropout_1 = 0.2,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x00000252062F3D48>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 9s 61ms/step - loss: 1.0545 - accuracy: 0.5513\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.6836 - accuracy: 0.7224\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.5223 - accuracy: 0.7964\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.4290 - accuracy: 0.8386\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.3712 - accuracy: 0.8600\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.3497 - accuracy: 0.8699\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.3236 - accuracy: 0.8777\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.3045 - accuracy: 0.8859\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2863 - accuracy: 0.8909\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2770 - accuracy: 0.8957\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2679 - accuracy: 0.8989\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2539 - accuracy: 0.9062\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2554 - accuracy: 0.9050\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2362 - accuracy: 0.9129 0s - loss: 0.235\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2655 - accuracy: 0.9022\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2247 - accuracy: 0.9161\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2198 - accuracy: 0.9184\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2079 - accuracy: 0.9207\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.2056 - accuracy: 0.9231\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1969 - accuracy: 0.9267\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1893 - accuracy: 0.9286\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1860 - accuracy: 0.9286\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1879 - accuracy: 0.9282\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1736 - accuracy: 0.9333\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1685 - accuracy: 0.9362\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1739 - accuracy: 0.9337\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1648 - accuracy: 0.9371\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1547 - accuracy: 0.9396\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1453 - accuracy: 0.9439\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1530 - accuracy: 0.9417\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1441 - accuracy: 0.9447\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1321 - accuracy: 0.9492\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1396 - accuracy: 0.9461\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1299 - accuracy: 0.9511\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1233 - accuracy: 0.9532\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1374 - accuracy: 0.9491\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1256 - accuracy: 0.9532\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1231 - accuracy: 0.9542\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1145 - accuracy: 0.9576\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1099 - accuracy: 0.9590\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1059 - accuracy: 0.9620\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1086 - accuracy: 0.9607\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1032 - accuracy: 0.9600\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1023 - accuracy: 0.9617\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0977 - accuracy: 0.9636\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1002 - accuracy: 0.9619\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1005 - accuracy: 0.9629\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0963 - accuracy: 0.9635\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0961 - accuracy: 0.9647\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0906 - accuracy: 0.9657\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0809 - accuracy: 0.9702\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0785 - accuracy: 0.9710\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0820 - accuracy: 0.9698\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0884 - accuracy: 0.9677\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0843 - accuracy: 0.9686\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0798 - accuracy: 0.9717\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0776 - accuracy: 0.9724\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0879 - accuracy: 0.9676\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0815 - accuracy: 0.9696\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.0786 - accuracy: 0.9707\n",
      "76/76 [==============================] - 2s 23ms/step - loss: 0.3152 - accuracy: 0.9393\n",
      "Model: \"sequential_52\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_156 (Conv1D)          (None, 88, 768)           7680      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 44, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 43, 512)           786944    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_52 (Averag (None, 21, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 20, 512)           524800    \n",
      "_________________________________________________________________\n",
      "lstm_101 (LSTM)              (None, 20, 128)           328192    \n",
      "_________________________________________________________________\n",
      "lstm_102 (LSTM)              (None, 20, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_50 (Flatten)         (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense_150 (Dense)            (None, 384)               983424    \n",
      "_________________________________________________________________\n",
      "dropout_100 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_151 (Dense)            (None, 384)               147840    \n",
      "_________________________________________________________________\n",
      "dropout_101 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_152 (Dense)            (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 2,912,774\n",
      "Trainable params: 2,912,774\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 3,  n_conv_2 = 512, k_conv_2 = 2, \n",
      "          n_conv_3 = 512,  k_conv_3 = 2,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 128,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 384, dropout_1 = 0.2,\n",
      "          n_dense_2 = 384, dropout_2 = 0.2, activation_conv=  relu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x00000252062F3D48>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 11s 71ms/step - loss: 1.0174 - accuracy: 0.5657\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 11s 70ms/step - loss: 0.6539 - accuracy: 0.7361\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.4655 - accuracy: 0.8210\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 11s 70ms/step - loss: 0.3914 - accuracy: 0.8502\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.3594 - accuracy: 0.8603\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.3241 - accuracy: 0.8756\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.3118 - accuracy: 0.8818\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2907 - accuracy: 0.8887\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2894 - accuracy: 0.8920\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2595 - accuracy: 0.9030\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2497 - accuracy: 0.9048\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2513 - accuracy: 0.9055\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2357 - accuracy: 0.9121\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2212 - accuracy: 0.9164\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2307 - accuracy: 0.9120\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2185 - accuracy: 0.9162\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2024 - accuracy: 0.9224\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1980 - accuracy: 0.9239\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1910 - accuracy: 0.9266\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1910 - accuracy: 0.9282\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1777 - accuracy: 0.9328\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1778 - accuracy: 0.9320\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1673 - accuracy: 0.9352\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1818 - accuracy: 0.9317\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1532 - accuracy: 0.9406\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1581 - accuracy: 0.9384\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1537 - accuracy: 0.9409\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1397 - accuracy: 0.9452\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1416 - accuracy: 0.9453\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1340 - accuracy: 0.9484\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1245 - accuracy: 0.9516\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1382 - accuracy: 0.9466\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1228 - accuracy: 0.9503\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1194 - accuracy: 0.9528\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1161 - accuracy: 0.9544\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1173 - accuracy: 0.9539\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1134 - accuracy: 0.9555\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1209 - accuracy: 0.9533\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1116 - accuracy: 0.9561\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1013 - accuracy: 0.9598\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1002 - accuracy: 0.9612\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1001 - accuracy: 0.9603\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1003 - accuracy: 0.9616\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.0963 - accuracy: 0.9622\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0915 - accuracy: 0.9646\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0932 - accuracy: 0.9647\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.0908 - accuracy: 0.9653\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.0930 - accuracy: 0.9636\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0847 - accuracy: 0.9677\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0977 - accuracy: 0.9639\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0772 - accuracy: 0.9703\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0834 - accuracy: 0.9686\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.0762 - accuracy: 0.9720\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0932 - accuracy: 0.9659\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0771 - accuracy: 0.9705\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0723 - accuracy: 0.9731\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.0779 - accuracy: 0.9708\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0755 - accuracy: 0.9718\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0774 - accuracy: 0.9710\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0633 - accuracy: 0.9759\n",
      "76/76 [==============================] - 2s 26ms/step - loss: 0.2843 - accuracy: 0.9441\n",
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_159 (Conv1D)          (None, 88, 768)           7680      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 44, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 43, 512)           786944    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_53 (Averag (None, 21, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 20, 512)           524800    \n",
      "_________________________________________________________________\n",
      "lstm_103 (LSTM)              (None, 20, 128)           328192    \n",
      "_________________________________________________________________\n",
      "lstm_104 (LSTM)              (None, 20, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_51 (Flatten)         (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense_153 (Dense)            (None, 384)               983424    \n",
      "_________________________________________________________________\n",
      "dropout_102 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_154 (Dense)            (None, 384)               147840    \n",
      "_________________________________________________________________\n",
      "dropout_103 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_155 (Dense)            (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 2,912,774\n",
      "Trainable params: 2,912,774\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 3,  n_conv_2 = 512, k_conv_2 = 2, \n",
      "          n_conv_3 = 512,  k_conv_3 = 2,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 128,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 384, dropout_1 = 0.2,\n",
      "          n_dense_2 = 384, dropout_2 = 0.2, activation_conv=  relu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x00000252062F3D48>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 11s 70ms/step - loss: 1.0424 - accuracy: 0.5535\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.6314 - accuracy: 0.7477\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.4704 - accuracy: 0.8189\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.3882 - accuracy: 0.8514\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.3504 - accuracy: 0.8642\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.3307 - accuracy: 0.8744\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.3124 - accuracy: 0.8806\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2920 - accuracy: 0.8868\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2726 - accuracy: 0.8953\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2625 - accuracy: 0.9001\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2681 - accuracy: 0.8984\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2485 - accuracy: 0.9044\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2395 - accuracy: 0.9087\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2264 - accuracy: 0.9132\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2223 - accuracy: 0.9147\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2255 - accuracy: 0.9154\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2046 - accuracy: 0.9213\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1984 - accuracy: 0.9244\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1847 - accuracy: 0.9287\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1873 - accuracy: 0.9283\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1718 - accuracy: 0.9336\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1742 - accuracy: 0.9314\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1684 - accuracy: 0.9344\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1687 - accuracy: 0.9342\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1679 - accuracy: 0.9352\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1511 - accuracy: 0.9405\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1497 - accuracy: 0.9414\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1665 - accuracy: 0.9368\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1467 - accuracy: 0.9437\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1319 - accuracy: 0.9468\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1356 - accuracy: 0.9489\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1284 - accuracy: 0.9494\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1197 - accuracy: 0.9541\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1217 - accuracy: 0.9527\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1121 - accuracy: 0.9561\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1246 - accuracy: 0.9542\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1203 - accuracy: 0.9552\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1089 - accuracy: 0.9588\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1106 - accuracy: 0.9595\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1053 - accuracy: 0.9594\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1083 - accuracy: 0.9600\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0954 - accuracy: 0.9634\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0966 - accuracy: 0.9627\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0990 - accuracy: 0.9627\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0891 - accuracy: 0.9666\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0910 - accuracy: 0.9671\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0894 - accuracy: 0.9661\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0872 - accuracy: 0.9672\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0817 - accuracy: 0.9697\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0837 - accuracy: 0.9693\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0847 - accuracy: 0.9690\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0871 - accuracy: 0.9681\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0836 - accuracy: 0.9684\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0828 - accuracy: 0.9686\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0719 - accuracy: 0.9726\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0858 - accuracy: 0.9686\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0826 - accuracy: 0.9703\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0742 - accuracy: 0.9723\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0682 - accuracy: 0.9740\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0664 - accuracy: 0.9751\n",
      "76/76 [==============================] - 2s 26ms/step - loss: 0.2875 - accuracy: 0.9435\n",
      "Model: \"sequential_54\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_162 (Conv1D)          (None, 88, 768)           7680      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 44, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 43, 512)           786944    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_54 (Averag (None, 21, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 20, 512)           524800    \n",
      "_________________________________________________________________\n",
      "lstm_105 (LSTM)              (None, 20, 128)           328192    \n",
      "_________________________________________________________________\n",
      "lstm_106 (LSTM)              (None, 20, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_52 (Flatten)         (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense_156 (Dense)            (None, 384)               983424    \n",
      "_________________________________________________________________\n",
      "dropout_104 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_157 (Dense)            (None, 384)               147840    \n",
      "_________________________________________________________________\n",
      "dropout_105 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_158 (Dense)            (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 2,912,774\n",
      "Trainable params: 2,912,774\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 3,  n_conv_2 = 512, k_conv_2 = 2, \n",
      "          n_conv_3 = 512,  k_conv_3 = 2,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 128,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 384, dropout_1 = 0.2,\n",
      "          n_dense_2 = 384, dropout_2 = 0.2, activation_conv=  relu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x00000252062F3D48>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2/152 [..............................] - ETA: 8s - loss: 1.7717 - accuracy: 0.2324WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0209s vs `on_train_batch_end` time: 0.0501s). Check your callbacks.\n",
      "152/152 [==============================] - 11s 70ms/step - loss: 1.0359 - accuracy: 0.5571\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.6218 - accuracy: 0.7502\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.4646 - accuracy: 0.8166\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.3920 - accuracy: 0.8491\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.3610 - accuracy: 0.8606\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.3250 - accuracy: 0.8744\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.3133 - accuracy: 0.8796\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2931 - accuracy: 0.8852\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2800 - accuracy: 0.89240s - loss: 0.2789 - \n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2721 - accuracy: 0.8962\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2654 - accuracy: 0.8983\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2472 - accuracy: 0.9062\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2520 - accuracy: 0.9057\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2294 - accuracy: 0.9133\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2262 - accuracy: 0.9141\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2094 - accuracy: 0.9209\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2062 - accuracy: 0.9222\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2018 - accuracy: 0.9228\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1976 - accuracy: 0.9244\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - ETA: 0s - loss: 0.1860 - accuracy: 0.92 - 10s 69ms/step - loss: 0.1861 - accuracy: 0.9278\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1850 - accuracy: 0.9285\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1722 - accuracy: 0.9334\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1793 - accuracy: 0.9323\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1672 - accuracy: 0.9354\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1576 - accuracy: 0.9393\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1553 - accuracy: 0.9405\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1512 - accuracy: 0.9426\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1433 - accuracy: 0.9445\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1484 - accuracy: 0.9425\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1365 - accuracy: 0.9468\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1377 - accuracy: 0.9467\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1327 - accuracy: 0.9492\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1206 - accuracy: 0.9544\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1272 - accuracy: 0.9509\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1125 - accuracy: 0.9581\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1285 - accuracy: 0.9510\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1187 - accuracy: 0.9564\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1066 - accuracy: 0.9594\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1031 - accuracy: 0.9615\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1163 - accuracy: 0.9571\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1045 - accuracy: 0.9607\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0981 - accuracy: 0.9611\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0988 - accuracy: 0.9635\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0969 - accuracy: 0.9632\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.0914 - accuracy: 0.9658\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0876 - accuracy: 0.9672\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0964 - accuracy: 0.9645\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0990 - accuracy: 0.9639\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0839 - accuracy: 0.9688\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0871 - accuracy: 0.9691\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0894 - accuracy: 0.9663\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0820 - accuracy: 0.9703\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0792 - accuracy: 0.9696\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0800 - accuracy: 0.9704\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0852 - accuracy: 0.9680\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0786 - accuracy: 0.9711\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0732 - accuracy: 0.9720\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0759 - accuracy: 0.9721\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0699 - accuracy: 0.9727\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.0715 - accuracy: 0.9736\n",
      "76/76 [==============================] - 2s 26ms/step - loss: 0.2899 - accuracy: 0.9470\n",
      "Model: \"sequential_55\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_165 (Conv1D)          (None, 88, 768)           7680      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 44, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 42, 256)           590080    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_55 (Averag (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 19, 256)           196864    \n",
      "_________________________________________________________________\n",
      "lstm_107 (LSTM)              (None, 19, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_108 (LSTM)              (None, 19, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_53 (Flatten)         (None, 4864)              0         \n",
      "_________________________________________________________________\n",
      "dense_159 (Dense)            (None, 256)               1245440   \n",
      "_________________________________________________________________\n",
      "dropout_106 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_160 (Dense)            (None, 384)               98688     \n",
      "_________________________________________________________________\n",
      "dropout_107 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_161 (Dense)            (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 2,732,422\n",
      "Trainable params: 2,732,422\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 3,  n_conv_2 = 256, k_conv_2 = 3, \n",
      "          n_conv_3 = 256,  k_conv_3 = 3,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000002527E6DD448>,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 9s 60ms/step - loss: 1.1036 - accuracy: 0.5239\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.8097 - accuracy: 0.6572\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.6221 - accuracy: 0.7469 0s - loss: 0.6\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.5309 - accuracy: 0.7844\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.4730 - accuracy: 0.8094\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.4397 - accuracy: 0.8224\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.4121 - accuracy: 0.8341\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3964 - accuracy: 0.8392\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3705 - accuracy: 0.8504\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3638 - accuracy: 0.8519\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3464 - accuracy: 0.8573\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3346 - accuracy: 0.8637\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3226 - accuracy: 0.8692\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3193 - accuracy: 0.8745\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2958 - accuracy: 0.8830\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2932 - accuracy: 0.8830\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2713 - accuracy: 0.8935\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2692 - accuracy: 0.8934\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2504 - accuracy: 0.9026\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2515 - accuracy: 0.9010\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2324 - accuracy: 0.9089\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2440 - accuracy: 0.9063\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2111 - accuracy: 0.9194\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2315 - accuracy: 0.9130\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2064 - accuracy: 0.9216\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1987 - accuracy: 0.9242\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1918 - accuracy: 0.9252\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1884 - accuracy: 0.9281\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1882 - accuracy: 0.9282\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1639 - accuracy: 0.9351\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1578 - accuracy: 0.9396\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1627 - accuracy: 0.9368\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1735 - accuracy: 0.9327\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1557 - accuracy: 0.9395\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1514 - accuracy: 0.9435\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1424 - accuracy: 0.9451\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1554 - accuracy: 0.9405\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1395 - accuracy: 0.9465\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1463 - accuracy: 0.9440\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1256 - accuracy: 0.9513\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1357 - accuracy: 0.9499\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1227 - accuracy: 0.9527\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1317 - accuracy: 0.9506\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1150 - accuracy: 0.9568\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1221 - accuracy: 0.9540\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1328 - accuracy: 0.9499\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1194 - accuracy: 0.9550\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1194 - accuracy: 0.9544\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1016 - accuracy: 0.9617\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1145 - accuracy: 0.9578\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1158 - accuracy: 0.9556\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1289 - accuracy: 0.9518\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1010 - accuracy: 0.9619\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1114 - accuracy: 0.9584\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0969 - accuracy: 0.9621\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0993 - accuracy: 0.9623\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1091 - accuracy: 0.9585\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1049 - accuracy: 0.9605 1s\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0947 - accuracy: 0.9647\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0899 - accuracy: 0.9655\n",
      "76/76 [==============================] - 2s 22ms/step - loss: 0.3044 - accuracy: 0.9270\n",
      "Model: \"sequential_56\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_168 (Conv1D)          (None, 88, 768)           7680      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 44, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 42, 256)           590080    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_56 (Averag (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 19, 256)           196864    \n",
      "_________________________________________________________________\n",
      "lstm_109 (LSTM)              (None, 19, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_110 (LSTM)              (None, 19, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_54 (Flatten)         (None, 4864)              0         \n",
      "_________________________________________________________________\n",
      "dense_162 (Dense)            (None, 256)               1245440   \n",
      "_________________________________________________________________\n",
      "dropout_108 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_163 (Dense)            (None, 384)               98688     \n",
      "_________________________________________________________________\n",
      "dropout_109 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_164 (Dense)            (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 2,732,422\n",
      "Trainable params: 2,732,422\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 3,  n_conv_2 = 256, k_conv_2 = 3, \n",
      "          n_conv_3 = 256,  k_conv_3 = 3,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000002527E6DD448>,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 9s 61ms/step - loss: 1.0932 - accuracy: 0.5279\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.7920 - accuracy: 0.6717\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.6125 - accuracy: 0.7503\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.5250 - accuracy: 0.7847\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.4695 - accuracy: 0.8063\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.4287 - accuracy: 0.8264\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3977 - accuracy: 0.8380\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3927 - accuracy: 0.8416\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3727 - accuracy: 0.8474\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3660 - accuracy: 0.8539\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3375 - accuracy: 0.8630\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3211 - accuracy: 0.8726\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3148 - accuracy: 0.8746 0s - loss: 0.3139 - accura\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3207 - accuracy: 0.8725\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3059 - accuracy: 0.8795\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2793 - accuracy: 0.8864\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2834 - accuracy: 0.8885\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2622 - accuracy: 0.8947\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2525 - accuracy: 0.9027\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2575 - accuracy: 0.8987\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2348 - accuracy: 0.9064\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2342 - accuracy: 0.9069\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2132 - accuracy: 0.9176\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2268 - accuracy: 0.9138\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2205 - accuracy: 0.9150\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1963 - accuracy: 0.9248\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1874 - accuracy: 0.9283 0s - loss: 0.1866 - accura\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1788 - accuracy: 0.9313\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2001 - accuracy: 0.9245\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1845 - accuracy: 0.9300\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1775 - accuracy: 0.9315\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1607 - accuracy: 0.9404\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1671 - accuracy: 0.9374\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1562 - accuracy: 0.9414\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1409 - accuracy: 0.9458\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1516 - accuracy: 0.9422\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1386 - accuracy: 0.9483\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1449 - accuracy: 0.9445\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1450 - accuracy: 0.9447 \n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1472 - accuracy: 0.9446\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1313 - accuracy: 0.9515\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1286 - accuracy: 0.9513\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1353 - accuracy: 0.9497\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1339 - accuracy: 0.9492\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1126 - accuracy: 0.9575\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1261 - accuracy: 0.9522\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1104 - accuracy: 0.9579\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1201 - accuracy: 0.9537\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1410 - accuracy: 0.9475\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1051 - accuracy: 0.9598\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1115 - accuracy: 0.9568\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1164 - accuracy: 0.9556\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1089 - accuracy: 0.9592 0s - loss: 0.1091 - accu\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1118 - accuracy: 0.9574\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1065 - accuracy: 0.9602\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0987 - accuracy: 0.9618\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0999 - accuracy: 0.9627\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0997 - accuracy: 0.9619\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1065 - accuracy: 0.9599\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1073 - accuracy: 0.9595\n",
      "76/76 [==============================] - 2s 22ms/step - loss: 0.3203 - accuracy: 0.9223\n",
      "Model: \"sequential_57\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_171 (Conv1D)          (None, 88, 768)           7680      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 44, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 42, 256)           590080    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_57 (Averag (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 19, 256)           196864    \n",
      "_________________________________________________________________\n",
      "lstm_111 (LSTM)              (None, 19, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_112 (LSTM)              (None, 19, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_55 (Flatten)         (None, 4864)              0         \n",
      "_________________________________________________________________\n",
      "dense_165 (Dense)            (None, 256)               1245440   \n",
      "_________________________________________________________________\n",
      "dropout_110 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_166 (Dense)            (None, 384)               98688     \n",
      "_________________________________________________________________\n",
      "dropout_111 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_167 (Dense)            (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 2,732,422\n",
      "Trainable params: 2,732,422\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 3,  n_conv_2 = 256, k_conv_2 = 3, \n",
      "          n_conv_3 = 256,  k_conv_3 = 3,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000002527E6DD448>,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 9s 60ms/step - loss: 1.1140 - accuracy: 0.5173\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.8501 - accuracy: 0.6377\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.6713 - accuracy: 0.7286\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.5485 - accuracy: 0.7773\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.4982 - accuracy: 0.7949\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.4570 - accuracy: 0.8132\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.4228 - accuracy: 0.8264\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.4081 - accuracy: 0.8304\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3832 - accuracy: 0.8426\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3823 - accuracy: 0.8431\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3624 - accuracy: 0.8506\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3447 - accuracy: 0.8592\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3440 - accuracy: 0.8607\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3191 - accuracy: 0.8711\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3043 - accuracy: 0.8778\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2947 - accuracy: 0.8833\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2793 - accuracy: 0.8903\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2676 - accuracy: 0.8943\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2530 - accuracy: 0.9007\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2570 - accuracy: 0.8973\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2389 - accuracy: 0.9067\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2321 - accuracy: 0.9112\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2239 - accuracy: 0.9125\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2044 - accuracy: 0.9203\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2057 - accuracy: 0.9196\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1897 - accuracy: 0.9261\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1892 - accuracy: 0.9251\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1807 - accuracy: 0.9293\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1782 - accuracy: 0.9308\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1612 - accuracy: 0.9369\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1643 - accuracy: 0.9365 0s -\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1674 - accuracy: 0.9355\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1529 - accuracy: 0.9417\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1469 - accuracy: 0.9433\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1522 - accuracy: 0.9409\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1370 - accuracy: 0.9462\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1443 - accuracy: 0.9445\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1331 - accuracy: 0.9503\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1348 - accuracy: 0.9480\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1416 - accuracy: 0.9461\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1413 - accuracy: 0.9471\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1248 - accuracy: 0.9507\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1204 - accuracy: 0.9537 0s - loss: 0.1212 - accu\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1263 - accuracy: 0.9517\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1104 - accuracy: 0.9574\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 9s 61ms/step - loss: 0.1074 - accuracy: 0.9581\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1168 - accuracy: 0.9549\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1159 - accuracy: 0.9576\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1172 - accuracy: 0.9559\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1047 - accuracy: 0.9605\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1031 - accuracy: 0.9611\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1104 - accuracy: 0.9586\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0976 - accuracy: 0.9641\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1139 - accuracy: 0.9568\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1051 - accuracy: 0.9606\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1096 - accuracy: 0.9587\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1056 - accuracy: 0.9603\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1026 - accuracy: 0.9609\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0971 - accuracy: 0.9624\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0895 - accuracy: 0.9668\n",
      "76/76 [==============================] - 2s 22ms/step - loss: 0.2743 - accuracy: 0.9392\n",
      "Model: \"sequential_58\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_174 (Conv1D)          (None, 88, 768)           7680      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 29, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_175 (Conv1D)          (None, 27, 256)           590080    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_58 (Averag (None, 9, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_176 (Conv1D)          (None, 8, 512)            262656    \n",
      "_________________________________________________________________\n",
      "lstm_113 (LSTM)              (None, 8, 128)            328192    \n",
      "_________________________________________________________________\n",
      "lstm_114 (LSTM)              (None, 8, 256)            394240    \n",
      "_________________________________________________________________\n",
      "flatten_56 (Flatten)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_168 (Dense)            (None, 384)               786816    \n",
      "_________________________________________________________________\n",
      "dropout_112 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_169 (Dense)            (None, 512)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_113 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_170 (Dense)            (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 2,569,862\n",
      "Trainable params: 2,569,862\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 3,  n_conv_2 = 256, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 2,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 3,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 384, dropout_1 = 0.2,\n",
      "          n_dense_2 = 512, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 7s 44ms/step - loss: 1.0554 - accuracy: 0.5499\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.6720 - accuracy: 0.7320\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.4955 - accuracy: 0.8119\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.3987 - accuracy: 0.8494\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.3590 - accuracy: 0.8658\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.3249 - accuracy: 0.8788\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.3086 - accuracy: 0.8847\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2865 - accuracy: 0.8940\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2814 - accuracy: 0.8958\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2746 - accuracy: 0.8971\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2552 - accuracy: 0.9062\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2395 - accuracy: 0.9116\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2390 - accuracy: 0.9119\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2219 - accuracy: 0.9171\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2226 - accuracy: 0.9178\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2147 - accuracy: 0.9200\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2081 - accuracy: 0.9218\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2040 - accuracy: 0.9242\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1897 - accuracy: 0.9288\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1977 - accuracy: 0.9272\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1869 - accuracy: 0.9305\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1777 - accuracy: 0.9333\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1744 - accuracy: 0.9347\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1722 - accuracy: 0.9356\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1744 - accuracy: 0.9355\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1681 - accuracy: 0.9371\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1499 - accuracy: 0.9425\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1564 - accuracy: 0.9409\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1540 - accuracy: 0.9436\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1382 - accuracy: 0.9487\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1359 - accuracy: 0.9489\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1360 - accuracy: 0.9476\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1369 - accuracy: 0.9484\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1272 - accuracy: 0.9517\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1261 - accuracy: 0.9519\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1190 - accuracy: 0.9560\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1178 - accuracy: 0.9557\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1206 - accuracy: 0.9535\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1216 - accuracy: 0.9544\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1245 - accuracy: 0.9543\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1065 - accuracy: 0.9603\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1037 - accuracy: 0.9609\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1116 - accuracy: 0.9594\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1026 - accuracy: 0.9627\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1035 - accuracy: 0.9615\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0995 - accuracy: 0.9636\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1059 - accuracy: 0.9614\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0942 - accuracy: 0.9649\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0968 - accuracy: 0.9641\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0928 - accuracy: 0.9654\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0879 - accuracy: 0.9668\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0896 - accuracy: 0.9668\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0880 - accuracy: 0.9677\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0868 - accuracy: 0.9680\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0871 - accuracy: 0.9683\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0827 - accuracy: 0.9700\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0774 - accuracy: 0.9713\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0743 - accuracy: 0.9725\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0797 - accuracy: 0.9701\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0771 - accuracy: 0.9726\n",
      "76/76 [==============================] - 1s 17ms/step - loss: 0.2730 - accuracy: 0.9411\n",
      "Model: \"sequential_59\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_177 (Conv1D)          (None, 88, 768)           7680      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 29, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_178 (Conv1D)          (None, 27, 256)           590080    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_59 (Averag (None, 9, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_179 (Conv1D)          (None, 8, 512)            262656    \n",
      "_________________________________________________________________\n",
      "lstm_115 (LSTM)              (None, 8, 128)            328192    \n",
      "_________________________________________________________________\n",
      "lstm_116 (LSTM)              (None, 8, 256)            394240    \n",
      "_________________________________________________________________\n",
      "flatten_57 (Flatten)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_171 (Dense)            (None, 384)               786816    \n",
      "_________________________________________________________________\n",
      "dropout_114 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_172 (Dense)            (None, 512)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_115 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_173 (Dense)            (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 2,569,862\n",
      "Trainable params: 2,569,862\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 3,  n_conv_2 = 256, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 2,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 3,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 384, dropout_1 = 0.2,\n",
      "          n_dense_2 = 512, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 7s 45ms/step - loss: 1.0453 - accuracy: 0.5554\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.6571 - accuracy: 0.7367\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.4847 - accuracy: 0.8166\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.3922 - accuracy: 0.8525\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.3617 - accuracy: 0.8644\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.3268 - accuracy: 0.8778\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.3067 - accuracy: 0.8836\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2899 - accuracy: 0.8887\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2794 - accuracy: 0.8966\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2687 - accuracy: 0.8987\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2546 - accuracy: 0.9048\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2435 - accuracy: 0.9098\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2350 - accuracy: 0.9123\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2266 - accuracy: 0.9154\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2151 - accuracy: 0.9186\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2155 - accuracy: 0.9192\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2025 - accuracy: 0.9254\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2090 - accuracy: 0.9222\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1958 - accuracy: 0.9261\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2025 - accuracy: 0.9256\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1821 - accuracy: 0.9320\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1762 - accuracy: 0.9332\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1719 - accuracy: 0.9343\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1685 - accuracy: 0.9376\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1713 - accuracy: 0.9335\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1550 - accuracy: 0.9413\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1600 - accuracy: 0.9389\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1485 - accuracy: 0.9432\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1498 - accuracy: 0.9441\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1411 - accuracy: 0.9451\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1354 - accuracy: 0.9481\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1334 - accuracy: 0.9478\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1328 - accuracy: 0.9496\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1262 - accuracy: 0.9513\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1286 - accuracy: 0.9505\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1154 - accuracy: 0.9558\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1234 - accuracy: 0.9529\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1186 - accuracy: 0.9544\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1240 - accuracy: 0.9536\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1077 - accuracy: 0.9578\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1108 - accuracy: 0.9578\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1049 - accuracy: 0.9596\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0971 - accuracy: 0.9633\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1042 - accuracy: 0.9596\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0991 - accuracy: 0.9636\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0902 - accuracy: 0.9663\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0968 - accuracy: 0.9635\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0919 - accuracy: 0.9660\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0907 - accuracy: 0.9657\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0865 - accuracy: 0.9671\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0858 - accuracy: 0.9680\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0922 - accuracy: 0.9672\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0800 - accuracy: 0.9709\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0805 - accuracy: 0.9700\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0863 - accuracy: 0.9690\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0798 - accuracy: 0.9702\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0764 - accuracy: 0.9718\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0795 - accuracy: 0.9715\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0740 - accuracy: 0.9730\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0675 - accuracy: 0.9745\n",
      "76/76 [==============================] - 1s 17ms/step - loss: 0.2784 - accuracy: 0.9405\n",
      "Model: \"sequential_60\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_180 (Conv1D)          (None, 88, 768)           7680      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 29, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_181 (Conv1D)          (None, 27, 256)           590080    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_60 (Averag (None, 9, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_182 (Conv1D)          (None, 8, 512)            262656    \n",
      "_________________________________________________________________\n",
      "lstm_117 (LSTM)              (None, 8, 128)            328192    \n",
      "_________________________________________________________________\n",
      "lstm_118 (LSTM)              (None, 8, 256)            394240    \n",
      "_________________________________________________________________\n",
      "flatten_58 (Flatten)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_174 (Dense)            (None, 384)               786816    \n",
      "_________________________________________________________________\n",
      "dropout_116 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_175 (Dense)            (None, 512)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_117 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_176 (Dense)            (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 2,569,862\n",
      "Trainable params: 2,569,862\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 3,  n_conv_2 = 256, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 2,  maxpooling_pool_size = 3,\n",
      "          avepooling_pool_size = 3,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 384, dropout_1 = 0.2,\n",
      "          n_dense_2 = 512, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 7s 44ms/step - loss: 1.0673 - accuracy: 0.5477\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.6552 - accuracy: 0.7391\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.4806 - accuracy: 0.8176\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.4058 - accuracy: 0.8501\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.3567 - accuracy: 0.8669\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.3317 - accuracy: 0.8750\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.3097 - accuracy: 0.8842\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.3021 - accuracy: 0.8864\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2790 - accuracy: 0.8962\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2764 - accuracy: 0.8984\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2562 - accuracy: 0.9049\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2444 - accuracy: 0.9096\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2450 - accuracy: 0.9099\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2348 - accuracy: 0.9130\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2300 - accuracy: 0.9138\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2196 - accuracy: 0.9189\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2140 - accuracy: 0.9201\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2122 - accuracy: 0.9206\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1947 - accuracy: 0.9284\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.2012 - accuracy: 0.9246\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1945 - accuracy: 0.9262\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1815 - accuracy: 0.9309\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1802 - accuracy: 0.9314\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1832 - accuracy: 0.9306\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1706 - accuracy: 0.9353\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1677 - accuracy: 0.9364\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1695 - accuracy: 0.9354\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1571 - accuracy: 0.9396\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1607 - accuracy: 0.9395\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1541 - accuracy: 0.9403\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1483 - accuracy: 0.9430\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1446 - accuracy: 0.9440\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1343 - accuracy: 0.9482\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1350 - accuracy: 0.9468\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1319 - accuracy: 0.9502\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1330 - accuracy: 0.9474\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1360 - accuracy: 0.9490\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1256 - accuracy: 0.9516\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1132 - accuracy: 0.9569\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1141 - accuracy: 0.9575\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1241 - accuracy: 0.9536\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1126 - accuracy: 0.9581\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1030 - accuracy: 0.9619\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1054 - accuracy: 0.9602\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1035 - accuracy: 0.9622\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.1140 - accuracy: 0.9591\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0980 - accuracy: 0.9634\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0922 - accuracy: 0.9657\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0943 - accuracy: 0.9641\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0952 - accuracy: 0.9639\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0915 - accuracy: 0.9658\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0942 - accuracy: 0.9651\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0907 - accuracy: 0.9662\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0870 - accuracy: 0.9676\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0884 - accuracy: 0.9674\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0803 - accuracy: 0.9711\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0881 - accuracy: 0.9678\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0807 - accuracy: 0.9700\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0754 - accuracy: 0.9715\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 0.0837 - accuracy: 0.9684\n",
      "76/76 [==============================] - 1s 17ms/step - loss: 0.2770 - accuracy: 0.9360\n",
      "Model: \"sequential_61\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_183 (Conv1D)          (None, 88, 512)           5120      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 44, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_184 (Conv1D)          (None, 43, 512)           524800    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_61 (Averag (None, 14, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_185 (Conv1D)          (None, 12, 512)           786944    \n",
      "_________________________________________________________________\n",
      "lstm_119 (LSTM)              (None, 12, 256)           787456    \n",
      "_________________________________________________________________\n",
      "lstm_120 (LSTM)              (None, 12, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_59 (Flatten)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_177 (Dense)            (None, 512)               1573376   \n",
      "_________________________________________________________________\n",
      "dropout_118 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_178 (Dense)            (None, 384)               196992    \n",
      "_________________________________________________________________\n",
      "dropout_119 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_179 (Dense)            (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 4,402,310\n",
      "Trainable params: 4,402,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 512, k_conv_1 = 3,  n_conv_2 = 512, k_conv_2 = 2, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 3,\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 512, dropout_1 = 0.3,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000002527E6DD448>,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 9s 62ms/step - loss: 1.1004 - accuracy: 0.5169\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.8500 - accuracy: 0.6389\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.6495 - accuracy: 0.7336\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.5359 - accuracy: 0.7802\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.4839 - accuracy: 0.8011\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.4460 - accuracy: 0.8161\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.4228 - accuracy: 0.8275\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.4013 - accuracy: 0.8365\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3751 - accuracy: 0.8466\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3591 - accuracy: 0.8538\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3440 - accuracy: 0.8597\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3206 - accuracy: 0.8683\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3125 - accuracy: 0.8704\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2904 - accuracy: 0.8813\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2766 - accuracy: 0.8909\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2800 - accuracy: 0.8856\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2538 - accuracy: 0.8983\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2442 - accuracy: 0.9026\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2254 - accuracy: 0.9107\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2191 - accuracy: 0.9142\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2140 - accuracy: 0.9169\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1916 - accuracy: 0.9257\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1976 - accuracy: 0.9236\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1912 - accuracy: 0.9248\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1740 - accuracy: 0.9329\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1725 - accuracy: 0.9330\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1684 - accuracy: 0.9342\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1568 - accuracy: 0.9412\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1526 - accuracy: 0.9419\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1477 - accuracy: 0.9428\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1470 - accuracy: 0.9441\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1422 - accuracy: 0.9445\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1469 - accuracy: 0.9446\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1470 - accuracy: 0.9445\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1395 - accuracy: 0.9466\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1344 - accuracy: 0.9478\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1207 - accuracy: 0.9539\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1199 - accuracy: 0.9532\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1350 - accuracy: 0.9494\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1186 - accuracy: 0.9544\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1110 - accuracy: 0.9575\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1024 - accuracy: 0.9600\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1196 - accuracy: 0.9565\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1116 - accuracy: 0.9590\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1141 - accuracy: 0.9580\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1200 - accuracy: 0.9550\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1165 - accuracy: 0.9575\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1101 - accuracy: 0.9592\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1146 - accuracy: 0.9580\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1017 - accuracy: 0.9614\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1040 - accuracy: 0.9606\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1031 - accuracy: 0.9605\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0938 - accuracy: 0.9644\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1037 - accuracy: 0.9602\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0952 - accuracy: 0.9630\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0965 - accuracy: 0.9640\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0951 - accuracy: 0.9642\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0969 - accuracy: 0.9638\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0814 - accuracy: 0.9693\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0918 - accuracy: 0.9654\n",
      "76/76 [==============================] - 2s 23ms/step - loss: 0.3387 - accuracy: 0.9236\n",
      "Model: \"sequential_62\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_186 (Conv1D)          (None, 88, 512)           5120      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 44, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_187 (Conv1D)          (None, 43, 512)           524800    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_62 (Averag (None, 14, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_188 (Conv1D)          (None, 12, 512)           786944    \n",
      "_________________________________________________________________\n",
      "lstm_121 (LSTM)              (None, 12, 256)           787456    \n",
      "_________________________________________________________________\n",
      "lstm_122 (LSTM)              (None, 12, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_60 (Flatten)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_180 (Dense)            (None, 512)               1573376   \n",
      "_________________________________________________________________\n",
      "dropout_120 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_181 (Dense)            (None, 384)               196992    \n",
      "_________________________________________________________________\n",
      "dropout_121 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_182 (Dense)            (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 4,402,310\n",
      "Trainable params: 4,402,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 512, k_conv_1 = 3,  n_conv_2 = 512, k_conv_2 = 2, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 3,\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 512, dropout_1 = 0.3,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000002527E6DD448>,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 9s 60ms/step - loss: 1.0930 - accuracy: 0.5212\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.8586 - accuracy: 0.6325\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.6407 - accuracy: 0.7370\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.5303 - accuracy: 0.7843\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.4785 - accuracy: 0.8000\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.4362 - accuracy: 0.8241\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3989 - accuracy: 0.8377\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3832 - accuracy: 0.8458\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3619 - accuracy: 0.8516\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3436 - accuracy: 0.8627\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3433 - accuracy: 0.8585\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3173 - accuracy: 0.8738\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3047 - accuracy: 0.8763\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2803 - accuracy: 0.8864\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2758 - accuracy: 0.8913\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2608 - accuracy: 0.8962\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2440 - accuracy: 0.9036\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2321 - accuracy: 0.9078\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2197 - accuracy: 0.9132\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2131 - accuracy: 0.9164\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2103 - accuracy: 0.9180\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1948 - accuracy: 0.9231\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1753 - accuracy: 0.9315\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1773 - accuracy: 0.9307\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1684 - accuracy: 0.9361\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1747 - accuracy: 0.9317\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1539 - accuracy: 0.9401\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1482 - accuracy: 0.9422\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1566 - accuracy: 0.9391\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1443 - accuracy: 0.9449\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1383 - accuracy: 0.9467\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1451 - accuracy: 0.9452\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1315 - accuracy: 0.9504\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1261 - accuracy: 0.9525\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1270 - accuracy: 0.9511\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1280 - accuracy: 0.9501\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1197 - accuracy: 0.9544\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1316 - accuracy: 0.9496\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1161 - accuracy: 0.9556\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1170 - accuracy: 0.9563\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1166 - accuracy: 0.9567\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1027 - accuracy: 0.9609\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1073 - accuracy: 0.9592\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1148 - accuracy: 0.9581\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1075 - accuracy: 0.9594\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1035 - accuracy: 0.9595\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1093 - accuracy: 0.9594\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1042 - accuracy: 0.9604\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0973 - accuracy: 0.9625\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1097 - accuracy: 0.9582\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0981 - accuracy: 0.9620\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0946 - accuracy: 0.9635\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0877 - accuracy: 0.9667\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0913 - accuracy: 0.9660\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1021 - accuracy: 0.9615\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0921 - accuracy: 0.9646\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0966 - accuracy: 0.9632\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0873 - accuracy: 0.9671\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0875 - accuracy: 0.9678\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0934 - accuracy: 0.9637\n",
      "76/76 [==============================] - 2s 22ms/step - loss: 0.3805 - accuracy: 0.9258\n",
      "Model: \"sequential_63\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_189 (Conv1D)          (None, 88, 512)           5120      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 44, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_190 (Conv1D)          (None, 43, 512)           524800    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_63 (Averag (None, 14, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_191 (Conv1D)          (None, 12, 512)           786944    \n",
      "_________________________________________________________________\n",
      "lstm_123 (LSTM)              (None, 12, 256)           787456    \n",
      "_________________________________________________________________\n",
      "lstm_124 (LSTM)              (None, 12, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_61 (Flatten)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_183 (Dense)            (None, 512)               1573376   \n",
      "_________________________________________________________________\n",
      "dropout_122 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_184 (Dense)            (None, 384)               196992    \n",
      "_________________________________________________________________\n",
      "dropout_123 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_185 (Dense)            (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 4,402,310\n",
      "Trainable params: 4,402,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 512, k_conv_1 = 3,  n_conv_2 = 512, k_conv_2 = 2, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 3,\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 512, dropout_1 = 0.3,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000002527E6DD448>,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 9s 60ms/step - loss: 1.0880 - accuracy: 0.5305\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.8466 - accuracy: 0.6389\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.6776 - accuracy: 0.7272\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.5416 - accuracy: 0.7802\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.4974 - accuracy: 0.7943\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.4532 - accuracy: 0.8101\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.4217 - accuracy: 0.8272\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.4079 - accuracy: 0.8329\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3703 - accuracy: 0.8512\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3641 - accuracy: 0.8511\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3469 - accuracy: 0.8586\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3247 - accuracy: 0.8678\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.3159 - accuracy: 0.8700\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2987 - accuracy: 0.8793\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2908 - accuracy: 0.8886\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2672 - accuracy: 0.8944\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2437 - accuracy: 0.9028\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2432 - accuracy: 0.9042 0s - loss: 0.2429 - accuracy: \n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2346 - accuracy: 0.9088\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2199 - accuracy: 0.9148\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2126 - accuracy: 0.9168\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.2047 - accuracy: 0.9209\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1967 - accuracy: 0.9218\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1849 - accuracy: 0.9284\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1817 - accuracy: 0.9290\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1831 - accuracy: 0.9315\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1601 - accuracy: 0.9385 0s - loss: 0.1597 - \n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1608 - accuracy: 0.9385\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1521 - accuracy: 0.9409\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1535 - accuracy: 0.9400\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1421 - accuracy: 0.9449\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1459 - accuracy: 0.9438\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1466 - accuracy: 0.9434\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1368 - accuracy: 0.9485\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1309 - accuracy: 0.9508\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1325 - accuracy: 0.9483\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1344 - accuracy: 0.9491\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1246 - accuracy: 0.9533\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1067 - accuracy: 0.9589\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1308 - accuracy: 0.9518\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1139 - accuracy: 0.9565\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1096 - accuracy: 0.9586\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1188 - accuracy: 0.9559\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1184 - accuracy: 0.9558\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1166 - accuracy: 0.9559\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0970 - accuracy: 0.9626\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1018 - accuracy: 0.9616\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1122 - accuracy: 0.9577\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1058 - accuracy: 0.9605\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0993 - accuracy: 0.9623 0s - loss: 0.0997 - accuracy\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1048 - accuracy: 0.9615\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0954 - accuracy: 0.9646\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0973 - accuracy: 0.9632\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0959 - accuracy: 0.9646\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.1013 - accuracy: 0.9621\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0958 - accuracy: 0.9642\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0920 - accuracy: 0.9649\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0956 - accuracy: 0.9646\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0970 - accuracy: 0.9637\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 9s 60ms/step - loss: 0.0939 - accuracy: 0.9641\n",
      "76/76 [==============================] - 2s 22ms/step - loss: 0.3109 - accuracy: 0.9312\n",
      "Model: \"sequential_64\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_192 (Conv1D)          (None, 88, 768)           7680      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 44, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_193 (Conv1D)          (None, 43, 512)           786944    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_64 (Averag (None, 21, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_194 (Conv1D)          (None, 20, 512)           524800    \n",
      "_________________________________________________________________\n",
      "lstm_125 (LSTM)              (None, 20, 128)           328192    \n",
      "_________________________________________________________________\n",
      "lstm_126 (LSTM)              (None, 20, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_62 (Flatten)         (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense_186 (Dense)            (None, 384)               983424    \n",
      "_________________________________________________________________\n",
      "dropout_124 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_187 (Dense)            (None, 384)               147840    \n",
      "_________________________________________________________________\n",
      "dropout_125 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_188 (Dense)            (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 2,912,774\n",
      "Trainable params: 2,912,774\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "n_conv_1 = 768, k_conv_1 = 3,  n_conv_2 = 512, k_conv_2 = 2, \n",
      "          n_conv_3 = 512,  k_conv_3 = 2,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,\n",
      "          n_lstm_1 = 128, n_lstm_2 = 128,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          n_dense_1 = 384, dropout_1 = 0.2,\n",
      "          n_dense_2 = 384, dropout_2 = 0.2, activation_conv=  relu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x00000252062F3D48>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2/228 [..............................] - ETA: 12s - loss: 1.7772 - accuracy: 0.1777WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0295s vs `on_train_batch_end` time: 0.0497s). Check your callbacks.\n",
      "228/228 [==============================] - 16s 71ms/step - loss: 0.9191 - accuracy: 0.6115\n",
      "Epoch 2/60\n",
      "228/228 [==============================] - 16s 70ms/step - loss: 0.4939 - accuracy: 0.8100\n",
      "Epoch 3/60\n",
      "228/228 [==============================] - 16s 70ms/step - loss: 0.3881 - accuracy: 0.8522\n",
      "Epoch 4/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.3426 - accuracy: 0.8692\n",
      "Epoch 5/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.3052 - accuracy: 0.8817\n",
      "Epoch 6/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.2887 - accuracy: 0.8888\n",
      "Epoch 7/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.2789 - accuracy: 0.8928\n",
      "Epoch 8/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.2631 - accuracy: 0.8998\n",
      "Epoch 9/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.2479 - accuracy: 0.9066\n",
      "Epoch 10/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.2320 - accuracy: 0.9124\n",
      "Epoch 11/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.2278 - accuracy: 0.9133\n",
      "Epoch 12/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.2202 - accuracy: 0.9161\n",
      "Epoch 13/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.2068 - accuracy: 0.9216\n",
      "Epoch 14/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.2021 - accuracy: 0.9238\n",
      "Epoch 15/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.1895 - accuracy: 0.9276\n",
      "Epoch 16/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.1853 - accuracy: 0.9279\n",
      "Epoch 17/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.1799 - accuracy: 0.9313\n",
      "Epoch 18/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.1663 - accuracy: 0.9361\n",
      "Epoch 19/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.1639 - accuracy: 0.9380\n",
      "Epoch 20/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.1640 - accuracy: 0.9378\n",
      "Epoch 21/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.1502 - accuracy: 0.9429\n",
      "Epoch 22/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.1439 - accuracy: 0.9456\n",
      "Epoch 23/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.1416 - accuracy: 0.9459\n",
      "Epoch 24/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.1350 - accuracy: 0.9485\n",
      "Epoch 25/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.1294 - accuracy: 0.9510\n",
      "Epoch 26/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.1306 - accuracy: 0.9517\n",
      "Epoch 27/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.1179 - accuracy: 0.9563\n",
      "Epoch 28/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.1217 - accuracy: 0.9537\n",
      "Epoch 29/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.1140 - accuracy: 0.9576\n",
      "Epoch 30/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.1063 - accuracy: 0.9602\n",
      "Epoch 31/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.1024 - accuracy: 0.9613\n",
      "Epoch 32/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.1019 - accuracy: 0.9607\n",
      "Epoch 33/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.0967 - accuracy: 0.9643\n",
      "Epoch 34/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.0968 - accuracy: 0.9631\n",
      "Epoch 35/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.0915 - accuracy: 0.9654\n",
      "Epoch 36/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.0916 - accuracy: 0.9653\n",
      "Epoch 37/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.0878 - accuracy: 0.9675\n",
      "Epoch 38/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.0857 - accuracy: 0.9683\n",
      "Epoch 39/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.0848 - accuracy: 0.9684\n",
      "Epoch 40/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.0864 - accuracy: 0.9686\n",
      "Epoch 41/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.0786 - accuracy: 0.9701\n",
      "Epoch 42/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.0778 - accuracy: 0.9714\n",
      "Epoch 43/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.0751 - accuracy: 0.9722\n",
      "Epoch 44/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.0739 - accuracy: 0.9723\n",
      "Epoch 45/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.0719 - accuracy: 0.9728\n",
      "Epoch 46/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.0701 - accuracy: 0.9737\n",
      "Epoch 47/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.0672 - accuracy: 0.9753\n",
      "Epoch 48/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.0687 - accuracy: 0.9742\n",
      "Epoch 49/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.0662 - accuracy: 0.9747\n",
      "Epoch 50/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.0704 - accuracy: 0.9736\n",
      "Epoch 51/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.0653 - accuracy: 0.9759\n",
      "Epoch 52/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.0608 - accuracy: 0.9769\n",
      "Epoch 53/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.0660 - accuracy: 0.9756\n",
      "Epoch 54/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.0622 - accuracy: 0.9767\n",
      "Epoch 55/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.0608 - accuracy: 0.9770\n",
      "Epoch 56/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.0579 - accuracy: 0.9787\n",
      "Epoch 57/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.0589 - accuracy: 0.9788\n",
      "Epoch 58/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.0565 - accuracy: 0.9788\n",
      "Epoch 59/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.0548 - accuracy: 0.9789\n",
      "Epoch 60/60\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 0.0502 - accuracy: 0.9811\n",
      "Wall time: 8h 48min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rscv = RandomizedSearchCV(model, param_distributions=params, cv=cv, n_iter=n_tune_iter)\n",
    "rscv_results = rscv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score is: 0.9448848764101664 using {'n_lstm_2': 128, 'n_lstm_1': 128, 'n_dense_2': 384, 'n_dense_1': 384, 'n_conv_3': 512, 'n_conv_2': 512, 'n_conv_1': 768, 'maxpooling_pool_size': 2, 'k_conv_3': 2, 'k_conv_2': 2, 'k_conv_1': 3, 'dropout_2': 0.2, 'dropout_1': 0.2, 'drop_lstm_2': 0.02, 'drop_lstm_1': 0.1, 'avepooling_pool_size': 2, 'activation_dense': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x00000252062F3D48>, 'activation_conv': 'relu'}\n"
     ]
    }
   ],
   "source": [
    "print('Best score is: {} using {}'.format(rscv_results.best_score_,\n",
    "rscv_results.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = rscv_results.best_estimator_\n",
    "best_param = rscv_results.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JkbhJMox8Jl9"
   },
   "source": [
    "### Test the best model based on the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZwGk5dR8JmS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py:241: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    }
   ],
   "source": [
    "y_hat = best_model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E_GNq-R_8JmZ",
    "outputId": "0fc16efe-7739-4394-f1a3-46da92e79584"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = to_categorical(y_hat)\n",
    "y_hat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFqRQ5XB8Jmc",
    "outputId": "a957fbed-92f8-4bdd-d24b-5df3361a8bac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FTWf6Cf-8Jme",
    "outputId": "1256b170-33dc-4171-f2ff-7a3d7ddd74e3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_hat)\n",
    "_ = plt.axvline(x=0.5, color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "95.96973440395305"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import  accuracy_score as score\n",
    "y_pred_classes = y_hat.round()\n",
    "print(y_valid[0], y_hat[0], y_pred_classes[0])\n",
    "acc = score(y_valid, y_pred_classes) * 100\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RNljSx3v8Jmh"
   },
   "outputs": [],
   "source": [
    "pct_auc = roc_auc_score(y_valid, y_hat)*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SO_N9lJG8Jmj",
    "outputId": "20968a52-8ca0-44d3-abf6-a1382bba5cde"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'97.53'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{:0.2f}\".format(pct_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vU3hnA298Jmo"
   },
   "outputs": [],
   "source": [
    "float_y_hat = []\n",
    "for y in y_hat:\n",
    "    float_y_hat.append(y[0:6].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zrTf-aQl8Jmr"
   },
   "outputs": [],
   "source": [
    "ydf = pd.DataFrame(list(zip(float_y_hat, y_valid)), columns=['y_hat', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zAyKh_mX8Jmu",
    "outputId": "611224b8-f980-47df-d596-9aa3a5811009"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_hat</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            y_hat                               y\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
       "1  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n",
       "2  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
       "3  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "4  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
       "5  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
       "6  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
       "7  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
       "8  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "9  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ydf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_lstm_2</th>\n",
       "      <th>n_lstm_1</th>\n",
       "      <th>n_dense_2</th>\n",
       "      <th>n_dense_1</th>\n",
       "      <th>n_conv_3</th>\n",
       "      <th>n_conv_2</th>\n",
       "      <th>n_conv_1</th>\n",
       "      <th>maxpooling_pool_size</th>\n",
       "      <th>k_conv_3</th>\n",
       "      <th>k_conv_2</th>\n",
       "      <th>k_conv_1</th>\n",
       "      <th>dropout_2</th>\n",
       "      <th>dropout_1</th>\n",
       "      <th>drop_lstm_2</th>\n",
       "      <th>drop_lstm_1</th>\n",
       "      <th>avepooling_pool_size</th>\n",
       "      <th>activation_dense</th>\n",
       "      <th>activation_conv</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>384</td>\n",
       "      <td>384</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.944885</td>\n",
       "      <td>0.001534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>512</td>\n",
       "      <td>384</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.944284</td>\n",
       "      <td>0.000925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.939771</td>\n",
       "      <td>0.002293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>512</td>\n",
       "      <td>384</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>768</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>elu</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.939188</td>\n",
       "      <td>0.002291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.938416</td>\n",
       "      <td>0.001870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>512</td>\n",
       "      <td>384</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.938330</td>\n",
       "      <td>0.003349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>384</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>768</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>elu</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.938107</td>\n",
       "      <td>0.001120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>384</td>\n",
       "      <td>384</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>768</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.936700</td>\n",
       "      <td>0.002642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>0.929819</td>\n",
       "      <td>0.005816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>384</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>768</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>0.929493</td>\n",
       "      <td>0.007134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>384</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3</td>\n",
       "      <td>relu</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>0.926868</td>\n",
       "      <td>0.003218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>768</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>0.926816</td>\n",
       "      <td>0.005533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2</td>\n",
       "      <td>elu</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.926610</td>\n",
       "      <td>0.006252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>384</td>\n",
       "      <td>384</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>768</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>elu</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>0.926576</td>\n",
       "      <td>0.001620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>384</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>elu</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.921823</td>\n",
       "      <td>0.001838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3</td>\n",
       "      <td>elu</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.920639</td>\n",
       "      <td>0.003394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.917773</td>\n",
       "      <td>0.001202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.914633</td>\n",
       "      <td>0.002404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>384</td>\n",
       "      <td>256</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>768</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.914599</td>\n",
       "      <td>0.001607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.891005</td>\n",
       "      <td>0.007689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_lstm_2  n_lstm_1  n_dense_2  n_dense_1  n_conv_3  n_conv_2  n_conv_1  \\\n",
       "16       128       128        384        384       512       512       768   \n",
       "12       256       256        512        384       512       512       768   \n",
       "11       128       128        512        256       512       256       512   \n",
       "18       256       128        512        384       512       256       768   \n",
       "4        128       128        256        512       512       256       512   \n",
       "0        256       256        512        384       512       256       512   \n",
       "14       256       128        256        384       256       256       768   \n",
       "15       128       256        384        384       256       256       768   \n",
       "3        256       256        256        512       512       512       512   \n",
       "17       256       128        384        256       256       256       768   \n",
       "19       256       256        384        512       512       512       512   \n",
       "7        256       128        256        256       256       256       768   \n",
       "10       256       128        512        256       256       512       512   \n",
       "8        128       256        384        384       512       256       768   \n",
       "1        256       256        256        384       512       512       768   \n",
       "13       128       128        512        256       512       256       512   \n",
       "9        256       256        256        256       512       512       768   \n",
       "2        128       256        256        512       512       512       768   \n",
       "6        256       128        384        256       512       256       768   \n",
       "5        256       128        256        256       512       512       768   \n",
       "\n",
       "    maxpooling_pool_size  k_conv_3  k_conv_2  k_conv_1  dropout_2  dropout_1  \\\n",
       "16                     2         2         2         3        0.2        0.2   \n",
       "12                     2         2         2         3        0.3        0.2   \n",
       "11                     2         2         2         2        0.2        0.2   \n",
       "18                     3         2         3         3        0.3        0.2   \n",
       "4                      3         3         2         3        0.3        0.2   \n",
       "0                      3         3         2         3        0.3        0.3   \n",
       "14                     3         2         3         3        0.2        0.2   \n",
       "15                     2         3         3         2        0.3        0.2   \n",
       "3                      3         2         3         2        0.2        0.2   \n",
       "17                     2         3         3         3        0.3        0.2   \n",
       "19                     2         3         2         3        0.3        0.3   \n",
       "7                      3         3         3         2        0.3        0.3   \n",
       "10                     3         2         2         3        0.3        0.3   \n",
       "8                      3         2         2         2        0.3        0.2   \n",
       "1                      2         3         3         2        0.2        0.3   \n",
       "13                     2         2         3         2        0.3        0.2   \n",
       "9                      2         3         3         2        0.3        0.2   \n",
       "2                      3         3         2         3        0.2        0.3   \n",
       "6                      3         2         3         2        0.3        0.3   \n",
       "5                      3         3         3         2        0.3        0.3   \n",
       "\n",
       "    drop_lstm_2  drop_lstm_1  avepooling_pool_size  \\\n",
       "16         0.02         0.10                     2   \n",
       "12         0.10         0.10                     3   \n",
       "11         0.02         0.10                     2   \n",
       "18         0.02         0.10                     3   \n",
       "4          0.02         0.10                     2   \n",
       "0          0.02         0.10                     3   \n",
       "14         0.10         0.10                     2   \n",
       "15         0.10         0.02                     2   \n",
       "3          0.10         0.02                     2   \n",
       "17         0.02         0.02                     2   \n",
       "19         0.02         0.02                     3   \n",
       "7          0.02         0.02                     2   \n",
       "10         0.02         0.02                     2   \n",
       "8          0.10         0.10                     3   \n",
       "1          0.10         0.10                     2   \n",
       "13         0.02         0.02                     3   \n",
       "9          0.10         0.10                     2   \n",
       "2          0.02         0.10                     3   \n",
       "6          0.02         0.02                     2   \n",
       "5          0.10         0.02                     2   \n",
       "\n",
       "                                     activation_dense  \\\n",
       "16  <tensorflow.python.keras.layers.advanced_activ...   \n",
       "12                                               relu   \n",
       "11  <tensorflow.python.keras.layers.advanced_activ...   \n",
       "18                                                elu   \n",
       "4   <tensorflow.python.keras.layers.advanced_activ...   \n",
       "0   <tensorflow.python.keras.layers.advanced_activ...   \n",
       "14                                                elu   \n",
       "15  <tensorflow.python.keras.layers.advanced_activ...   \n",
       "3   <tensorflow.python.keras.layers.advanced_activ...   \n",
       "17                                               relu   \n",
       "19                                               relu   \n",
       "7   <tensorflow.python.keras.layers.advanced_activ...   \n",
       "10                                                elu   \n",
       "8                                                 elu   \n",
       "1                                                 elu   \n",
       "13                                                elu   \n",
       "9   <tensorflow.python.keras.layers.advanced_activ...   \n",
       "2   <tensorflow.python.keras.layers.advanced_activ...   \n",
       "6                                                relu   \n",
       "5                                                relu   \n",
       "\n",
       "                                      activation_conv      mean       std  \n",
       "16                                               relu  0.944885  0.001534  \n",
       "12                                               relu  0.944284  0.000925  \n",
       "11                                               relu  0.939771  0.002293  \n",
       "18                                               relu  0.939188  0.002291  \n",
       "4                                                relu  0.938416  0.001870  \n",
       "0                                                relu  0.938330  0.003349  \n",
       "14                                               relu  0.938107  0.001120  \n",
       "15                                               relu  0.936700  0.002642  \n",
       "3   <tensorflow.python.keras.layers.advanced_activ...  0.929819  0.005816  \n",
       "17  <tensorflow.python.keras.layers.advanced_activ...  0.929493  0.007134  \n",
       "19  <tensorflow.python.keras.layers.advanced_activ...  0.926868  0.003218  \n",
       "7   <tensorflow.python.keras.layers.advanced_activ...  0.926816  0.005533  \n",
       "10                                                elu  0.926610  0.006252  \n",
       "8   <tensorflow.python.keras.layers.advanced_activ...  0.926576  0.001620  \n",
       "1                                                 elu  0.921823  0.001838  \n",
       "13                                                elu  0.920639  0.003394  \n",
       "9                                                 elu  0.917773  0.001202  \n",
       "2                                                 elu  0.914633  0.002404  \n",
       "6                                                 elu  0.914599  0.001607  \n",
       "5                                                 elu  0.891005  0.007689  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(rscv_results.cv_results_['params'])\n",
    "results_df['mean'] = rscv_results.cv_results_['mean_test_score']\n",
    "results_df['std'] = rscv_results.cv_results_['std_test_score']\n",
    "results_df.sort_values('mean', ascending=False, ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating best model from the best param and train it for 60 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_lstm_2': 128,\n",
       " 'n_lstm_1': 128,\n",
       " 'n_dense_2': 384,\n",
       " 'n_dense_1': 384,\n",
       " 'n_conv_3': 512,\n",
       " 'n_conv_2': 512,\n",
       " 'n_conv_1': 768,\n",
       " 'maxpooling_pool_size': 2,\n",
       " 'k_conv_3': 2,\n",
       " 'k_conv_2': 2,\n",
       " 'k_conv_1': 3,\n",
       " 'dropout_2': 0.2,\n",
       " 'dropout_1': 0.2,\n",
       " 'drop_lstm_2': 0.02,\n",
       " 'drop_lstm_1': 0.1,\n",
       " 'avepooling_pool_size': 2,\n",
       " 'activation_dense': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU at 0x252062f3d48>,\n",
       " 'activation_conv': 'relu'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rscv_results.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lstm_2 = 128\n",
    "n_lstm_1 = 128\n",
    "n_dense_2= 384\n",
    "n_dense_1= 384\n",
    "n_conv_3= 512\n",
    "n_conv_2= 512\n",
    "n_conv_1= 768\n",
    "maxpooling_pool_size= 2\n",
    "k_conv_3= 2\n",
    "k_conv_2= 2\n",
    "k_conv_1= 2\n",
    "dropout_2= 0.2\n",
    "dropout_1= 0.2\n",
    "drop_lstm_2 = 0.02\n",
    "drop_lstm_1 = 0.1\n",
    "avepooling_pool_size= 2\n",
    "activation_dense = LeakyReLU()\n",
    "activation_conv = 'relu'\n",
    "model = Sequential()\n",
    "model.add(Conv1D(n_conv_1, k_conv_1, activation=activation_conv, input_shape=input_shape[1:]))\n",
    "model.add(MaxPool1D(pool_size = maxpooling_pool_size))\n",
    "model.add(Conv1D(n_conv_2, k_conv_2, activation=activation_conv))\n",
    "model.add(AveragePooling1D(pool_size = avepooling_pool_size))\n",
    "model.add(Conv1D(n_conv_3, k_conv_3, activation=activation_conv))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# LSTM layers\n",
    "model.add(LSTM(n_lstm_1, dropout=drop_lstm_1, return_sequences=True)) \n",
    "model.add(LSTM(n_lstm_2, dropout=drop_lstm_2, return_sequences=True))\n",
    "\n",
    "# Dense layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(n_dense_1, activation=activation_dense))\n",
    "model.add(Dropout(dropout_1))\n",
    "model.add(Dense(n_dense_2, activation=activation_dense))\n",
    "model.add(Dropout(dropout_2))\n",
    "model.add(Dense(n_class, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir= 'model_ouput/Final-CNN-LSTM'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "modelcheckpoint = ModelCheckpoint(filepath=output_dir+\n",
    "                                  \"/weights.{epoch:02d}.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "228/228 [==============================] - 17s 76ms/step - loss: 0.9483 - accuracy: 0.5963 - val_loss: 0.6727 - val_accuracy: 0.7288\n",
      "Epoch 2/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.5682 - accuracy: 0.7747 - val_loss: 0.4335 - val_accuracy: 0.8223\n",
      "Epoch 3/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.4172 - accuracy: 0.8399 - val_loss: 0.3706 - val_accuracy: 0.8562\n",
      "Epoch 4/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.3481 - accuracy: 0.8662 - val_loss: 0.3244 - val_accuracy: 0.8780\n",
      "Epoch 5/120\n",
      "228/228 [==============================] - 16s 71ms/step - loss: 0.3166 - accuracy: 0.8762 - val_loss: 0.3082 - val_accuracy: 0.8779\n",
      "Epoch 6/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.2983 - accuracy: 0.8836 - val_loss: 0.2678 - val_accuracy: 0.9015\n",
      "Epoch 7/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.2806 - accuracy: 0.8920 - val_loss: 0.2774 - val_accuracy: 0.8843\n",
      "Epoch 8/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.2595 - accuracy: 0.9011 - val_loss: 0.2631 - val_accuracy: 0.8968\n",
      "Epoch 9/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.2495 - accuracy: 0.9045 - val_loss: 0.2591 - val_accuracy: 0.8989\n",
      "Epoch 10/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.2394 - accuracy: 0.9090 - val_loss: 0.2438 - val_accuracy: 0.8999\n",
      "Epoch 11/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.2255 - accuracy: 0.9136 - val_loss: 0.2386 - val_accuracy: 0.9035\n",
      "Epoch 12/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.2172 - accuracy: 0.9177 - val_loss: 0.2402 - val_accuracy: 0.9155\n",
      "Epoch 13/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.2083 - accuracy: 0.9213 - val_loss: 0.2091 - val_accuracy: 0.9209\n",
      "Epoch 14/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.2079 - accuracy: 0.9215 - val_loss: 0.2394 - val_accuracy: 0.9143\n",
      "Epoch 15/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.1950 - accuracy: 0.9250 - val_loss: 0.2063 - val_accuracy: 0.9209\n",
      "Epoch 16/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.1813 - accuracy: 0.9301 - val_loss: 0.2063 - val_accuracy: 0.9237\n",
      "Epoch 17/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.1816 - accuracy: 0.9317 - val_loss: 0.2138 - val_accuracy: 0.9219\n",
      "Epoch 18/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.1715 - accuracy: 0.9330 - val_loss: 0.2032 - val_accuracy: 0.9250\n",
      "Epoch 19/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.1702 - accuracy: 0.9342 - val_loss: 0.2266 - val_accuracy: 0.9229\n",
      "Epoch 20/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.1648 - accuracy: 0.9358 - val_loss: 0.2009 - val_accuracy: 0.9296\n",
      "Epoch 21/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.1564 - accuracy: 0.9389 - val_loss: 0.1808 - val_accuracy: 0.9322\n",
      "Epoch 22/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.1502 - accuracy: 0.9417 - val_loss: 0.1840 - val_accuracy: 0.9330\n",
      "Epoch 23/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.1441 - accuracy: 0.9443 - val_loss: 0.1960 - val_accuracy: 0.9338\n",
      "Epoch 24/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.1387 - accuracy: 0.9463 - val_loss: 0.1998 - val_accuracy: 0.9287\n",
      "Epoch 25/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.1410 - accuracy: 0.9457 - val_loss: 0.1743 - val_accuracy: 0.9381\n",
      "Epoch 26/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.1295 - accuracy: 0.9512 - val_loss: 0.1902 - val_accuracy: 0.9348\n",
      "Epoch 27/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.1310 - accuracy: 0.9501 - val_loss: 0.1927 - val_accuracy: 0.9367\n",
      "Epoch 28/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.1210 - accuracy: 0.9539 - val_loss: 0.1776 - val_accuracy: 0.9484\n",
      "Epoch 29/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.1200 - accuracy: 0.9552 - val_loss: 0.1889 - val_accuracy: 0.9407\n",
      "Epoch 30/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.1190 - accuracy: 0.9547 - val_loss: 0.2013 - val_accuracy: 0.9402\n",
      "Epoch 31/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.1091 - accuracy: 0.9589 - val_loss: 0.1715 - val_accuracy: 0.9478\n",
      "Epoch 32/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.1042 - accuracy: 0.9608 - val_loss: 0.1715 - val_accuracy: 0.9455\n",
      "Epoch 33/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.1044 - accuracy: 0.9612 - val_loss: 0.1766 - val_accuracy: 0.9483\n",
      "Epoch 34/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0925 - accuracy: 0.9661 - val_loss: 0.1864 - val_accuracy: 0.9492\n",
      "Epoch 35/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0957 - accuracy: 0.9640 - val_loss: 0.1754 - val_accuracy: 0.9504\n",
      "Epoch 36/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0902 - accuracy: 0.9658 - val_loss: 0.1852 - val_accuracy: 0.9520\n",
      "Epoch 37/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0943 - accuracy: 0.9650 - val_loss: 0.1861 - val_accuracy: 0.9464\n",
      "Epoch 38/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0917 - accuracy: 0.9665 - val_loss: 0.1943 - val_accuracy: 0.9416\n",
      "Epoch 39/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0812 - accuracy: 0.9703 - val_loss: 0.2083 - val_accuracy: 0.9507\n",
      "Epoch 40/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0854 - accuracy: 0.9694 - val_loss: 0.2089 - val_accuracy: 0.9507\n",
      "Epoch 41/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0821 - accuracy: 0.9694 - val_loss: 0.1982 - val_accuracy: 0.9518\n",
      "Epoch 42/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0831 - accuracy: 0.9685 - val_loss: 0.1893 - val_accuracy: 0.9506\n",
      "Epoch 43/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0802 - accuracy: 0.9710 - val_loss: 0.1925 - val_accuracy: 0.9526\n",
      "Epoch 44/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0761 - accuracy: 0.9721 - val_loss: 0.1979 - val_accuracy: 0.9532\n",
      "Epoch 45/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0740 - accuracy: 0.9731 - val_loss: 0.2072 - val_accuracy: 0.9412\n",
      "Epoch 46/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0748 - accuracy: 0.9726 - val_loss: 0.2193 - val_accuracy: 0.9527\n",
      "Epoch 47/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0661 - accuracy: 0.9750 - val_loss: 0.2375 - val_accuracy: 0.9470\n",
      "Epoch 48/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0676 - accuracy: 0.9753 - val_loss: 0.2191 - val_accuracy: 0.9540\n",
      "Epoch 49/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0706 - accuracy: 0.9741 - val_loss: 0.1953 - val_accuracy: 0.9531\n",
      "Epoch 50/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0662 - accuracy: 0.9755 - val_loss: 0.2222 - val_accuracy: 0.9509\n",
      "Epoch 51/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0706 - accuracy: 0.9738 - val_loss: 0.1913 - val_accuracy: 0.9535\n",
      "Epoch 52/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0726 - accuracy: 0.9731 - val_loss: 0.2042 - val_accuracy: 0.9552\n",
      "Epoch 53/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0605 - accuracy: 0.9775 - val_loss: 0.2448 - val_accuracy: 0.9460\n",
      "Epoch 54/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0588 - accuracy: 0.9785 - val_loss: 0.2127 - val_accuracy: 0.9557\n",
      "Epoch 55/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0625 - accuracy: 0.9772 - val_loss: 0.2181 - val_accuracy: 0.9552\n",
      "Epoch 56/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0608 - accuracy: 0.9773 - val_loss: 0.2111 - val_accuracy: 0.9523\n",
      "Epoch 57/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0578 - accuracy: 0.9786 - val_loss: 0.2178 - val_accuracy: 0.9565\n",
      "Epoch 58/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0519 - accuracy: 0.9804 - val_loss: 0.2299 - val_accuracy: 0.9557\n",
      "Epoch 59/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0546 - accuracy: 0.9808 - val_loss: 0.2110 - val_accuracy: 0.9565\n",
      "Epoch 60/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0543 - accuracy: 0.9801 - val_loss: 0.2321 - val_accuracy: 0.9546\n",
      "Epoch 61/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0536 - accuracy: 0.9807 - val_loss: 0.2144 - val_accuracy: 0.9577\n",
      "Epoch 62/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0532 - accuracy: 0.9810 - val_loss: 0.2299 - val_accuracy: 0.9506\n",
      "Epoch 63/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0539 - accuracy: 0.9802 - val_loss: 0.2168 - val_accuracy: 0.9543\n",
      "Epoch 64/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.0556 - accuracy: 0.9805 - val_loss: 0.2102 - val_accuracy: 0.9580\n",
      "Epoch 65/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0499 - accuracy: 0.9820 - val_loss: 0.2262 - val_accuracy: 0.9599\n",
      "Epoch 66/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0470 - accuracy: 0.9819 - val_loss: 0.2279 - val_accuracy: 0.9554\n",
      "Epoch 67/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0476 - accuracy: 0.9819 - val_loss: 0.2566 - val_accuracy: 0.9527\n",
      "Epoch 68/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0567 - accuracy: 0.9793 - val_loss: 0.2197 - val_accuracy: 0.9589\n",
      "Epoch 69/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0497 - accuracy: 0.9819 - val_loss: 0.2410 - val_accuracy: 0.9558\n",
      "Epoch 70/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0428 - accuracy: 0.9840 - val_loss: 0.2516 - val_accuracy: 0.9569\n",
      "Epoch 71/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0499 - accuracy: 0.9829 - val_loss: 0.2203 - val_accuracy: 0.9560\n",
      "Epoch 72/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0419 - accuracy: 0.9843 - val_loss: 0.2488 - val_accuracy: 0.9619\n",
      "Epoch 73/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0484 - accuracy: 0.9830 - val_loss: 0.2079 - val_accuracy: 0.9578\n",
      "Epoch 74/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0433 - accuracy: 0.9844 - val_loss: 0.2202 - val_accuracy: 0.9608\n",
      "Epoch 75/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0401 - accuracy: 0.9855 - val_loss: 0.2522 - val_accuracy: 0.9551\n",
      "Epoch 76/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0476 - accuracy: 0.9832 - val_loss: 0.2305 - val_accuracy: 0.9582\n",
      "Epoch 77/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0429 - accuracy: 0.9841 - val_loss: 0.2350 - val_accuracy: 0.9565\n",
      "Epoch 78/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0417 - accuracy: 0.9850 - val_loss: 0.2299 - val_accuracy: 0.9609\n",
      "Epoch 79/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0422 - accuracy: 0.9845 - val_loss: 0.2334 - val_accuracy: 0.9577\n",
      "Epoch 80/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0424 - accuracy: 0.9846 - val_loss: 0.2443 - val_accuracy: 0.9548\n",
      "Epoch 81/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0456 - accuracy: 0.9840 - val_loss: 0.2202 - val_accuracy: 0.9586\n",
      "Epoch 82/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0383 - accuracy: 0.9863 - val_loss: 0.2507 - val_accuracy: 0.9565\n",
      "Epoch 83/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0410 - accuracy: 0.9854 - val_loss: 0.2387 - val_accuracy: 0.9555\n",
      "Epoch 84/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0457 - accuracy: 0.9844 - val_loss: 0.2194 - val_accuracy: 0.9608\n",
      "Epoch 85/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0360 - accuracy: 0.9872 - val_loss: 0.2385 - val_accuracy: 0.9591\n",
      "Epoch 86/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0359 - accuracy: 0.9869 - val_loss: 0.2499 - val_accuracy: 0.9540\n",
      "Epoch 87/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0393 - accuracy: 0.9858 - val_loss: 0.2385 - val_accuracy: 0.9551\n",
      "Epoch 88/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0363 - accuracy: 0.9873 - val_loss: 0.2362 - val_accuracy: 0.9586\n",
      "Epoch 89/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0436 - accuracy: 0.9846 - val_loss: 0.2398 - val_accuracy: 0.9591\n",
      "Epoch 90/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0330 - accuracy: 0.9883 - val_loss: 0.2629 - val_accuracy: 0.9582\n",
      "Epoch 91/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0354 - accuracy: 0.9868 - val_loss: 0.2589 - val_accuracy: 0.9586\n",
      "Epoch 92/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0362 - accuracy: 0.9873 - val_loss: 0.2473 - val_accuracy: 0.9588\n",
      "Epoch 93/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0390 - accuracy: 0.9865 - val_loss: 0.2449 - val_accuracy: 0.9605\n",
      "Epoch 94/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0364 - accuracy: 0.9869 - val_loss: 0.2631 - val_accuracy: 0.9606\n",
      "Epoch 95/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0332 - accuracy: 0.9879 - val_loss: 0.2685 - val_accuracy: 0.9603\n",
      "Epoch 96/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0385 - accuracy: 0.9872 - val_loss: 0.2206 - val_accuracy: 0.9606\n",
      "Epoch 97/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.0332 - accuracy: 0.9880 - val_loss: 0.2560 - val_accuracy: 0.9580\n",
      "Epoch 98/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0389 - accuracy: 0.9866 - val_loss: 0.2303 - val_accuracy: 0.9578\n",
      "Epoch 99/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0340 - accuracy: 0.9873 - val_loss: 0.2926 - val_accuracy: 0.9527\n",
      "Epoch 100/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.0331 - accuracy: 0.9880 - val_loss: 0.2311 - val_accuracy: 0.9632\n",
      "Epoch 101/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0297 - accuracy: 0.9895 - val_loss: 0.2462 - val_accuracy: 0.9629\n",
      "Epoch 102/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.0342 - accuracy: 0.9881 - val_loss: 0.2753 - val_accuracy: 0.9591\n",
      "Epoch 103/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0323 - accuracy: 0.9886 - val_loss: 0.2463 - val_accuracy: 0.9619\n",
      "Epoch 104/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0329 - accuracy: 0.9883 - val_loss: 0.2729 - val_accuracy: 0.9612\n",
      "Epoch 105/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0302 - accuracy: 0.9890 - val_loss: 0.2690 - val_accuracy: 0.9612\n",
      "Epoch 106/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0332 - accuracy: 0.9883 - val_loss: 0.2667 - val_accuracy: 0.9575\n",
      "Epoch 107/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.0307 - accuracy: 0.9892 - val_loss: 0.2456 - val_accuracy: 0.9614\n",
      "Epoch 108/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.0271 - accuracy: 0.9898 - val_loss: 0.2922 - val_accuracy: 0.9606\n",
      "Epoch 109/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0356 - accuracy: 0.9877 - val_loss: 0.2592 - val_accuracy: 0.9631\n",
      "Epoch 110/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.0267 - accuracy: 0.9909 - val_loss: 0.2925 - val_accuracy: 0.9606\n",
      "Epoch 111/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.0293 - accuracy: 0.9894 - val_loss: 0.2832 - val_accuracy: 0.9546\n",
      "Epoch 112/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.0333 - accuracy: 0.9890 - val_loss: 0.2531 - val_accuracy: 0.9589\n",
      "Epoch 113/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0315 - accuracy: 0.9888 - val_loss: 0.2509 - val_accuracy: 0.9617\n",
      "Epoch 114/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.0257 - accuracy: 0.9910 - val_loss: 0.2644 - val_accuracy: 0.9616\n",
      "Epoch 115/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0275 - accuracy: 0.9905 - val_loss: 0.2529 - val_accuracy: 0.9589\n",
      "Epoch 116/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.0288 - accuracy: 0.9901 - val_loss: 0.2535 - val_accuracy: 0.9575\n",
      "Epoch 117/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.0299 - accuracy: 0.9899 - val_loss: 0.2432 - val_accuracy: 0.9622\n",
      "Epoch 118/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.0291 - accuracy: 0.9897 - val_loss: 0.2481 - val_accuracy: 0.9597\n",
      "Epoch 119/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.0300 - accuracy: 0.9899 - val_loss: 0.2594 - val_accuracy: 0.9595\n",
      "Epoch 120/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0330 - accuracy: 0.9886 - val_loss: 0.2466 - val_accuracy: 0.9586\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x253ccab2488>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 120\n",
    "model.fit(X_train, y_train, \n",
    "         batch_size=batch_size, epochs=epochs, verbose=1, \n",
    "         validation_data=(X_valid, y_valid), \n",
    "         callbacks=[modelcheckpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z8t0nVCw8JmP"
   },
   "outputs": [],
   "source": [
    "model.load_weights(output_dir+\"/weights.100.hdf5\") # 96.32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZwGk5dR8JmS"
   },
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EPc5_h6K8JmW",
    "outputId": "cad99da9-9f89-437f-854b-a315616ed50f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6476"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E_GNq-R_8JmZ",
    "outputId": "0fc16efe-7739-4394-f1a3-46da92e79584"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.1243720e-06, 1.6568372e-06, 7.3366004e-07, 3.5447638e-06,\n",
       "       2.3228047e-06, 9.9999070e-01], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFqRQ5XB8Jmc",
    "outputId": "a957fbed-92f8-4bdd-d24b-5df3361a8bac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FTWf6Cf-8Jme",
    "outputId": "1256b170-33dc-4171-f2ff-7a3d7ddd74e3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_hat)\n",
    "_ = plt.axvline(x=0.5, color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RNljSx3v8Jmh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'99.66'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pct_auc = roc_auc_score(y_valid, y_hat)*100.0\n",
    "\"{:0.2f}\".format(pct_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SO_N9lJG8Jmj",
    "outputId": "20968a52-8ca0-44d3-abf6-a1382bba5cde"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'96.22'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_classes = y_hat.round()\n",
    "acc = score(y_valid, y_pred_classes) * 100\n",
    "\"{:0.2f}\".format(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vU3hnA298Jmo"
   },
   "outputs": [],
   "source": [
    "float_y_hat = []\n",
    "for y in y_hat:\n",
    "    float_y_hat.append(y[0:6].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zrTf-aQl8Jmr"
   },
   "outputs": [],
   "source": [
    "ydf = pd.DataFrame(list(zip(float_y_hat, y_valid)), columns=['y_hat', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zAyKh_mX8Jmu",
    "outputId": "611224b8-f980-47df-d596-9aa3a5811009"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_hat</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            y_hat                               y\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
       "1  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n",
       "2  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
       "3  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "4  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
       "5  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
       "6  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
       "7  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
       "8  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "9  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ydf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "dense_sentiment_classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
