{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ahUZuAfz8Jje"
   },
   "source": [
    "# Sklearn Tuner for LSTM Activity Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Re0ecmVI8Jjk"
   },
   "source": [
    "In this notebook, we use SKlearn  to tune a LSTM neural net to classify PE activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ixsGb9tY8Jjm"
   },
   "source": [
    "#### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5At1PKQp8Jjp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1276608612805494057\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 12984453876963605300\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4022219571\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 4408072045768815224\n",
      "physical_device_desc: \"device: 0, name: Quadro P2000, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 13974246354186393395\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n",
      "# GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv1D, GlobalMaxPooling1D, LSTM\n",
    "from tensorflow.keras.layers import AveragePooling1D, LeakyReLU , MaxPool1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint \n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "import tensorflow as tf\n",
    "print(\"# GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "import sklearn\n",
    "# bug in sklearn wrapper\n",
    "#https://stackoverflow.com/questions/59746974/cannot-clone-object-tensorflow-python-keras-wrappers-scikit-learn-kerasclassifi\n",
    "if sklearn.__version__ != '0.21.2':\n",
    "    print(\"updating sklearn ...\")\n",
    "    !pip install --user scikit-learn==0.21.2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hg5wX2Dd8Jjz"
   },
   "source": [
    "#### Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lYLOM99-8Jj2"
   },
   "outputs": [],
   "source": [
    "# output directory name:\n",
    "output_dir = 'model_output/tune-lstm'\n",
    "input_dir =  'Z:/Research/dfuller/Walkabilly/studies/smarphone_accel/data/Ethica_Jaeger_Merged/pocket/'\n",
    "input_file_name = 'pocket-NN-data.npz'\n",
    "\n",
    "# from the data preparation section we have:\n",
    "window_size_second = 3\n",
    "frequency = 30\n",
    "lenght_of_each_seq = window_size_second * frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn hyperparams\n",
    "params = {\n",
    "    \n",
    "    # LSTM layers\n",
    "    'n_lstm_1' : [128, 256],\n",
    "    'n_lstm_2' : [128, 256],\n",
    "    'n_lstm_3' : [128, 256],\n",
    "    'n_lstm_4' : [128, 256],\n",
    "    'drop_lstm_1' : [0.02,0.1],\n",
    "    'drop_lstm_2' : [0.02,0.1],\n",
    "    'drop_lstm_3' : [0.02,0.1],\n",
    "    'drop_lstm_4' : [0.02,0.1],\n",
    "\n",
    "    \n",
    "    # Dense layers\n",
    "    'n_dense_1':[256, 384, 512],\n",
    "    'dropout_1':[0.2, 0.3],\n",
    "    'n_dense_2':[256, 384, 512],\n",
    "    'dropout_2':[0.2, 0.3],\n",
    "    'activation_conv':['elu', 'relu', LeakyReLU()],\n",
    "    'activation_dense':['elu', 'relu', LeakyReLU()]\n",
    "}\n",
    "\n",
    "# training:\n",
    "n_tune_iter = 20\n",
    "cv = 3\n",
    "epochs = 60\n",
    "batch_size = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to get reproducable results\n",
    "from numpy.random import seed\n",
    "seed(85)\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DaLx4yZ48Jj9"
   },
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CD9X0yJg8Jj_"
   },
   "source": [
    "##### For this notebook we use the acceleration data gathered from the pocket location. It was prepared in the DataPrep-Deep notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6r44JKj-8JkA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acceleration_data\n",
      "metadata\n",
      "labels\n"
     ]
    }
   ],
   "source": [
    "# read the raw file and get the keys:\n",
    "raw_data = np.load(file=input_dir+input_file_name,allow_pickle=True)\n",
    "for k in raw_data.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "\n",
    "accel_array = raw_data['acceleration_data']\n",
    "meta_array = raw_data['metadata']\n",
    "labels_array = raw_data['labels']\n",
    "input_shape = list(accel_array.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WWq3ipbu8JlG"
   },
   "source": [
    "#### Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the  labels to integer.\n",
    "In the raw data format of the labels is String and there are 6 classes. 'Lying', 'Sitting', 'Self Pace walk', 'Running 3 METs',\n",
    "       'Running 5 METs', 'Running 7 METs' <br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LyReiequ8Jln"
   },
   "outputs": [],
   "source": [
    "n_class = len(np.unique(labels_array))\n",
    "class_list, labels_array_int = np.unique(labels_array,return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels_array_int, num_classes=n_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "40x3s0KY8Jlv",
    "outputId": "d3ac3e31-3c47-4b01-c2f9-ada0684b31b7"
   },
   "source": [
    "### Splitting and shuffeling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f5grc2H78Jlz",
    "outputId": "3f7eeb55-f851-4796-de4e-0c68eb9df88d"
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "     accel_array, y, test_size=0.1, random_state=65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "GscSnCpk8Jlm"
   },
   "source": [
    "\n",
    "#### Design neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_lstm_1': [128, 256],\n",
       " 'n_lstm_2': [128, 256],\n",
       " 'n_lstm_3': [128, 256],\n",
       " 'n_lstm_4': [128, 256],\n",
       " 'drop_lstm_1': [0.02, 0.1],\n",
       " 'drop_lstm_2': [0.02, 0.1],\n",
       " 'drop_lstm_3': [0.02, 0.1],\n",
       " 'drop_lstm_4': [0.02, 0.1],\n",
       " 'n_dense_1': [256, 384, 512],\n",
       " 'dropout_1': [0.2, 0.3],\n",
       " 'n_dense_2': [256, 384, 512],\n",
       " 'dropout_2': [0.2, 0.3],\n",
       " 'activation_conv': ['elu',\n",
       "  'relu',\n",
       "  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU at 0x1b6fb510688>],\n",
       " 'activation_dense': ['elu',\n",
       "  'relu',\n",
       "  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU at 0x1b6fb510788>]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model( n_lstm_1=128, n_lstm_2=128, n_lstm_3=128, n_lstm_4=128, drop_lstm_1=0.02, drop_lstm_2=0.02,\n",
    "                 drop_lstm_3=0.02,drop_lstm_4=0.02, n_dense_1=256, dropout_1=0.2,\n",
    "                 n_dense_2=256, dropout_2=0.2, activation_conv= 'relu', activation_dense= 'elu'\n",
    "                ):\n",
    "    model = Sequential()\n",
    "    # LSTM layers\n",
    "    model.add(LSTM(n_lstm_1, dropout=drop_lstm_1, \n",
    "                             return_sequences=True, input_shape=input_shape[1:])) \n",
    "    model.add(LSTM(n_lstm_2, dropout=drop_lstm_2, \n",
    "                             return_sequences=True))\n",
    "    model.add(LSTM(n_lstm_3, dropout=drop_lstm_3, \n",
    "                             return_sequences=True)) \n",
    "    model.add(LSTM(n_lstm_4, dropout=drop_lstm_4, \n",
    "                             return_sequences=True))\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(n_dense_1, activation=activation_dense))\n",
    "    model.add(Dropout(dropout_1))\n",
    "    model.add(Dense(n_dense_2, activation=activation_dense))\n",
    "    model.add(Dropout(dropout_2))\n",
    "    model.add(Dense(n_class, activation='softmax'))\n",
    "    model.summary()\n",
    "    print(f\"\"\"\n",
    "          n_lstm_1 = {n_lstm_1}, n_lstm_2 = {n_lstm_2}, n_lstm_3 = {n_lstm_3}, n_lstm_4 = {n_lstm_4},\n",
    "          drop_lstm_1 = {drop_lstm_1}, drop_lstm_2 = {drop_lstm_2},\n",
    "          drop_lstm_3 = {drop_lstm_3}, drop_lstm_4 = {drop_lstm_4},\n",
    "          n_dense_1 = {n_dense_1}, dropout_1 = {dropout_1},\n",
    "          n_dense_2 = {n_dense_2}, dropout_2 = {dropout_2}, activation_conv=  {activation_conv},\n",
    "          activation_dense=  {activation_dense}\"\"\")\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_16 (LSTM)               (None, 90, 128)           67584     \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 11520)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 256)               2949376   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 3,479,046\n",
      "Trainable params: 3,479,046\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 128, n_lstm_2 = 128, n_lstm_3 = 128, n_lstm_4 = 128,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 256, dropout_2 = 0.2, activation_conv=  relu,\n",
      "          activation_dense=  elu\n"
     ]
    }
   ],
   "source": [
    "model_default = create_model()\n",
    "# model_default.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_20 (LSTM)               (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 256)               5898496   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 7,348,742\n",
      "Trainable params: 7,348,742\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128, n_lstm_3 = 256, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 256, dropout_2 = 0.3, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510688>,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 21s 139ms/step - loss: 1.2610 - accuracy: 0.4689\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 21s 139ms/step - loss: 0.9199 - accuracy: 0.6081\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 21s 139ms/step - loss: 0.8157 - accuracy: 0.6527\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 22s 142ms/step - loss: 0.7361 - accuracy: 0.6924\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.6620 - accuracy: 0.7237\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.5803 - accuracy: 0.7612\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 21s 141ms/step - loss: 0.5357 - accuracy: 0.7835\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.5156 - accuracy: 0.7899\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 22s 142ms/step - loss: 0.4664 - accuracy: 0.8103\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 21s 141ms/step - loss: 0.4294 - accuracy: 0.8303\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.4133 - accuracy: 0.8366\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3956 - accuracy: 0.8443\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3759 - accuracy: 0.8513\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3481 - accuracy: 0.8644\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3461 - accuracy: 0.8665\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3289 - accuracy: 0.8722\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3181 - accuracy: 0.8776\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3026 - accuracy: 0.8804\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2928 - accuracy: 0.8851\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2817 - accuracy: 0.8889\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2875 - accuracy: 0.8871\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2675 - accuracy: 0.8943\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2533 - accuracy: 0.9017\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2539 - accuracy: 0.8991\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2451 - accuracy: 0.9040\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2468 - accuracy: 0.9011\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2276 - accuracy: 0.9096\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2309 - accuracy: 0.9079\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2185 - accuracy: 0.9136\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2114 - accuracy: 0.9161\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2079 - accuracy: 0.9184\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2008 - accuracy: 0.9215\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1964 - accuracy: 0.9236\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1941 - accuracy: 0.9242\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1907 - accuracy: 0.9246\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1887 - accuracy: 0.9274\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2036 - accuracy: 0.9210\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1728 - accuracy: 0.9313\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1780 - accuracy: 0.9300\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1688 - accuracy: 0.9332\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1645 - accuracy: 0.9339\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1660 - accuracy: 0.9341\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1530 - accuracy: 0.9391\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1561 - accuracy: 0.9369\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1608 - accuracy: 0.9369\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1535 - accuracy: 0.9396\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1529 - accuracy: 0.9376\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1424 - accuracy: 0.9439\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1477 - accuracy: 0.9410\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1363 - accuracy: 0.9462\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1336 - accuracy: 0.9476\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1470 - accuracy: 0.9423\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1315 - accuracy: 0.9481\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1343 - accuracy: 0.9468\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1311 - accuracy: 0.9483\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1272 - accuracy: 0.9491\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1317 - accuracy: 0.9493\n",
      "Epoch 58/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1240 - accuracy: 0.9512\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1178 - accuracy: 0.9522\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1175 - accuracy: 0.9531\n",
      "76/76 [==============================] - 4s 50ms/step - loss: 0.3227 - accuracy: 0.9095\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_24 (LSTM)               (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_26 (LSTM)               (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_27 (LSTM)               (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 256)               5898496   \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 7,348,742\n",
      "Trainable params: 7,348,742\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128, n_lstm_3 = 256, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 256, dropout_2 = 0.3, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510688>,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 1.2474 - accuracy: 0.4728\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.9258 - accuracy: 0.6076\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.8271 - accuracy: 0.6473\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.7292 - accuracy: 0.6914\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.6689 - accuracy: 0.7214\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.5942 - accuracy: 0.7551\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.5451 - accuracy: 0.7790\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.4966 - accuracy: 0.8011\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.4662 - accuracy: 0.8135\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.4382 - accuracy: 0.8240\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.4201 - accuracy: 0.8371\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3897 - accuracy: 0.8457\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3786 - accuracy: 0.8522\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3583 - accuracy: 0.8596\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3380 - accuracy: 0.8669\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3222 - accuracy: 0.8723\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3166 - accuracy: 0.8737\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3027 - accuracy: 0.8806\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2968 - accuracy: 0.8832\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2877 - accuracy: 0.8830\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2716 - accuracy: 0.8923\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2684 - accuracy: 0.8916\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2603 - accuracy: 0.8963\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2542 - accuracy: 0.8981\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2449 - accuracy: 0.9008\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2393 - accuracy: 0.9027\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2299 - accuracy: 0.9069\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2278 - accuracy: 0.9067\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2201 - accuracy: 0.9122\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2217 - accuracy: 0.9095\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2147 - accuracy: 0.9137\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2022 - accuracy: 0.9177\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1949 - accuracy: 0.9201\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1968 - accuracy: 0.9218\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1890 - accuracy: 0.9251\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1826 - accuracy: 0.9272\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1808 - accuracy: 0.9283\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1714 - accuracy: 0.9318\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1727 - accuracy: 0.9306\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1601 - accuracy: 0.9360\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1644 - accuracy: 0.9342\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1658 - accuracy: 0.9341\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1591 - accuracy: 0.9371\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1638 - accuracy: 0.9361\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1497 - accuracy: 0.9413\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1454 - accuracy: 0.9433\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1446 - accuracy: 0.9428\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1468 - accuracy: 0.9422\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1444 - accuracy: 0.9416\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1392 - accuracy: 0.9436\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1406 - accuracy: 0.9434\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1397 - accuracy: 0.9432\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1273 - accuracy: 0.9487\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1267 - accuracy: 0.9489\n",
      "Epoch 55/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1294 - accuracy: 0.9474\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1280 - accuracy: 0.9496\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1305 - accuracy: 0.9477\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1188 - accuracy: 0.9526\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 21s 141ms/step - loss: 0.1185 - accuracy: 0.9531\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1161 - accuracy: 0.9523\n",
      "76/76 [==============================] - 4s 50ms/step - loss: 0.4246 - accuracy: 0.8875\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_28 (LSTM)               (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_29 (LSTM)               (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_30 (LSTM)               (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_31 (LSTM)               (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 256)               5898496   \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 7,348,742\n",
      "Trainable params: 7,348,742\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128, n_lstm_3 = 256, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 256, dropout_2 = 0.3, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510688>,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 1.2903 - accuracy: 0.4556\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.9437 - accuracy: 0.5970\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.8252 - accuracy: 0.6496\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.7481 - accuracy: 0.6811\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.6860 - accuracy: 0.7135\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.6070 - accuracy: 0.7481\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.5549 - accuracy: 0.7764\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.5097 - accuracy: 0.7944\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.4841 - accuracy: 0.8046\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.4481 - accuracy: 0.8209\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.4233 - accuracy: 0.8353\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.4003 - accuracy: 0.8404\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3849 - accuracy: 0.8452\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3580 - accuracy: 0.8559\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3435 - accuracy: 0.8633\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3252 - accuracy: 0.8673\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3155 - accuracy: 0.8698\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3068 - accuracy: 0.8734\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2985 - accuracy: 0.8793\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2837 - accuracy: 0.8855\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2815 - accuracy: 0.8878\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2760 - accuracy: 0.8925\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2736 - accuracy: 0.8912\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2591 - accuracy: 0.8968\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2528 - accuracy: 0.8986\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2419 - accuracy: 0.9031\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2335 - accuracy: 0.9050\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2300 - accuracy: 0.9084\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2129 - accuracy: 0.9120\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2106 - accuracy: 0.9146\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2121 - accuracy: 0.9159\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2126 - accuracy: 0.9136\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2004 - accuracy: 0.9196\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1944 - accuracy: 0.9214\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1896 - accuracy: 0.9235\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1769 - accuracy: 0.9280\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1755 - accuracy: 0.9294\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1804 - accuracy: 0.9280\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1717 - accuracy: 0.9311\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1716 - accuracy: 0.9322\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 21s 141ms/step - loss: 0.1682 - accuracy: 0.9335\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1577 - accuracy: 0.9368\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1534 - accuracy: 0.9392\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1588 - accuracy: 0.9367\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1524 - accuracy: 0.9393\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1586 - accuracy: 0.9376\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1462 - accuracy: 0.9411\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1474 - accuracy: 0.9408\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1408 - accuracy: 0.9444\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1352 - accuracy: 0.9465\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1407 - accuracy: 0.9442\n",
      "Epoch 52/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1369 - accuracy: 0.9462\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1366 - accuracy: 0.9447\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1303 - accuracy: 0.9480\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1286 - accuracy: 0.9477\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1233 - accuracy: 0.9499\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1225 - accuracy: 0.9500\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1248 - accuracy: 0.9509\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1308 - accuracy: 0.9488\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1193 - accuracy: 0.9520\n",
      "76/76 [==============================] - 4s 50ms/step - loss: 0.4379 - accuracy: 0.8800\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_32 (LSTM)               (None, 90, 128)           67584     \n",
      "_________________________________________________________________\n",
      "lstm_33 (LSTM)               (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_34 (LSTM)               (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_35 (LSTM)               (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 512)               11796992  \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 13,442,310\n",
      "Trainable params: 13,442,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256, n_lstm_3 = 256, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.1,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 512, dropout_1 = 0.2,\n",
      "          n_dense_2 = 256, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 1.2985 - accuracy: 0.4536\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 1.0048 - accuracy: 0.5770\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.8074 - accuracy: 0.6677\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.6918 - accuracy: 0.7138\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.6210 - accuracy: 0.7451\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.5476 - accuracy: 0.7781\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.4988 - accuracy: 0.7984\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.4606 - accuracy: 0.8119\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.4286 - accuracy: 0.8273\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.3909 - accuracy: 0.8429\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.3850 - accuracy: 0.8464\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.3554 - accuracy: 0.8556\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.3505 - accuracy: 0.8587\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.3269 - accuracy: 0.8700\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.3134 - accuracy: 0.8706\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.3038 - accuracy: 0.8755\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.2914 - accuracy: 0.8815\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.2720 - accuracy: 0.8882\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.2691 - accuracy: 0.8910\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.2641 - accuracy: 0.8923\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.2638 - accuracy: 0.8903\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2564 - accuracy: 0.8958\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.2383 - accuracy: 0.9061\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2440 - accuracy: 0.9036\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.2276 - accuracy: 0.9066\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2260 - accuracy: 0.9082\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.2114 - accuracy: 0.9157\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2040 - accuracy: 0.9170\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1966 - accuracy: 0.9210\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.1977 - accuracy: 0.9205\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1873 - accuracy: 0.9256\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1776 - accuracy: 0.9282\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1753 - accuracy: 0.9302\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1813 - accuracy: 0.9269\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1657 - accuracy: 0.9327\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1681 - accuracy: 0.9325\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1614 - accuracy: 0.9331\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1639 - accuracy: 0.9345\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1636 - accuracy: 0.9342\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1533 - accuracy: 0.9367\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1487 - accuracy: 0.9396\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1581 - accuracy: 0.9371\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1412 - accuracy: 0.9420\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1428 - accuracy: 0.9430\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1395 - accuracy: 0.9444\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1465 - accuracy: 0.9412\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1347 - accuracy: 0.9446\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1335 - accuracy: 0.9459\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1405 - accuracy: 0.9430\n",
      "Epoch 50/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 23s 150ms/step - loss: 0.1304 - accuracy: 0.9466\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1347 - accuracy: 0.9463\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1240 - accuracy: 0.9510\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1292 - accuracy: 0.9496\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1185 - accuracy: 0.9513\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1215 - accuracy: 0.9497\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.1263 - accuracy: 0.9483\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1310 - accuracy: 0.9487\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.1194 - accuracy: 0.9526\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1150 - accuracy: 0.9539\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1184 - accuracy: 0.9528\n",
      "76/76 [==============================] - 4s 53ms/step - loss: 0.3085 - accuracy: 0.9239\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_36 (LSTM)               (None, 90, 128)           67584     \n",
      "_________________________________________________________________\n",
      "lstm_37 (LSTM)               (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_38 (LSTM)               (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_39 (LSTM)               (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 512)               11796992  \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 13,442,310\n",
      "Trainable params: 13,442,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256, n_lstm_3 = 256, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.1,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 512, dropout_1 = 0.2,\n",
      "          n_dense_2 = 256, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 1.2913 - accuracy: 0.4610\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 1.0204 - accuracy: 0.5666\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.8161 - accuracy: 0.6614\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.6955 - accuracy: 0.7090\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.6163 - accuracy: 0.7444\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.5449 - accuracy: 0.7765\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.4897 - accuracy: 0.8010\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.4630 - accuracy: 0.8146\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.4243 - accuracy: 0.8293\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.3997 - accuracy: 0.8389\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.3840 - accuracy: 0.8451\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.3596 - accuracy: 0.8568\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.3384 - accuracy: 0.8633\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.3269 - accuracy: 0.8696\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.3187 - accuracy: 0.8713\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.3099 - accuracy: 0.8734\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2851 - accuracy: 0.8838\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2848 - accuracy: 0.8845\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2719 - accuracy: 0.8900\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2657 - accuracy: 0.8902\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2551 - accuracy: 0.8957\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2565 - accuracy: 0.8934\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2404 - accuracy: 0.8996\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2330 - accuracy: 0.9041\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.2242 - accuracy: 0.9084\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2123 - accuracy: 0.9138\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2111 - accuracy: 0.9132\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2144 - accuracy: 0.9123\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2109 - accuracy: 0.9124\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1987 - accuracy: 0.9190\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1940 - accuracy: 0.9190\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1892 - accuracy: 0.9219\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1849 - accuracy: 0.9244\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1780 - accuracy: 0.9278\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1737 - accuracy: 0.9300\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1672 - accuracy: 0.9330\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1605 - accuracy: 0.9343\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1632 - accuracy: 0.9338\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1583 - accuracy: 0.9362\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1560 - accuracy: 0.9380\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.1513 - accuracy: 0.9382\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1538 - accuracy: 0.9376\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.1478 - accuracy: 0.9395\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1514 - accuracy: 0.9390\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1475 - accuracy: 0.9389\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1449 - accuracy: 0.9427\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1381 - accuracy: 0.9433\n",
      "Epoch 48/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1411 - accuracy: 0.9429\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1420 - accuracy: 0.9435\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1317 - accuracy: 0.9463\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1317 - accuracy: 0.9464\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1281 - accuracy: 0.9472\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.1301 - accuracy: 0.9468\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1242 - accuracy: 0.9493\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1264 - accuracy: 0.9494\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1357 - accuracy: 0.9463\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1215 - accuracy: 0.9511\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.1187 - accuracy: 0.9522\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1200 - accuracy: 0.9516\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1102 - accuracy: 0.9554\n",
      "76/76 [==============================] - 4s 53ms/step - loss: 0.3372 - accuracy: 0.9243\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_40 (LSTM)               (None, 90, 128)           67584     \n",
      "_________________________________________________________________\n",
      "lstm_41 (LSTM)               (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_42 (LSTM)               (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_43 (LSTM)               (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 512)               11796992  \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 13,442,310\n",
      "Trainable params: 13,442,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256, n_lstm_3 = 256, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.1,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 512, dropout_1 = 0.2,\n",
      "          n_dense_2 = 256, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 1.2962 - accuracy: 0.4551\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.9849 - accuracy: 0.5873\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.7780 - accuracy: 0.6783\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.6788 - accuracy: 0.7200\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.6102 - accuracy: 0.7480\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.5428 - accuracy: 0.7805\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.5036 - accuracy: 0.7958\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.4569 - accuracy: 0.8191\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.4260 - accuracy: 0.8309\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.4017 - accuracy: 0.8406\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.3830 - accuracy: 0.8496\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.3558 - accuracy: 0.8582\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.3434 - accuracy: 0.8642\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.3278 - accuracy: 0.8703\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.3089 - accuracy: 0.8766\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2928 - accuracy: 0.8826\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2915 - accuracy: 0.8810\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2889 - accuracy: 0.8829\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2781 - accuracy: 0.8873\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2680 - accuracy: 0.8918\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.2490 - accuracy: 0.8978\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.2493 - accuracy: 0.9005\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2353 - accuracy: 0.9059\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2352 - accuracy: 0.9060\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2285 - accuracy: 0.9077\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2202 - accuracy: 0.9114\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.2150 - accuracy: 0.9124\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2064 - accuracy: 0.9167\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2039 - accuracy: 0.9165\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1904 - accuracy: 0.9226\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1976 - accuracy: 0.9183\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1814 - accuracy: 0.9249\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1874 - accuracy: 0.9250\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1821 - accuracy: 0.9264\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1730 - accuracy: 0.9303\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1779 - accuracy: 0.9291\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1632 - accuracy: 0.9343\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1692 - accuracy: 0.9314\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1660 - accuracy: 0.9347\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1649 - accuracy: 0.9337\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1518 - accuracy: 0.9362\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1528 - accuracy: 0.9392\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1453 - accuracy: 0.9412\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1439 - accuracy: 0.9429\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1545 - accuracy: 0.9378\n",
      "Epoch 46/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 23s 150ms/step - loss: 0.1484 - accuracy: 0.9407\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1337 - accuracy: 0.9464\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1355 - accuracy: 0.9471\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.1345 - accuracy: 0.9463\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1312 - accuracy: 0.9470\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1299 - accuracy: 0.9468\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1325 - accuracy: 0.9476\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1244 - accuracy: 0.9490\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1303 - accuracy: 0.9478\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1210 - accuracy: 0.9502\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1268 - accuracy: 0.9482\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1202 - accuracy: 0.9517\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1151 - accuracy: 0.9533\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1191 - accuracy: 0.9505\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1352 - accuracy: 0.9450\n",
      "76/76 [==============================] - 4s 53ms/step - loss: 0.3359 - accuracy: 0.9163\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_44 (LSTM)               (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_45 (LSTM)               (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_46 (LSTM)               (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_47 (LSTM)               (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 512)               11796992  \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 384)               196992    \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 13,379,206\n",
      "Trainable params: 13,379,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128, n_lstm_3 = 256, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.1,\n",
      "          n_dense_1 = 512, dropout_1 = 0.2,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510688>,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 22s 144ms/step - loss: 1.2391 - accuracy: 0.4727\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 22s 144ms/step - loss: 0.8578 - accuracy: 0.6365\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 22s 144ms/step - loss: 0.7222 - accuracy: 0.7013\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.6276 - accuracy: 0.7434\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 22s 144ms/step - loss: 0.5630 - accuracy: 0.7722\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.5012 - accuracy: 0.7991\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.4616 - accuracy: 0.8182\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.4392 - accuracy: 0.8269\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.4098 - accuracy: 0.8400\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 22s 144ms/step - loss: 0.3893 - accuracy: 0.8474\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.3704 - accuracy: 0.8573\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.3525 - accuracy: 0.8625\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.3364 - accuracy: 0.8661\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.3202 - accuracy: 0.8707\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.3115 - accuracy: 0.8773\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.3014 - accuracy: 0.8769\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2965 - accuracy: 0.8825\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2821 - accuracy: 0.8874\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2721 - accuracy: 0.8919\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2608 - accuracy: 0.8966\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2471 - accuracy: 0.9014\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2495 - accuracy: 0.9000\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2524 - accuracy: 0.8992\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2373 - accuracy: 0.9036\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2367 - accuracy: 0.9047\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2204 - accuracy: 0.9104\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2284 - accuracy: 0.9073\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2147 - accuracy: 0.9146\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2043 - accuracy: 0.9186\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2065 - accuracy: 0.9183\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1990 - accuracy: 0.9191\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1925 - accuracy: 0.9218\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2000 - accuracy: 0.9196\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1947 - accuracy: 0.9208\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1823 - accuracy: 0.9274\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1949 - accuracy: 0.9227\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1854 - accuracy: 0.9267\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1754 - accuracy: 0.9282\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1770 - accuracy: 0.9292\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1684 - accuracy: 0.9314\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1687 - accuracy: 0.9327\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1655 - accuracy: 0.9336\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1682 - accuracy: 0.9324\n",
      "Epoch 44/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1621 - accuracy: 0.9377\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1517 - accuracy: 0.9404\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1641 - accuracy: 0.9344\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1435 - accuracy: 0.9422\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1477 - accuracy: 0.9393\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1543 - accuracy: 0.9371\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1456 - accuracy: 0.9413\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1444 - accuracy: 0.9418\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1459 - accuracy: 0.9418\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1390 - accuracy: 0.9436\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1429 - accuracy: 0.9426\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1502 - accuracy: 0.9413\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1470 - accuracy: 0.9420\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1364 - accuracy: 0.9462\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1233 - accuracy: 0.9498\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1278 - accuracy: 0.9483\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1263 - accuracy: 0.9479\n",
      "76/76 [==============================] - 4s 51ms/step - loss: 0.3353 - accuracy: 0.9085\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_48 (LSTM)               (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_49 (LSTM)               (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_50 (LSTM)               (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_51 (LSTM)               (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 512)               11796992  \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 384)               196992    \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 13,379,206\n",
      "Trainable params: 13,379,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128, n_lstm_3 = 256, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.1,\n",
      "          n_dense_1 = 512, dropout_1 = 0.2,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510688>,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 22s 144ms/step - loss: 1.2543 - accuracy: 0.4686\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.8764 - accuracy: 0.6288\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 22s 144ms/step - loss: 0.7351 - accuracy: 0.6977\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.6348 - accuracy: 0.7379\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 22s 144ms/step - loss: 0.5717 - accuracy: 0.7693\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.5040 - accuracy: 0.7961\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 22s 144ms/step - loss: 0.4649 - accuracy: 0.8134\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.4324 - accuracy: 0.8296\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.4074 - accuracy: 0.8393\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.3790 - accuracy: 0.8517\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.3666 - accuracy: 0.8566\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.3502 - accuracy: 0.8625\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.3303 - accuracy: 0.8691\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.3227 - accuracy: 0.8710\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.3076 - accuracy: 0.8773\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2917 - accuracy: 0.8846\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2890 - accuracy: 0.8846\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2801 - accuracy: 0.8880\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2792 - accuracy: 0.8876\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2667 - accuracy: 0.8940\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2532 - accuracy: 0.8965\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2671 - accuracy: 0.8934\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2421 - accuracy: 0.9020\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2407 - accuracy: 0.9043\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2252 - accuracy: 0.9083\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2238 - accuracy: 0.9101\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2169 - accuracy: 0.9110\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2077 - accuracy: 0.9150\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2121 - accuracy: 0.9148\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1994 - accuracy: 0.9201\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1907 - accuracy: 0.9216\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1883 - accuracy: 0.9244\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1900 - accuracy: 0.9243\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1928 - accuracy: 0.9226\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1862 - accuracy: 0.9240\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1774 - accuracy: 0.9283\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1825 - accuracy: 0.9270\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1690 - accuracy: 0.9330\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1729 - accuracy: 0.9306\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1694 - accuracy: 0.9319\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1597 - accuracy: 0.9346\n",
      "Epoch 42/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1522 - accuracy: 0.9391\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1548 - accuracy: 0.9366\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1671 - accuracy: 0.9325\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1514 - accuracy: 0.9383\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1549 - accuracy: 0.9377\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1498 - accuracy: 0.9396\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1442 - accuracy: 0.9403\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1480 - accuracy: 0.9394\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1476 - accuracy: 0.9400\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1407 - accuracy: 0.9413\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1375 - accuracy: 0.9436\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1312 - accuracy: 0.9457\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1318 - accuracy: 0.9458\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1429 - accuracy: 0.9418\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1341 - accuracy: 0.9436\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1379 - accuracy: 0.9439\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1249 - accuracy: 0.9476\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1218 - accuracy: 0.9485\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1205 - accuracy: 0.9501\n",
      "76/76 [==============================] - 4s 51ms/step - loss: 0.3395 - accuracy: 0.9171\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_52 (LSTM)               (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_53 (LSTM)               (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_54 (LSTM)               (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_55 (LSTM)               (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 512)               11796992  \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 384)               196992    \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 13,379,206\n",
      "Trainable params: 13,379,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128, n_lstm_3 = 256, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.1,\n",
      "          n_dense_1 = 512, dropout_1 = 0.2,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510688>,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 22s 144ms/step - loss: 1.2683 - accuracy: 0.4619\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.9092 - accuracy: 0.6176\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.7478 - accuracy: 0.6904\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 22s 144ms/step - loss: 0.6446 - accuracy: 0.7385\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.5768 - accuracy: 0.7665\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.5214 - accuracy: 0.7928\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.4866 - accuracy: 0.8075\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.4422 - accuracy: 0.8250\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.4167 - accuracy: 0.8349\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.4007 - accuracy: 0.8428\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.3795 - accuracy: 0.8481\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.3594 - accuracy: 0.8564\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.3522 - accuracy: 0.8626\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.3297 - accuracy: 0.8695\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.3112 - accuracy: 0.8756\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.3084 - accuracy: 0.8772\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2957 - accuracy: 0.8817\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2979 - accuracy: 0.8796\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2788 - accuracy: 0.8862\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2702 - accuracy: 0.8914\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2616 - accuracy: 0.8945\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2500 - accuracy: 0.8990\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2505 - accuracy: 0.8973\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2397 - accuracy: 0.9026\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 22s 146ms/step - loss: 0.2353 - accuracy: 0.9065\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2275 - accuracy: 0.9094\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2267 - accuracy: 0.9086\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2198 - accuracy: 0.9110\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2085 - accuracy: 0.9154\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2049 - accuracy: 0.9161\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.2043 - accuracy: 0.9159\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1973 - accuracy: 0.9202\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1956 - accuracy: 0.9224\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1895 - accuracy: 0.9229\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1842 - accuracy: 0.9252\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1867 - accuracy: 0.9258\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1790 - accuracy: 0.9274\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1692 - accuracy: 0.9305\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1770 - accuracy: 0.9297\n",
      "Epoch 40/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1697 - accuracy: 0.9315\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1701 - accuracy: 0.9307\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1673 - accuracy: 0.9331\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1608 - accuracy: 0.9342\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1545 - accuracy: 0.9369\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1569 - accuracy: 0.9367\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1561 - accuracy: 0.9362\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1525 - accuracy: 0.9386\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1489 - accuracy: 0.9399\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1497 - accuracy: 0.9397\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1586 - accuracy: 0.9369\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1413 - accuracy: 0.9444\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1441 - accuracy: 0.9421\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1331 - accuracy: 0.9456\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1321 - accuracy: 0.9465\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1330 - accuracy: 0.9470\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1249 - accuracy: 0.9487\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1344 - accuracy: 0.9470\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1269 - accuracy: 0.9490\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1260 - accuracy: 0.9488\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1235 - accuracy: 0.9513\n",
      "76/76 [==============================] - 4s 51ms/step - loss: 0.3305 - accuracy: 0.9175\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_56 (LSTM)               (None, 90, 128)           67584     \n",
      "_________________________________________________________________\n",
      "lstm_57 (LSTM)               (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_58 (LSTM)               (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_59 (LSTM)               (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 11520)             0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 256)               2949376   \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 4,268,294\n",
      "Trainable params: 4,268,294\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256, n_lstm_3 = 256, n_lstm_4 = 128,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.1,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 512, dropout_2 = 0.3, activation_conv=  elu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 1.2763 - accuracy: 0.4597\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.9238 - accuracy: 0.6082\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.7878 - accuracy: 0.6739\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.7103 - accuracy: 0.7043\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.6518 - accuracy: 0.7312\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.5847 - accuracy: 0.7614\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.5274 - accuracy: 0.7854\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.4875 - accuracy: 0.8036\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.4566 - accuracy: 0.8152\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.4259 - accuracy: 0.8283\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.3965 - accuracy: 0.8404\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3806 - accuracy: 0.8456\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.3613 - accuracy: 0.8535\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3428 - accuracy: 0.8621\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.3357 - accuracy: 0.8658\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3232 - accuracy: 0.8692\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3157 - accuracy: 0.8737\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2921 - accuracy: 0.8829\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.2897 - accuracy: 0.8851\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.2712 - accuracy: 0.8891\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2764 - accuracy: 0.8878\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2556 - accuracy: 0.8957\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2586 - accuracy: 0.8956\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2514 - accuracy: 0.8987\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2472 - accuracy: 0.9010\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2413 - accuracy: 0.9024\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2207 - accuracy: 0.9123\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2240 - accuracy: 0.9097\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2111 - accuracy: 0.9162\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2046 - accuracy: 0.9177\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2045 - accuracy: 0.9187\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.1941 - accuracy: 0.9226\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1862 - accuracy: 0.9249\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1897 - accuracy: 0.9238\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1805 - accuracy: 0.9287\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.1770 - accuracy: 0.9305\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1665 - accuracy: 0.9343\n",
      "Epoch 38/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1762 - accuracy: 0.9292\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1689 - accuracy: 0.9335\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1638 - accuracy: 0.9343\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1558 - accuracy: 0.9385\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1689 - accuracy: 0.9333\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.1634 - accuracy: 0.9358\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.1545 - accuracy: 0.9386\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.1555 - accuracy: 0.9375\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1519 - accuracy: 0.9401\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1479 - accuracy: 0.9404\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1423 - accuracy: 0.9439\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1415 - accuracy: 0.9443\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1324 - accuracy: 0.9477\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1379 - accuracy: 0.9461\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1335 - accuracy: 0.9471\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1306 - accuracy: 0.9484\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1280 - accuracy: 0.9505\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1275 - accuracy: 0.9490\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1265 - accuracy: 0.9500\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1238 - accuracy: 0.9513\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1196 - accuracy: 0.9531\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1218 - accuracy: 0.9514\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1209 - accuracy: 0.9519\n",
      "76/76 [==============================] - 3s 44ms/step - loss: 0.3213 - accuracy: 0.9098\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_60 (LSTM)               (None, 90, 128)           67584     \n",
      "_________________________________________________________________\n",
      "lstm_61 (LSTM)               (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_62 (LSTM)               (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_63 (LSTM)               (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 11520)             0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 256)               2949376   \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 4,268,294\n",
      "Trainable params: 4,268,294\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256, n_lstm_3 = 256, n_lstm_4 = 128,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.1,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 512, dropout_2 = 0.3, activation_conv=  elu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n",
      "  2/152 [..............................] - ETA: 12s - loss: 1.7748 - accuracy: 0.2383WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0574s vs `on_train_batch_end` time: 0.0914s). Check your callbacks.\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 1.2825 - accuracy: 0.4587\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.8993 - accuracy: 0.6188\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.7796 - accuracy: 0.6759\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.7110 - accuracy: 0.7059\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.6394 - accuracy: 0.7346\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.5775 - accuracy: 0.7617\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.5265 - accuracy: 0.7876\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.4757 - accuracy: 0.8069\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.4383 - accuracy: 0.8245\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.4201 - accuracy: 0.8315\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3875 - accuracy: 0.8443\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3740 - accuracy: 0.8506\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3574 - accuracy: 0.8571\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3391 - accuracy: 0.8650\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3247 - accuracy: 0.8717\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3221 - accuracy: 0.8712\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3112 - accuracy: 0.8740\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2900 - accuracy: 0.8840\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3006 - accuracy: 0.8792\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2892 - accuracy: 0.8832\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2764 - accuracy: 0.8888\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2550 - accuracy: 0.8957\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2597 - accuracy: 0.8950\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2432 - accuracy: 0.9018\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2370 - accuracy: 0.9051\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2296 - accuracy: 0.9095\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2207 - accuracy: 0.9108\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2137 - accuracy: 0.9115\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2175 - accuracy: 0.9122\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2075 - accuracy: 0.9149\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2012 - accuracy: 0.9192\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2023 - accuracy: 0.9168\n",
      "Epoch 33/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1927 - accuracy: 0.9210\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1906 - accuracy: 0.9222\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1879 - accuracy: 0.9238\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1803 - accuracy: 0.9289\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1682 - accuracy: 0.9328\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.1830 - accuracy: 0.9246\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1781 - accuracy: 0.9274\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1702 - accuracy: 0.9309\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1586 - accuracy: 0.9352\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1600 - accuracy: 0.9356\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1629 - accuracy: 0.9342\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1503 - accuracy: 0.9392\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1508 - accuracy: 0.9399\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1494 - accuracy: 0.9413\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1470 - accuracy: 0.9396\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1433 - accuracy: 0.9408\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1393 - accuracy: 0.9434\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1456 - accuracy: 0.9418\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1480 - accuracy: 0.9413\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1346 - accuracy: 0.9460\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1384 - accuracy: 0.9442\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1326 - accuracy: 0.9464\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1325 - accuracy: 0.9453\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1261 - accuracy: 0.9477\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1263 - accuracy: 0.9487\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1276 - accuracy: 0.9484\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1296 - accuracy: 0.9473\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1196 - accuracy: 0.9512\n",
      "76/76 [==============================] - 3s 44ms/step - loss: 0.3922 - accuracy: 0.9005\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_64 (LSTM)               (None, 90, 128)           67584     \n",
      "_________________________________________________________________\n",
      "lstm_65 (LSTM)               (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_66 (LSTM)               (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_67 (LSTM)               (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 11520)             0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 256)               2949376   \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 4,268,294\n",
      "Trainable params: 4,268,294\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256, n_lstm_3 = 256, n_lstm_4 = 128,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.1,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 512, dropout_2 = 0.3, activation_conv=  elu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 18s 122ms/step - loss: 1.3028 - accuracy: 0.4441\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.8826 - accuracy: 0.6266\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.7730 - accuracy: 0.6789\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.7047 - accuracy: 0.7109\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.6373 - accuracy: 0.7369\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.5698 - accuracy: 0.7632\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.5212 - accuracy: 0.7874\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.4707 - accuracy: 0.8065\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.4447 - accuracy: 0.8189\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.4182 - accuracy: 0.8324\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3962 - accuracy: 0.8390\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.3762 - accuracy: 0.8468\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3554 - accuracy: 0.8566\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3425 - accuracy: 0.8620\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3267 - accuracy: 0.8681\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3069 - accuracy: 0.8768\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3170 - accuracy: 0.8733\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2875 - accuracy: 0.8837\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2816 - accuracy: 0.8871\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2748 - accuracy: 0.8902\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2646 - accuracy: 0.8922\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2595 - accuracy: 0.8967\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2636 - accuracy: 0.8940\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2459 - accuracy: 0.9021\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2347 - accuracy: 0.9065\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2266 - accuracy: 0.9112\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2148 - accuracy: 0.9139\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2178 - accuracy: 0.9131\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2043 - accuracy: 0.9164\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2062 - accuracy: 0.9176\n",
      "Epoch 31/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2015 - accuracy: 0.9187\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1963 - accuracy: 0.9208\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1854 - accuracy: 0.9249\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1818 - accuracy: 0.9275\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 19s 122ms/step - loss: 0.1787 - accuracy: 0.9293\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1766 - accuracy: 0.9281\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1752 - accuracy: 0.9306\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1741 - accuracy: 0.9318\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1643 - accuracy: 0.9334\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1605 - accuracy: 0.9355\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1683 - accuracy: 0.9326\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1566 - accuracy: 0.9372\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1493 - accuracy: 0.9394\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1504 - accuracy: 0.9402\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1584 - accuracy: 0.9390\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1515 - accuracy: 0.9399\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1457 - accuracy: 0.9418\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1444 - accuracy: 0.9426\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1394 - accuracy: 0.9441\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1456 - accuracy: 0.9421\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1357 - accuracy: 0.9455\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1345 - accuracy: 0.9463\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1386 - accuracy: 0.9454\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1332 - accuracy: 0.9463\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1326 - accuracy: 0.9467\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1371 - accuracy: 0.9445\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1324 - accuracy: 0.9484\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1283 - accuracy: 0.9479\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1207 - accuracy: 0.9515\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1197 - accuracy: 0.9513\n",
      "76/76 [==============================] - 3s 44ms/step - loss: 0.4473 - accuracy: 0.8711\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_68 (LSTM)               (None, 90, 128)           67584     \n",
      "_________________________________________________________________\n",
      "lstm_69 (LSTM)               (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_70 (LSTM)               (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_71 (LSTM)               (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 11520)             0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 256)               2949376   \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 3,479,046\n",
      "Trainable params: 3,479,046\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 128, n_lstm_2 = 128, n_lstm_3 = 128, n_lstm_4 = 128,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.1,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.1,\n",
      "          n_dense_1 = 256, dropout_1 = 0.3,\n",
      "          n_dense_2 = 256, dropout_2 = 0.2, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510688>,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 1.2915 - accuracy: 0.4566\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.9926 - accuracy: 0.5813\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.8713 - accuracy: 0.6248\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.7905 - accuracy: 0.6638\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.7204 - accuracy: 0.6922\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.6480 - accuracy: 0.7278\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.5853 - accuracy: 0.7562\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.5527 - accuracy: 0.7751\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.5099 - accuracy: 0.7930\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.4769 - accuracy: 0.8102\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.4394 - accuracy: 0.8261\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.4183 - accuracy: 0.8346\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.4031 - accuracy: 0.8404\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.3729 - accuracy: 0.8554\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.3695 - accuracy: 0.8539\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.3468 - accuracy: 0.8647\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.3359 - accuracy: 0.8689\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.3222 - accuracy: 0.8732\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.3132 - accuracy: 0.8776\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.3031 - accuracy: 0.8829\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2887 - accuracy: 0.8888\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2964 - accuracy: 0.8839\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2811 - accuracy: 0.8894\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2648 - accuracy: 0.8959\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2586 - accuracy: 0.8998\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2615 - accuracy: 0.8966\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2462 - accuracy: 0.9031\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2394 - accuracy: 0.9059\n",
      "Epoch 29/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2373 - accuracy: 0.9062\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2312 - accuracy: 0.9099\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2239 - accuracy: 0.9118\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2169 - accuracy: 0.9152\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2135 - accuracy: 0.9161\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2083 - accuracy: 0.9180\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2056 - accuracy: 0.9187\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2076 - accuracy: 0.9189\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1943 - accuracy: 0.9231\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1853 - accuracy: 0.9267\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1981 - accuracy: 0.9214\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1851 - accuracy: 0.9267\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1861 - accuracy: 0.9262\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1818 - accuracy: 0.9284\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1778 - accuracy: 0.9282\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1747 - accuracy: 0.9292\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1704 - accuracy: 0.9340\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1667 - accuracy: 0.9342\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1627 - accuracy: 0.9343\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1647 - accuracy: 0.9348\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1610 - accuracy: 0.9366\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1514 - accuracy: 0.9389\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1571 - accuracy: 0.9376\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1596 - accuracy: 0.9380\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1551 - accuracy: 0.9388\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1472 - accuracy: 0.9418\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1474 - accuracy: 0.9410\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1409 - accuracy: 0.9431\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1481 - accuracy: 0.9408\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1363 - accuracy: 0.9458\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 11s 70ms/step - loss: 0.1436 - accuracy: 0.9430\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1445 - accuracy: 0.9418\n",
      "76/76 [==============================] - 2s 26ms/step - loss: 0.3686 - accuracy: 0.8930 0s - loss: 0.3700 - accuracy: \n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_72 (LSTM)               (None, 90, 128)           67584     \n",
      "_________________________________________________________________\n",
      "lstm_73 (LSTM)               (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_74 (LSTM)               (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_75 (LSTM)               (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 11520)             0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 256)               2949376   \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 3,479,046\n",
      "Trainable params: 3,479,046\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 128, n_lstm_2 = 128, n_lstm_3 = 128, n_lstm_4 = 128,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.1,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.1,\n",
      "          n_dense_1 = 256, dropout_1 = 0.3,\n",
      "          n_dense_2 = 256, dropout_2 = 0.2, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510688>,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 1.2622 - accuracy: 0.4661\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 1.0108 - accuracy: 0.5691\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.8760 - accuracy: 0.6298\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.7819 - accuracy: 0.6652\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.7103 - accuracy: 0.7004\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.6317 - accuracy: 0.7354\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.5667 - accuracy: 0.7659\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.5178 - accuracy: 0.7876\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.4880 - accuracy: 0.8012\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.4477 - accuracy: 0.8228\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.4210 - accuracy: 0.8354\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.3993 - accuracy: 0.8421\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.3797 - accuracy: 0.8504\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.3650 - accuracy: 0.8583\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.3447 - accuracy: 0.8642\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.3348 - accuracy: 0.8678\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.3297 - accuracy: 0.8686\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.3147 - accuracy: 0.8733\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.3037 - accuracy: 0.8795\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2937 - accuracy: 0.8835\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2863 - accuracy: 0.8863\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2811 - accuracy: 0.8860\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2694 - accuracy: 0.8915\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2604 - accuracy: 0.8951\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2614 - accuracy: 0.8954\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2483 - accuracy: 0.8994\n",
      "Epoch 27/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2497 - accuracy: 0.8993\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2464 - accuracy: 0.9000\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2381 - accuracy: 0.9022\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2269 - accuracy: 0.9054\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2205 - accuracy: 0.9099\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2135 - accuracy: 0.9127\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2220 - accuracy: 0.9115\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1989 - accuracy: 0.9206\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1941 - accuracy: 0.9216\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2036 - accuracy: 0.9205\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1975 - accuracy: 0.9196\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1862 - accuracy: 0.9247\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1889 - accuracy: 0.9243\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1811 - accuracy: 0.9281\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1829 - accuracy: 0.9254\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1758 - accuracy: 0.9309\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1730 - accuracy: 0.9307\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1691 - accuracy: 0.9320\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1730 - accuracy: 0.9297\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1730 - accuracy: 0.9300\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1706 - accuracy: 0.9318\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1712 - accuracy: 0.9310\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1607 - accuracy: 0.9351\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1524 - accuracy: 0.9379\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1579 - accuracy: 0.9366\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1571 - accuracy: 0.9379\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1464 - accuracy: 0.9415\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1468 - accuracy: 0.9411\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 11s 70ms/step - loss: 0.1424 - accuracy: 0.9424\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1436 - accuracy: 0.9427\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1340 - accuracy: 0.9465\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1463 - accuracy: 0.9424\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1397 - accuracy: 0.9440\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1458 - accuracy: 0.9410\n",
      "76/76 [==============================] - 2s 26ms/step - loss: 0.3622 - accuracy: 0.9038\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_76 (LSTM)               (None, 90, 128)           67584     \n",
      "_________________________________________________________________\n",
      "lstm_77 (LSTM)               (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_78 (LSTM)               (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_79 (LSTM)               (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 11520)             0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 256)               2949376   \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 3,479,046\n",
      "Trainable params: 3,479,046\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 128, n_lstm_2 = 128, n_lstm_3 = 128, n_lstm_4 = 128,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.1,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.1,\n",
      "          n_dense_1 = 256, dropout_1 = 0.3,\n",
      "          n_dense_2 = 256, dropout_2 = 0.2, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510688>,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 1.2928 - accuracy: 0.4553\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.9802 - accuracy: 0.5824\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.8570 - accuracy: 0.6366\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.7807 - accuracy: 0.6701\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.6999 - accuracy: 0.7048\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.6368 - accuracy: 0.7362\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.5663 - accuracy: 0.7671\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.5197 - accuracy: 0.7915\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.4814 - accuracy: 0.8065\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.4503 - accuracy: 0.8228\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.4226 - accuracy: 0.8322\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.3989 - accuracy: 0.8437\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.3798 - accuracy: 0.8509\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.3632 - accuracy: 0.8572\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.3600 - accuracy: 0.8596\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.3401 - accuracy: 0.8656\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.3357 - accuracy: 0.8682\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.3209 - accuracy: 0.8716\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.3088 - accuracy: 0.8782\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.3053 - accuracy: 0.8791\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2965 - accuracy: 0.8817\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2845 - accuracy: 0.8886\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 11s 70ms/step - loss: 0.2778 - accuracy: 0.8888\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2634 - accuracy: 0.8960\n",
      "Epoch 25/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2702 - accuracy: 0.8905\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2594 - accuracy: 0.8979\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2467 - accuracy: 0.9029\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2459 - accuracy: 0.9018\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2343 - accuracy: 0.9083\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2337 - accuracy: 0.9072\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2233 - accuracy: 0.9124\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2229 - accuracy: 0.9115\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2119 - accuracy: 0.9173\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.2091 - accuracy: 0.9164\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2027 - accuracy: 0.9209\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1986 - accuracy: 0.9212\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.2021 - accuracy: 0.9207\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1914 - accuracy: 0.9256\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1867 - accuracy: 0.9268\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1899 - accuracy: 0.9263\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1844 - accuracy: 0.9280\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1787 - accuracy: 0.9299\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1773 - accuracy: 0.9301\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1667 - accuracy: 0.9338\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1653 - accuracy: 0.9346\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1685 - accuracy: 0.9327\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1691 - accuracy: 0.9328\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1626 - accuracy: 0.9375\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1629 - accuracy: 0.9358\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1585 - accuracy: 0.9367\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1547 - accuracy: 0.9389\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1565 - accuracy: 0.9377\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1555 - accuracy: 0.9393\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1449 - accuracy: 0.9429\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1467 - accuracy: 0.9424\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1487 - accuracy: 0.9408\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 11s 69ms/step - loss: 0.1396 - accuracy: 0.9463\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1388 - accuracy: 0.9440\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1355 - accuracy: 0.9459\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 10s 69ms/step - loss: 0.1363 - accuracy: 0.9464\n",
      "76/76 [==============================] - 2s 26ms/step - loss: 0.4094 - accuracy: 0.8843\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_80 (LSTM)               (None, 90, 128)           67584     \n",
      "_________________________________________________________________\n",
      "lstm_81 (LSTM)               (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_82 (LSTM)               (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_83 (LSTM)               (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 512)               11796992  \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 13,575,174\n",
      "Trainable params: 13,575,174\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256, n_lstm_3 = 256, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 512, dropout_1 = 0.3,\n",
      "          n_dense_2 = 512, dropout_2 = 0.2, activation_conv=  elu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 1.2928 - accuracy: 0.4539\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.9487 - accuracy: 0.5958\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.7644 - accuracy: 0.6842\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.6688 - accuracy: 0.7267\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.5950 - accuracy: 0.7537\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.5395 - accuracy: 0.7799\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.4866 - accuracy: 0.8056\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.4612 - accuracy: 0.8167\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.4244 - accuracy: 0.8315\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.3968 - accuracy: 0.8439\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.3780 - accuracy: 0.8516\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.3555 - accuracy: 0.8576\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.3467 - accuracy: 0.8623\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.3308 - accuracy: 0.8690\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.3247 - accuracy: 0.8713\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.3144 - accuracy: 0.8732\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2966 - accuracy: 0.8819\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2835 - accuracy: 0.8873\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2787 - accuracy: 0.8900\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2735 - accuracy: 0.8912\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2586 - accuracy: 0.8958\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2574 - accuracy: 0.8976\n",
      "Epoch 23/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2510 - accuracy: 0.8994\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2408 - accuracy: 0.9035\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2293 - accuracy: 0.9085\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2229 - accuracy: 0.9113\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.2157 - accuracy: 0.9125\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.2075 - accuracy: 0.9171\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2098 - accuracy: 0.9168\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1960 - accuracy: 0.9231\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1921 - accuracy: 0.9229\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1822 - accuracy: 0.9279\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1864 - accuracy: 0.9258\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1863 - accuracy: 0.9254\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1779 - accuracy: 0.9288\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1800 - accuracy: 0.9280\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1615 - accuracy: 0.9359\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1623 - accuracy: 0.9360\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1679 - accuracy: 0.9336\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1594 - accuracy: 0.9366\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1509 - accuracy: 0.9387\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1476 - accuracy: 0.9417\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1514 - accuracy: 0.9394\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1414 - accuracy: 0.9426\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1471 - accuracy: 0.9425\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1480 - accuracy: 0.9422\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1440 - accuracy: 0.9425\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1274 - accuracy: 0.9485\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1286 - accuracy: 0.9483\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1307 - accuracy: 0.9492\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1229 - accuracy: 0.9510\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1234 - accuracy: 0.9517\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1309 - accuracy: 0.9475\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1292 - accuracy: 0.9490\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1197 - accuracy: 0.9534\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1259 - accuracy: 0.9496\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1187 - accuracy: 0.9516\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1128 - accuracy: 0.9542\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1134 - accuracy: 0.9544\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1173 - accuracy: 0.9527\n",
      "76/76 [==============================] - 4s 53ms/step - loss: 0.2982 - accuracy: 0.9243\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_84 (LSTM)               (None, 90, 128)           67584     \n",
      "_________________________________________________________________\n",
      "lstm_85 (LSTM)               (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_86 (LSTM)               (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_87 (LSTM)               (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 512)               11796992  \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 13,575,174\n",
      "Trainable params: 13,575,174\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256, n_lstm_3 = 256, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 512, dropout_1 = 0.3,\n",
      "          n_dense_2 = 512, dropout_2 = 0.2, activation_conv=  elu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 1.3126 - accuracy: 0.4475\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.9611 - accuracy: 0.5884\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.7827 - accuracy: 0.6752\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.6602 - accuracy: 0.7268\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.5862 - accuracy: 0.7595\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.5235 - accuracy: 0.7895\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.4721 - accuracy: 0.8100\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.4419 - accuracy: 0.8226\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.4225 - accuracy: 0.8319\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.3883 - accuracy: 0.8465\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.3793 - accuracy: 0.8513\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.3640 - accuracy: 0.8573\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.3442 - accuracy: 0.8646\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.3235 - accuracy: 0.8725\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.3194 - accuracy: 0.8738\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.3165 - accuracy: 0.8753\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2938 - accuracy: 0.8837\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2891 - accuracy: 0.8848\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2731 - accuracy: 0.8916\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2757 - accuracy: 0.8912\n",
      "Epoch 21/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2589 - accuracy: 0.8946\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2414 - accuracy: 0.9019\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2545 - accuracy: 0.8984\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2443 - accuracy: 0.9001\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2288 - accuracy: 0.9061\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2247 - accuracy: 0.9089\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2174 - accuracy: 0.9114\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2151 - accuracy: 0.9126\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2011 - accuracy: 0.9179\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2006 - accuracy: 0.9180\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1887 - accuracy: 0.9228\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1969 - accuracy: 0.9199\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1911 - accuracy: 0.9235\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1708 - accuracy: 0.9316\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1830 - accuracy: 0.9269\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1754 - accuracy: 0.9276\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1645 - accuracy: 0.9329\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1663 - accuracy: 0.9309\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1645 - accuracy: 0.9334\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1538 - accuracy: 0.9378\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1494 - accuracy: 0.9399\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1464 - accuracy: 0.9413\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1487 - accuracy: 0.9403\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1383 - accuracy: 0.9442\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1366 - accuracy: 0.9440\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1377 - accuracy: 0.9468\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1397 - accuracy: 0.9440\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1250 - accuracy: 0.9499\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1277 - accuracy: 0.9473\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1322 - accuracy: 0.9484\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.1228 - accuracy: 0.9503\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1220 - accuracy: 0.9508\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1195 - accuracy: 0.9531\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1208 - accuracy: 0.9508\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1266 - accuracy: 0.9498\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1159 - accuracy: 0.9533\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1234 - accuracy: 0.9507\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1187 - accuracy: 0.9530\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1101 - accuracy: 0.9550\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1134 - accuracy: 0.9554\n",
      "76/76 [==============================] - 4s 53ms/step - loss: 0.3208 - accuracy: 0.9249\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_88 (LSTM)               (None, 90, 128)           67584     \n",
      "_________________________________________________________________\n",
      "lstm_89 (LSTM)               (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_90 (LSTM)               (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_91 (LSTM)               (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 512)               11796992  \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 13,575,174\n",
      "Trainable params: 13,575,174\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256, n_lstm_3 = 256, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 512, dropout_1 = 0.3,\n",
      "          n_dense_2 = 512, dropout_2 = 0.2, activation_conv=  elu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 1.2963 - accuracy: 0.4518\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.9449 - accuracy: 0.5986\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.7537 - accuracy: 0.6917\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.6614 - accuracy: 0.7281\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.5906 - accuracy: 0.7562\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.5309 - accuracy: 0.7827\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.4859 - accuracy: 0.8057\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.4572 - accuracy: 0.8158\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.4272 - accuracy: 0.8279\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.3978 - accuracy: 0.8410\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.3917 - accuracy: 0.8460\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.3707 - accuracy: 0.8529\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.3479 - accuracy: 0.8607\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.3317 - accuracy: 0.8672\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.3328 - accuracy: 0.8671\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.3113 - accuracy: 0.8753\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.3007 - accuracy: 0.8775\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2845 - accuracy: 0.8869\n",
      "Epoch 19/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2841 - accuracy: 0.8857\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2661 - accuracy: 0.8942\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 23s 150ms/step - loss: 0.2782 - accuracy: 0.8869\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2584 - accuracy: 0.8968\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2482 - accuracy: 0.8986\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2361 - accuracy: 0.9045\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2337 - accuracy: 0.9071\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2254 - accuracy: 0.9097\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2144 - accuracy: 0.9150\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2254 - accuracy: 0.9096\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2077 - accuracy: 0.9158\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.2003 - accuracy: 0.9178\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1970 - accuracy: 0.9213\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1925 - accuracy: 0.9220\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1806 - accuracy: 0.9266\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1842 - accuracy: 0.9268\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1753 - accuracy: 0.9296\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1757 - accuracy: 0.9271\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1615 - accuracy: 0.9337\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1746 - accuracy: 0.9301\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1616 - accuracy: 0.9338\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1697 - accuracy: 0.9309\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1582 - accuracy: 0.9370\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1504 - accuracy: 0.9407\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1495 - accuracy: 0.9402\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1459 - accuracy: 0.9422\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1372 - accuracy: 0.9450\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1429 - accuracy: 0.9441\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1444 - accuracy: 0.9414\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1325 - accuracy: 0.9470\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1332 - accuracy: 0.9475\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1364 - accuracy: 0.9467\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1346 - accuracy: 0.9477\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1308 - accuracy: 0.9498\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1295 - accuracy: 0.9494\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1221 - accuracy: 0.9512\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1207 - accuracy: 0.9534\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1181 - accuracy: 0.9539\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1205 - accuracy: 0.9520\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1161 - accuracy: 0.9538\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1154 - accuracy: 0.9531\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 23s 151ms/step - loss: 0.1124 - accuracy: 0.9563\n",
      "76/76 [==============================] - 4s 53ms/step - loss: 0.3152 - accuracy: 0.9256\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_92 (LSTM)               (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_93 (LSTM)               (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_94 (LSTM)               (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_95 (LSTM)               (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 256)               5898496   \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 384)               98688     \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 6,988,678\n",
      "Trainable params: 6,988,678\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128, n_lstm_3 = 128, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.1,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.3,\n",
      "          n_dense_2 = 384, dropout_2 = 0.2, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510688>,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 1.2753 - accuracy: 0.4655\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.9059 - accuracy: 0.6207\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.7763 - accuracy: 0.6778\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.6806 - accuracy: 0.7189\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.6125 - accuracy: 0.7455\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.5531 - accuracy: 0.7742\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.4813 - accuracy: 0.8050\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.4515 - accuracy: 0.8207\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.4229 - accuracy: 0.8341\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3859 - accuracy: 0.8484\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3708 - accuracy: 0.8556\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3630 - accuracy: 0.8595\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3489 - accuracy: 0.8640\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.3303 - accuracy: 0.8727\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3268 - accuracy: 0.8728\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3122 - accuracy: 0.8777\n",
      "Epoch 17/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3047 - accuracy: 0.8785\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2996 - accuracy: 0.8810\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2828 - accuracy: 0.8888\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2713 - accuracy: 0.8918\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2660 - accuracy: 0.8956\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2617 - accuracy: 0.8965\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2536 - accuracy: 0.8997\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2467 - accuracy: 0.9006\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2305 - accuracy: 0.9077\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2348 - accuracy: 0.9062\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2173 - accuracy: 0.9135\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2255 - accuracy: 0.9106\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2121 - accuracy: 0.9145\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2191 - accuracy: 0.9131\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2041 - accuracy: 0.9186\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.2038 - accuracy: 0.9198\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1921 - accuracy: 0.9249\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1928 - accuracy: 0.9237\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.1768 - accuracy: 0.9301\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1837 - accuracy: 0.9272\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1758 - accuracy: 0.9302\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1777 - accuracy: 0.9299\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1708 - accuracy: 0.9320\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1704 - accuracy: 0.9323\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1604 - accuracy: 0.9362\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1579 - accuracy: 0.9365\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1578 - accuracy: 0.9367\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1583 - accuracy: 0.9372\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1568 - accuracy: 0.9387\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.1555 - accuracy: 0.9360\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1542 - accuracy: 0.9382\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1479 - accuracy: 0.9408\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.1371 - accuracy: 0.9448\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1408 - accuracy: 0.9441\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1387 - accuracy: 0.9446\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1389 - accuracy: 0.9452\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1337 - accuracy: 0.9460\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1362 - accuracy: 0.9466\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1292 - accuracy: 0.9481\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1292 - accuracy: 0.9471\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1287 - accuracy: 0.9483\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1252 - accuracy: 0.9504\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1312 - accuracy: 0.9470\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1193 - accuracy: 0.9509\n",
      "76/76 [==============================] - 3s 40ms/step - loss: 0.2822 - accuracy: 0.9242\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_96 (LSTM)               (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_97 (LSTM)               (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_98 (LSTM)               (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_99 (LSTM)               (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_24 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 256)               5898496   \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 384)               98688     \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 6,988,678\n",
      "Trainable params: 6,988,678\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128, n_lstm_3 = 128, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.1,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.3,\n",
      "          n_dense_2 = 384, dropout_2 = 0.2, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510688>,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 1.2844 - accuracy: 0.4607\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.9071 - accuracy: 0.6233\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.7646 - accuracy: 0.6817\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.6646 - accuracy: 0.7223\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.5879 - accuracy: 0.7567\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.5306 - accuracy: 0.7830\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.4741 - accuracy: 0.8097\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.4343 - accuracy: 0.8292\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.4063 - accuracy: 0.8383\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3900 - accuracy: 0.8437\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3712 - accuracy: 0.8538\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3509 - accuracy: 0.8592\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3426 - accuracy: 0.8666\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3313 - accuracy: 0.8695\n",
      "Epoch 15/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3250 - accuracy: 0.8738\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3032 - accuracy: 0.8789\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.2935 - accuracy: 0.8871\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2956 - accuracy: 0.8847\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2898 - accuracy: 0.8857\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2724 - accuracy: 0.8918\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.2648 - accuracy: 0.8955\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2551 - accuracy: 0.8991\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.2510 - accuracy: 0.9020\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2363 - accuracy: 0.9063\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2293 - accuracy: 0.9075\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2281 - accuracy: 0.9099\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2237 - accuracy: 0.9123\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2117 - accuracy: 0.9155\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.2121 - accuracy: 0.9151\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.2045 - accuracy: 0.9194\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1970 - accuracy: 0.9231\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1988 - accuracy: 0.9222\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1846 - accuracy: 0.9261\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1856 - accuracy: 0.9251\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1836 - accuracy: 0.9275\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.1792 - accuracy: 0.9283\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1727 - accuracy: 0.9312\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1684 - accuracy: 0.9342\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1736 - accuracy: 0.9307\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.1651 - accuracy: 0.9348\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1685 - accuracy: 0.9313\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1568 - accuracy: 0.9366\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1607 - accuracy: 0.9369\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.1511 - accuracy: 0.9391\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1507 - accuracy: 0.9389\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1537 - accuracy: 0.9388\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.1548 - accuracy: 0.9378\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1406 - accuracy: 0.9424\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1448 - accuracy: 0.9444\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1365 - accuracy: 0.9450\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1391 - accuracy: 0.9438\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1387 - accuracy: 0.9455\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1269 - accuracy: 0.9488\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1260 - accuracy: 0.9490\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.1271 - accuracy: 0.9493\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1212 - accuracy: 0.9500\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1306 - accuracy: 0.9488\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1253 - accuracy: 0.9499\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1260 - accuracy: 0.9505\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1172 - accuracy: 0.9519\n",
      "76/76 [==============================] - 3s 40ms/step - loss: 0.3315 - accuracy: 0.9258\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_100 (LSTM)              (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_101 (LSTM)              (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_102 (LSTM)              (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_103 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_25 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 256)               5898496   \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 384)               98688     \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 6,988,678\n",
      "Trainable params: 6,988,678\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128, n_lstm_3 = 128, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.1,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.3,\n",
      "          n_dense_2 = 384, dropout_2 = 0.2, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510688>,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 1.2660 - accuracy: 0.4682\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.9040 - accuracy: 0.6228\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.7584 - accuracy: 0.6845\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.6729 - accuracy: 0.7234\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.5751 - accuracy: 0.7660\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.5183 - accuracy: 0.7949\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.4753 - accuracy: 0.8124\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.4292 - accuracy: 0.8279\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.4137 - accuracy: 0.8365\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3928 - accuracy: 0.8443\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3827 - accuracy: 0.8487\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3597 - accuracy: 0.8581\n",
      "Epoch 13/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3512 - accuracy: 0.8633\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3314 - accuracy: 0.8682\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.3282 - accuracy: 0.8696\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3259 - accuracy: 0.8696\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3037 - accuracy: 0.8804\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2942 - accuracy: 0.8844\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2943 - accuracy: 0.8845\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2864 - accuracy: 0.8881\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2714 - accuracy: 0.8944\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2563 - accuracy: 0.8991\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2628 - accuracy: 0.8961\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2580 - accuracy: 0.8989\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.2400 - accuracy: 0.9042\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2304 - accuracy: 0.9080\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2358 - accuracy: 0.9075\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2279 - accuracy: 0.9109\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2167 - accuracy: 0.9144\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.2057 - accuracy: 0.9186\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2029 - accuracy: 0.9201\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.1986 - accuracy: 0.9213\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2000 - accuracy: 0.9212\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1938 - accuracy: 0.9240\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1857 - accuracy: 0.9287\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1811 - accuracy: 0.9292\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1768 - accuracy: 0.9299\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1762 - accuracy: 0.9309\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1716 - accuracy: 0.9311\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1720 - accuracy: 0.9332\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1632 - accuracy: 0.9361\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1647 - accuracy: 0.9351\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1539 - accuracy: 0.9390\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1580 - accuracy: 0.9387\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1480 - accuracy: 0.9419\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1491 - accuracy: 0.9422\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1540 - accuracy: 0.9400\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1538 - accuracy: 0.9405\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.1442 - accuracy: 0.9436\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1358 - accuracy: 0.9455\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1467 - accuracy: 0.9426\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1402 - accuracy: 0.9463\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1383 - accuracy: 0.9450\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1317 - accuracy: 0.9482\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1314 - accuracy: 0.9482\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1274 - accuracy: 0.9494\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1264 - accuracy: 0.9502\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1247 - accuracy: 0.9495\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1286 - accuracy: 0.9503\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1210 - accuracy: 0.9517\n",
      "76/76 [==============================] - 3s 40ms/step - loss: 0.3051 - accuracy: 0.9200\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_104 (LSTM)              (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_105 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_106 (LSTM)              (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_107 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_26 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 384)               8847744   \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 384)               147840    \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 10,380,806\n",
      "Trainable params: 10,380,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256, n_lstm_3 = 128, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 384, dropout_1 = 0.2,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n",
      "  2/152 [..............................] - ETA: 14s - loss: 1.7735 - accuracy: 0.2207WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0619s vs `on_train_batch_end` time: 0.0937s). Check your callbacks.\n",
      "152/152 [==============================] - 22s 142ms/step - loss: 1.2669 - accuracy: 0.4631\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.9132 - accuracy: 0.6084\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.8014 - accuracy: 0.6613\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.7193 - accuracy: 0.6985\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.6422 - accuracy: 0.7315\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.5681 - accuracy: 0.7655\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.5129 - accuracy: 0.7938\n",
      "Epoch 8/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 22s 143ms/step - loss: 0.4821 - accuracy: 0.8083\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.4430 - accuracy: 0.8220\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.4139 - accuracy: 0.8369\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.3819 - accuracy: 0.8498\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.3658 - accuracy: 0.8554\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.3507 - accuracy: 0.8623\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.3295 - accuracy: 0.8696\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.3233 - accuracy: 0.8702\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.3202 - accuracy: 0.8739\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.3032 - accuracy: 0.8768\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2946 - accuracy: 0.8815\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2897 - accuracy: 0.8838\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2754 - accuracy: 0.8888\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2654 - accuracy: 0.8939\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2498 - accuracy: 0.9013\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2458 - accuracy: 0.9015\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2419 - accuracy: 0.9036\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2343 - accuracy: 0.9052\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2264 - accuracy: 0.9099\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2252 - accuracy: 0.9119\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2147 - accuracy: 0.9144\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2159 - accuracy: 0.9155\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2066 - accuracy: 0.9166\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2109 - accuracy: 0.9150\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1965 - accuracy: 0.9230\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1895 - accuracy: 0.9240\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1776 - accuracy: 0.9290\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1775 - accuracy: 0.9283\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1799 - accuracy: 0.9301\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1727 - accuracy: 0.9325\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1749 - accuracy: 0.9302\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1687 - accuracy: 0.9315\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1583 - accuracy: 0.9357\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1698 - accuracy: 0.9327\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1585 - accuracy: 0.9370\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1652 - accuracy: 0.9354\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1512 - accuracy: 0.9393\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1519 - accuracy: 0.9396\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1538 - accuracy: 0.9399\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1443 - accuracy: 0.9417\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1437 - accuracy: 0.9425\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1372 - accuracy: 0.9458\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1310 - accuracy: 0.9478\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1312 - accuracy: 0.9497\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1396 - accuracy: 0.9448\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1305 - accuracy: 0.9481\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1366 - accuracy: 0.9461\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1302 - accuracy: 0.9478\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1239 - accuracy: 0.9516\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1276 - accuracy: 0.9494\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1213 - accuracy: 0.9511\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1223 - accuracy: 0.9509\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1173 - accuracy: 0.9530\n",
      "76/76 [==============================] - 4s 50ms/step - loss: 0.4641 - accuracy: 0.8778\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_108 (LSTM)              (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_109 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_110 (LSTM)              (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_111 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_27 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 384)               8847744   \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 384)               147840    \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 10,380,806\n",
      "Trainable params: 10,380,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256, n_lstm_3 = 128, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 384, dropout_1 = 0.2,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 22s 142ms/step - loss: 1.2697 - accuracy: 0.4638\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.9262 - accuracy: 0.6009\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.8135 - accuracy: 0.6543\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.7256 - accuracy: 0.6920\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.6539 - accuracy: 0.7271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.5853 - accuracy: 0.7583\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.5233 - accuracy: 0.7914\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.4933 - accuracy: 0.8013\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.4454 - accuracy: 0.8246\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.4242 - accuracy: 0.8354\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.4009 - accuracy: 0.8448\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.3804 - accuracy: 0.8521\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.3559 - accuracy: 0.8613\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.3385 - accuracy: 0.8678\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.3247 - accuracy: 0.8733\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.3130 - accuracy: 0.8760\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.3058 - accuracy: 0.8784\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2959 - accuracy: 0.8838\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2844 - accuracy: 0.8869\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2757 - accuracy: 0.8901\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2587 - accuracy: 0.8979\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2609 - accuracy: 0.8964\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2554 - accuracy: 0.8987\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2527 - accuracy: 0.8993\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2371 - accuracy: 0.9041\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2287 - accuracy: 0.9094\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2259 - accuracy: 0.9097\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2190 - accuracy: 0.9108\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2136 - accuracy: 0.9145\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2067 - accuracy: 0.9173\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2118 - accuracy: 0.9152\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1970 - accuracy: 0.9216\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1992 - accuracy: 0.9193\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1820 - accuracy: 0.9264\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1871 - accuracy: 0.9253\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1871 - accuracy: 0.9245\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1764 - accuracy: 0.9294\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1740 - accuracy: 0.9288\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1692 - accuracy: 0.9327\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1603 - accuracy: 0.9357\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1643 - accuracy: 0.9337\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1574 - accuracy: 0.9367\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1611 - accuracy: 0.9344\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1497 - accuracy: 0.9401\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1478 - accuracy: 0.9402\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1480 - accuracy: 0.9407\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1441 - accuracy: 0.9409\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1487 - accuracy: 0.9422\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1436 - accuracy: 0.9430\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1358 - accuracy: 0.9447\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1303 - accuracy: 0.9472\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1286 - accuracy: 0.9485\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1362 - accuracy: 0.9446\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1285 - accuracy: 0.9480\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1292 - accuracy: 0.9476\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1310 - accuracy: 0.9472\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1265 - accuracy: 0.9491\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1205 - accuracy: 0.9502\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1195 - accuracy: 0.9531\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1146 - accuracy: 0.9538\n",
      "76/76 [==============================] - 4s 50ms/step - loss: 0.4137 - accuracy: 0.8850\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_112 (LSTM)              (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_113 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_114 (LSTM)              (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_115 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_28 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 384)               8847744   \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 384)               147840    \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 10,380,806\n",
      "Trainable params: 10,380,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256, n_lstm_3 = 128, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 384, dropout_1 = 0.2,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 22s 142ms/step - loss: 1.2807 - accuracy: 0.4586\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.9303 - accuracy: 0.6048\n",
      "Epoch 3/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 22s 143ms/step - loss: 0.8115 - accuracy: 0.6573\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.7140 - accuracy: 0.7025\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.6381 - accuracy: 0.7376\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.5729 - accuracy: 0.7665\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.5204 - accuracy: 0.7883\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.4798 - accuracy: 0.8090\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.4485 - accuracy: 0.8223\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.4159 - accuracy: 0.8368\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.4060 - accuracy: 0.8408\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.3739 - accuracy: 0.8532\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.3555 - accuracy: 0.8603\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.3373 - accuracy: 0.8672\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.3276 - accuracy: 0.8717\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.3152 - accuracy: 0.8777\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.3048 - accuracy: 0.8784\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2941 - accuracy: 0.8821\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2854 - accuracy: 0.8865\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2804 - accuracy: 0.8876\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2716 - accuracy: 0.8890\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2609 - accuracy: 0.8928\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2505 - accuracy: 0.8986\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2398 - accuracy: 0.9038\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2466 - accuracy: 0.9014\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2308 - accuracy: 0.9075\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2291 - accuracy: 0.9103\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2149 - accuracy: 0.9134\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2192 - accuracy: 0.9110\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2121 - accuracy: 0.9149\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.2055 - accuracy: 0.9190\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1996 - accuracy: 0.9210\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1930 - accuracy: 0.9224\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1823 - accuracy: 0.9269\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1819 - accuracy: 0.9269\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1804 - accuracy: 0.9287\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1753 - accuracy: 0.9309\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1784 - accuracy: 0.9269\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1707 - accuracy: 0.9331\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1682 - accuracy: 0.9333\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1622 - accuracy: 0.9360\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1557 - accuracy: 0.9379\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1580 - accuracy: 0.9374\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1484 - accuracy: 0.9424\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1533 - accuracy: 0.9397\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1481 - accuracy: 0.9419\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1548 - accuracy: 0.9388\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1510 - accuracy: 0.9417\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1459 - accuracy: 0.9419\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1456 - accuracy: 0.9416\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1378 - accuracy: 0.9454\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1389 - accuracy: 0.9454\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1327 - accuracy: 0.9487\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1305 - accuracy: 0.9494\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1350 - accuracy: 0.9480\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1283 - accuracy: 0.9481\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1372 - accuracy: 0.9445\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1235 - accuracy: 0.9490\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1254 - accuracy: 0.9497\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 22s 143ms/step - loss: 0.1187 - accuracy: 0.9522\n",
      "76/76 [==============================] - 4s 50ms/step - loss: 0.4417 - accuracy: 0.8793\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_116 (LSTM)              (None, 90, 128)           67584     \n",
      "_________________________________________________________________\n",
      "lstm_117 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_118 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_119 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_29 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 384)               8847744   \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 256)               98560     \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 10,460,294\n",
      "Trainable params: 10,460,294\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256, n_lstm_3 = 256, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.1,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.1,\n",
      "          n_dense_1 = 384, dropout_1 = 0.3,\n",
      "          n_dense_2 = 256, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 22s 147ms/step - loss: 1.3064 - accuracy: 0.4500\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 1.0123 - accuracy: 0.5707\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.8606 - accuracy: 0.6363\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.7707 - accuracy: 0.6770\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.7004 - accuracy: 0.7053\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.6205 - accuracy: 0.7433\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.5571 - accuracy: 0.7720\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.5131 - accuracy: 0.7913\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.4746 - accuracy: 0.8077\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.4346 - accuracy: 0.8271\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.4081 - accuracy: 0.8393\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.3800 - accuracy: 0.8461\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.3749 - accuracy: 0.8486\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.3531 - accuracy: 0.8613\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.3456 - accuracy: 0.8594\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.3327 - accuracy: 0.8672\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.3210 - accuracy: 0.8702\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.3045 - accuracy: 0.8758\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.3010 - accuracy: 0.8781\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2874 - accuracy: 0.8839\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2850 - accuracy: 0.8852\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2709 - accuracy: 0.8899\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2703 - accuracy: 0.8914\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2656 - accuracy: 0.8893\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2511 - accuracy: 0.8985\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2416 - accuracy: 0.9026\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2395 - accuracy: 0.9034\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2310 - accuracy: 0.9089\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2225 - accuracy: 0.9100\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2157 - accuracy: 0.9138\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2128 - accuracy: 0.9157\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2096 - accuracy: 0.9161\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2116 - accuracy: 0.9156\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1952 - accuracy: 0.9225\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1905 - accuracy: 0.9242\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1921 - accuracy: 0.9236\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1886 - accuracy: 0.9254\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1791 - accuracy: 0.9264\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1781 - accuracy: 0.9278\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1745 - accuracy: 0.9312\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1709 - accuracy: 0.9326\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1685 - accuracy: 0.9327\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1604 - accuracy: 0.9357\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1610 - accuracy: 0.9367\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1585 - accuracy: 0.9375\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1564 - accuracy: 0.9389\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1646 - accuracy: 0.9343\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1570 - accuracy: 0.9374\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1553 - accuracy: 0.9366\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1445 - accuracy: 0.9441\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1460 - accuracy: 0.9427\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1396 - accuracy: 0.9448\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1432 - accuracy: 0.9447\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1361 - accuracy: 0.9462\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1428 - accuracy: 0.9442\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 23s 149ms/step - loss: 0.1297 - accuracy: 0.9482\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1339 - accuracy: 0.9472\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 23s 149ms/step - loss: 0.1310 - accuracy: 0.9485\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1272 - accuracy: 0.9479\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1322 - accuracy: 0.9481\n",
      "76/76 [==============================] - 4s 53ms/step - loss: 0.4245 - accuracy: 0.8827\n",
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_120 (LSTM)              (None, 90, 128)           67584     \n",
      "_________________________________________________________________\n",
      "lstm_121 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_122 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_123 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_30 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 384)               8847744   \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 256)               98560     \n",
      "_________________________________________________________________\n",
      "dropout_61 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 10,460,294\n",
      "Trainable params: 10,460,294\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256, n_lstm_3 = 256, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.1,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.1,\n",
      "          n_dense_1 = 384, dropout_1 = 0.3,\n",
      "          n_dense_2 = 256, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2/152 [..............................] - ETA: 15s - loss: 1.9005 - accuracy: 0.2109WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0599s vs `on_train_batch_end` time: 0.1094s). Check your callbacks.\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 1.2910 - accuracy: 0.4566\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 1.0051 - accuracy: 0.5726\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.8454 - accuracy: 0.6458\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.7482 - accuracy: 0.6880\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.6785 - accuracy: 0.7196\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.5992 - accuracy: 0.7522\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.5413 - accuracy: 0.7796\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.4996 - accuracy: 0.7971\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.4504 - accuracy: 0.8172\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.4248 - accuracy: 0.8250\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.3955 - accuracy: 0.8454\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.3735 - accuracy: 0.8517\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.3557 - accuracy: 0.8550\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.3459 - accuracy: 0.8623\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.3304 - accuracy: 0.8697\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.3121 - accuracy: 0.8761\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.3089 - accuracy: 0.8753\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2970 - accuracy: 0.8788\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2910 - accuracy: 0.8818\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2785 - accuracy: 0.8863\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2704 - accuracy: 0.8905\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2622 - accuracy: 0.8896\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2571 - accuracy: 0.8935\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2450 - accuracy: 0.8976\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2421 - accuracy: 0.8992\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2332 - accuracy: 0.9022\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2373 - accuracy: 0.9002\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2302 - accuracy: 0.9033\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2115 - accuracy: 0.9125\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2162 - accuracy: 0.9112\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2103 - accuracy: 0.9138\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2071 - accuracy: 0.9115\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1982 - accuracy: 0.9176\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2016 - accuracy: 0.9164\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 23s 149ms/step - loss: 0.1897 - accuracy: 0.9190\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2081 - accuracy: 0.9135\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1959 - accuracy: 0.9173\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1786 - accuracy: 0.9267\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1869 - accuracy: 0.9231\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1629 - accuracy: 0.9357\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1740 - accuracy: 0.9278\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1682 - accuracy: 0.9295\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1758 - accuracy: 0.9282\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1688 - accuracy: 0.9294\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1575 - accuracy: 0.9350\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1586 - accuracy: 0.9330\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1555 - accuracy: 0.9357\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1570 - accuracy: 0.9361\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1549 - accuracy: 0.9363\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1454 - accuracy: 0.9393\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1566 - accuracy: 0.9368\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1474 - accuracy: 0.9400\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1471 - accuracy: 0.9382\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1411 - accuracy: 0.9417\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1407 - accuracy: 0.9433\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1352 - accuracy: 0.9451\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1268 - accuracy: 0.9493\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1307 - accuracy: 0.9477\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1276 - accuracy: 0.9467\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1256 - accuracy: 0.9490\n",
      "76/76 [==============================] - 4s 53ms/step - loss: 0.4854 - accuracy: 0.8744\n",
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_124 (LSTM)              (None, 90, 128)           67584     \n",
      "_________________________________________________________________\n",
      "lstm_125 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_126 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_127 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_31 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 384)               8847744   \n",
      "_________________________________________________________________\n",
      "dropout_62 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 256)               98560     \n",
      "_________________________________________________________________\n",
      "dropout_63 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 10,460,294\n",
      "Trainable params: 10,460,294\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256, n_lstm_3 = 256, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.1,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.1,\n",
      "          n_dense_1 = 384, dropout_1 = 0.3,\n",
      "          n_dense_2 = 256, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 22s 147ms/step - loss: 1.3198 - accuracy: 0.4467\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 1.0034 - accuracy: 0.5784\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.8604 - accuracy: 0.6380\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.7720 - accuracy: 0.6774\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.6893 - accuracy: 0.7124\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.6191 - accuracy: 0.7451\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.5497 - accuracy: 0.7794\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.4919 - accuracy: 0.8038\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.4635 - accuracy: 0.8159\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.4334 - accuracy: 0.8292\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.4090 - accuracy: 0.8396\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.3792 - accuracy: 0.8508\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.3646 - accuracy: 0.8571\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.3425 - accuracy: 0.8644\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.3448 - accuracy: 0.8633\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.3263 - accuracy: 0.8694\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.3183 - accuracy: 0.8726\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.3038 - accuracy: 0.8804\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.3020 - accuracy: 0.8782\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2925 - accuracy: 0.8828\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2789 - accuracy: 0.8874\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2699 - accuracy: 0.8909\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2536 - accuracy: 0.8974\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2500 - accuracy: 0.8976\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2471 - accuracy: 0.8992\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2455 - accuracy: 0.9015\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2382 - accuracy: 0.9042\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2282 - accuracy: 0.9071\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2193 - accuracy: 0.9114\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2171 - accuracy: 0.9127\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2082 - accuracy: 0.9161\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2123 - accuracy: 0.9144\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1987 - accuracy: 0.9216\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1960 - accuracy: 0.9215\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1939 - accuracy: 0.9225\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1862 - accuracy: 0.9252\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1853 - accuracy: 0.9268\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1827 - accuracy: 0.9266\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1798 - accuracy: 0.9288\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1709 - accuracy: 0.9318\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 23s 149ms/step - loss: 0.1719 - accuracy: 0.9306\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1678 - accuracy: 0.9318\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1651 - accuracy: 0.9347\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1625 - accuracy: 0.9346\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1620 - accuracy: 0.9372\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1522 - accuracy: 0.9401\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1595 - accuracy: 0.9362\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1504 - accuracy: 0.9410\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1421 - accuracy: 0.9442\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1430 - accuracy: 0.9420\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1487 - accuracy: 0.9414\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1404 - accuracy: 0.9441\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1349 - accuracy: 0.9457\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1450 - accuracy: 0.9432\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1331 - accuracy: 0.9476\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1283 - accuracy: 0.9480\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1252 - accuracy: 0.9519\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1294 - accuracy: 0.9484\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1232 - accuracy: 0.9513\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.1244 - accuracy: 0.9507\n",
      "76/76 [==============================] - 4s 53ms/step - loss: 0.3795 - accuracy: 0.8922\n",
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_128 (LSTM)              (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_129 (LSTM)              (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_130 (LSTM)              (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_131 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_32 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 256)               5898496   \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 6,955,014\n",
      "Trainable params: 6,955,014\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128, n_lstm_3 = 128, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.1,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.1,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 256, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 17s 113ms/step - loss: 1.2696 - accuracy: 0.4612\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.8887 - accuracy: 0.6252\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.7614 - accuracy: 0.6839\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.6759 - accuracy: 0.7208\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.5970 - accuracy: 0.7532\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.5437 - accuracy: 0.7765\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.4949 - accuracy: 0.7977\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.4539 - accuracy: 0.8194\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.4184 - accuracy: 0.8342\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.4018 - accuracy: 0.8404\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3912 - accuracy: 0.8466\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3710 - accuracy: 0.8549\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.3527 - accuracy: 0.8637\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3345 - accuracy: 0.8696\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3300 - accuracy: 0.8684\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3206 - accuracy: 0.8741\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3021 - accuracy: 0.8813\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2993 - accuracy: 0.8809\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2911 - accuracy: 0.8861\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2729 - accuracy: 0.8933\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2686 - accuracy: 0.8939\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2544 - accuracy: 0.9001\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2565 - accuracy: 0.9021\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2490 - accuracy: 0.9022\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2372 - accuracy: 0.9056\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2377 - accuracy: 0.9066\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2239 - accuracy: 0.9125\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2142 - accuracy: 0.9145\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2235 - accuracy: 0.9120\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2120 - accuracy: 0.9145\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2181 - accuracy: 0.9139\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2031 - accuracy: 0.9184\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2055 - accuracy: 0.9179\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1850 - accuracy: 0.9260\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1820 - accuracy: 0.9284\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.1832 - accuracy: 0.9289\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.1818 - accuracy: 0.9280\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1690 - accuracy: 0.9335\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1698 - accuracy: 0.9331\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1788 - accuracy: 0.9292\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1677 - accuracy: 0.9342\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1536 - accuracy: 0.9397\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1613 - accuracy: 0.9355\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1598 - accuracy: 0.9366\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1523 - accuracy: 0.9400\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1470 - accuracy: 0.9414\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1526 - accuracy: 0.9386\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1449 - accuracy: 0.9435\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1410 - accuracy: 0.9448\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1386 - accuracy: 0.9445\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1321 - accuracy: 0.9479\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1395 - accuracy: 0.9458\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1366 - accuracy: 0.9474\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1324 - accuracy: 0.9466\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1382 - accuracy: 0.9444\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1239 - accuracy: 0.9511\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1283 - accuracy: 0.9492\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1290 - accuracy: 0.9500\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1150 - accuracy: 0.9535\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1326 - accuracy: 0.9474\n",
      "76/76 [==============================] - 3s 40ms/step - loss: 0.2930 - accuracy: 0.9232\n",
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_132 (LSTM)              (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_133 (LSTM)              (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_134 (LSTM)              (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_135 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_33 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 256)               5898496   \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 6,955,014\n",
      "Trainable params: 6,955,014\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128, n_lstm_3 = 128, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.1,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.1,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 256, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2/152 [..............................] - ETA: 11s - loss: 1.8914 - accuracy: 0.1895WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0483s vs `on_train_batch_end` time: 0.0781s). Check your callbacks.\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 1.2460 - accuracy: 0.4745\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.9122 - accuracy: 0.6215\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.7686 - accuracy: 0.6781\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.6716 - accuracy: 0.7227\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.5953 - accuracy: 0.7558\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.5220 - accuracy: 0.7886\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.4796 - accuracy: 0.8082\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.4346 - accuracy: 0.8303\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.4154 - accuracy: 0.8379\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.4000 - accuracy: 0.8434\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3734 - accuracy: 0.8549\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3529 - accuracy: 0.8604\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3530 - accuracy: 0.8617\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.3375 - accuracy: 0.8673\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3326 - accuracy: 0.8677\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3263 - accuracy: 0.8678\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3114 - accuracy: 0.8771\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.2991 - accuracy: 0.8820\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2936 - accuracy: 0.8833\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2875 - accuracy: 0.8836\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2821 - accuracy: 0.8875\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2580 - accuracy: 0.8966\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2496 - accuracy: 0.9014\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2544 - accuracy: 0.8997\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2490 - accuracy: 0.9013\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2361 - accuracy: 0.9058\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2332 - accuracy: 0.9050\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2259 - accuracy: 0.9089\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2138 - accuracy: 0.9136\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2037 - accuracy: 0.9177\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2061 - accuracy: 0.9185\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1996 - accuracy: 0.9210\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1962 - accuracy: 0.9192\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1998 - accuracy: 0.9188\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1908 - accuracy: 0.9249\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.1772 - accuracy: 0.9284\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1796 - accuracy: 0.9292\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1785 - accuracy: 0.9288\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1819 - accuracy: 0.9269\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.1688 - accuracy: 0.9317\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1700 - accuracy: 0.9319\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1681 - accuracy: 0.9333\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1607 - accuracy: 0.9366\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1601 - accuracy: 0.9358\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1506 - accuracy: 0.9404\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1513 - accuracy: 0.9402\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1526 - accuracy: 0.9395\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1444 - accuracy: 0.9424\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1466 - accuracy: 0.9410\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1373 - accuracy: 0.9437\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1422 - accuracy: 0.9428\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1352 - accuracy: 0.9451\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1322 - accuracy: 0.9465\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.1376 - accuracy: 0.9446\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1309 - accuracy: 0.9478\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1336 - accuracy: 0.9456\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1247 - accuracy: 0.9502\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1261 - accuracy: 0.9494\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1205 - accuracy: 0.9514\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1323 - accuracy: 0.9486\n",
      "76/76 [==============================] - 3s 40ms/step - loss: 0.3218 - accuracy: 0.9212\n",
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_136 (LSTM)              (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_137 (LSTM)              (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_138 (LSTM)              (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_139 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_34 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 256)               5898496   \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 6,955,014\n",
      "Trainable params: 6,955,014\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128, n_lstm_3 = 128, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.1,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.1,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 256, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  elu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 17s 113ms/step - loss: 1.2510 - accuracy: 0.4755\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.8822 - accuracy: 0.6343\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.7487 - accuracy: 0.6888\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.6624 - accuracy: 0.7264\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.5862 - accuracy: 0.7630\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.5208 - accuracy: 0.7907\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.4884 - accuracy: 0.8059\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.4439 - accuracy: 0.8233\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.4190 - accuracy: 0.8321\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.4059 - accuracy: 0.8406\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3850 - accuracy: 0.8483\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3644 - accuracy: 0.8573\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3451 - accuracy: 0.8621\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3384 - accuracy: 0.8650\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3375 - accuracy: 0.8661\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3258 - accuracy: 0.8701\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.3128 - accuracy: 0.8748\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.2899 - accuracy: 0.8841\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2807 - accuracy: 0.8899\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2738 - accuracy: 0.8914\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2634 - accuracy: 0.8953\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2642 - accuracy: 0.8948\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2434 - accuracy: 0.9031\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2514 - accuracy: 0.9009\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2411 - accuracy: 0.9035\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2319 - accuracy: 0.9099\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2138 - accuracy: 0.9144\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2241 - accuracy: 0.9115\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2128 - accuracy: 0.9156\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.2130 - accuracy: 0.9157\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1998 - accuracy: 0.9210\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.2008 - accuracy: 0.9208\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1892 - accuracy: 0.9241\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1852 - accuracy: 0.9276\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1843 - accuracy: 0.9282\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1791 - accuracy: 0.9292\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1731 - accuracy: 0.9323\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1642 - accuracy: 0.9354\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1715 - accuracy: 0.9327\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.1784 - accuracy: 0.9283\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1624 - accuracy: 0.9363\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.1604 - accuracy: 0.9386\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1573 - accuracy: 0.9375\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1580 - accuracy: 0.9375\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.1517 - accuracy: 0.9407\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1535 - accuracy: 0.9390\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1465 - accuracy: 0.9415\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1552 - accuracy: 0.9382\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1460 - accuracy: 0.9425\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1457 - accuracy: 0.9422\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1373 - accuracy: 0.9461\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1321 - accuracy: 0.9480\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1335 - accuracy: 0.9476\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1259 - accuracy: 0.9508\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1350 - accuracy: 0.9461\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1265 - accuracy: 0.9502\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1254 - accuracy: 0.9488\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1248 - accuracy: 0.9507\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 17s 114ms/step - loss: 0.1197 - accuracy: 0.9515\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 17s 113ms/step - loss: 0.1178 - accuracy: 0.9529\n",
      "76/76 [==============================] - 3s 40ms/step - loss: 0.3087 - accuracy: 0.9222\n",
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_140 (LSTM)              (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_141 (LSTM)              (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_142 (LSTM)              (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_143 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_35 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 512)               11796992  \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 12,919,046\n",
      "Trainable params: 12,919,046\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128, n_lstm_3 = 128, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.1,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 512, dropout_1 = 0.3,\n",
      "          n_dense_2 = 256, dropout_2 = 0.2, activation_conv=  relu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 18s 118ms/step - loss: 1.2428 - accuracy: 0.4799\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.8695 - accuracy: 0.6331\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.7328 - accuracy: 0.6929\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.6455 - accuracy: 0.7350\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.5727 - accuracy: 0.7712\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.5123 - accuracy: 0.7965\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.4613 - accuracy: 0.8188\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.4407 - accuracy: 0.8263\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.4145 - accuracy: 0.8379\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3888 - accuracy: 0.8473\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3660 - accuracy: 0.8554\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3491 - accuracy: 0.8615\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3419 - accuracy: 0.8663\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3294 - accuracy: 0.8708\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3232 - accuracy: 0.8714\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3137 - accuracy: 0.8751\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3008 - accuracy: 0.8805\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2923 - accuracy: 0.8832\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2836 - accuracy: 0.8892\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2759 - accuracy: 0.8896\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2640 - accuracy: 0.8944\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 18s 119ms/step - loss: 0.2602 - accuracy: 0.8979\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2536 - accuracy: 0.8993\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2451 - accuracy: 0.9033\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2478 - accuracy: 0.9017\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 18s 119ms/step - loss: 0.2431 - accuracy: 0.9017\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2283 - accuracy: 0.9103\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2275 - accuracy: 0.9086\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2267 - accuracy: 0.9099\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2152 - accuracy: 0.9156\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1968 - accuracy: 0.9212\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2021 - accuracy: 0.9223\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 18s 119ms/step - loss: 0.2016 - accuracy: 0.9210\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1929 - accuracy: 0.9237\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1799 - accuracy: 0.9282\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1794 - accuracy: 0.9300\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1797 - accuracy: 0.9299\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1765 - accuracy: 0.9310\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1771 - accuracy: 0.9298\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1657 - accuracy: 0.9343\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1583 - accuracy: 0.9377\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1721 - accuracy: 0.9326\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1591 - accuracy: 0.9385\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1525 - accuracy: 0.9400\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1525 - accuracy: 0.9409\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1554 - accuracy: 0.9384\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 18s 119ms/step - loss: 0.1540 - accuracy: 0.9386\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1446 - accuracy: 0.9427\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1464 - accuracy: 0.9419\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1369 - accuracy: 0.9451\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 18s 119ms/step - loss: 0.1388 - accuracy: 0.9453\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 18s 119ms/step - loss: 0.1340 - accuracy: 0.9480\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1347 - accuracy: 0.9479\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1296 - accuracy: 0.9491\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1322 - accuracy: 0.9483\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 18s 119ms/step - loss: 0.1266 - accuracy: 0.9489\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1279 - accuracy: 0.9503\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1184 - accuracy: 0.9534\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1188 - accuracy: 0.9525\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1196 - accuracy: 0.9514\n",
      "76/76 [==============================] - 3s 41ms/step - loss: 0.3102 - accuracy: 0.9223\n",
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_144 (LSTM)              (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_145 (LSTM)              (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_146 (LSTM)              (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_147 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_36 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_108 (Dense)            (None, 512)               11796992  \n",
      "_________________________________________________________________\n",
      "dropout_72 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_73 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 12,919,046\n",
      "Trainable params: 12,919,046\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128, n_lstm_3 = 128, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.1,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 512, dropout_1 = 0.3,\n",
      "          n_dense_2 = 256, dropout_2 = 0.2, activation_conv=  relu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 18s 117ms/step - loss: 1.2704 - accuracy: 0.4633\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.8917 - accuracy: 0.6224\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.7423 - accuracy: 0.6924\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.6457 - accuracy: 0.7334\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.5755 - accuracy: 0.7661\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.5062 - accuracy: 0.7972\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.4688 - accuracy: 0.8164\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 18s 119ms/step - loss: 0.4340 - accuracy: 0.8295\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.4144 - accuracy: 0.8385\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3855 - accuracy: 0.8497\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3708 - accuracy: 0.8532\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3530 - accuracy: 0.8605\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3377 - accuracy: 0.8673\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3264 - accuracy: 0.8721\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3153 - accuracy: 0.8758\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3025 - accuracy: 0.8803\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2917 - accuracy: 0.8844\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2813 - accuracy: 0.8881\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2738 - accuracy: 0.8896\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2645 - accuracy: 0.8955\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2570 - accuracy: 0.8979\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2499 - accuracy: 0.8996\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2453 - accuracy: 0.9029\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2438 - accuracy: 0.9025\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2314 - accuracy: 0.9060\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2325 - accuracy: 0.9066\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2285 - accuracy: 0.9075\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2167 - accuracy: 0.9120\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2096 - accuracy: 0.9160\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2101 - accuracy: 0.9153\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2042 - accuracy: 0.9193\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1951 - accuracy: 0.9222\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1965 - accuracy: 0.9214\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1821 - accuracy: 0.9284\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1778 - accuracy: 0.9281\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1732 - accuracy: 0.9302\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1747 - accuracy: 0.9301\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1689 - accuracy: 0.9333\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1649 - accuracy: 0.9351\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1637 - accuracy: 0.9373\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1561 - accuracy: 0.9375\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 18s 119ms/step - loss: 0.1543 - accuracy: 0.9384\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1471 - accuracy: 0.9419\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1463 - accuracy: 0.9414\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1476 - accuracy: 0.9415\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1391 - accuracy: 0.9448\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1396 - accuracy: 0.9439\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1412 - accuracy: 0.9447\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1314 - accuracy: 0.9472\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1367 - accuracy: 0.9458\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1324 - accuracy: 0.9463\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1284 - accuracy: 0.9484\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1306 - accuracy: 0.9485\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1258 - accuracy: 0.9496\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1276 - accuracy: 0.9509\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1241 - accuracy: 0.9517\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1225 - accuracy: 0.9508\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1160 - accuracy: 0.9516\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1198 - accuracy: 0.9527\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 18s 119ms/step - loss: 0.1183 - accuracy: 0.9524\n",
      "76/76 [==============================] - 3s 41ms/step - loss: 0.3370 - accuracy: 0.9218\n",
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_148 (LSTM)              (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_149 (LSTM)              (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_150 (LSTM)              (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_151 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_37 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 512)               11796992  \n",
      "_________________________________________________________________\n",
      "dropout_74 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 12,919,046\n",
      "Trainable params: 12,919,046\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128, n_lstm_3 = 128, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.1,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 512, dropout_1 = 0.3,\n",
      "          n_dense_2 = 256, dropout_2 = 0.2, activation_conv=  relu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2/152 [..............................] - ETA: 12s - loss: 1.7868 - accuracy: 0.2305WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0526s vs `on_train_batch_end` time: 0.0837s). Check your callbacks.\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 1.2668 - accuracy: 0.4661\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.8969 - accuracy: 0.6230\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.7448 - accuracy: 0.6906\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.6660 - accuracy: 0.7244\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.5884 - accuracy: 0.7610\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.5275 - accuracy: 0.7883\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.4895 - accuracy: 0.8060\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.4495 - accuracy: 0.8220\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.4343 - accuracy: 0.8303\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.4087 - accuracy: 0.8388\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3865 - accuracy: 0.8486\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3592 - accuracy: 0.8585\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3446 - accuracy: 0.8642\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3336 - accuracy: 0.8694\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3337 - accuracy: 0.8688\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3089 - accuracy: 0.8757\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3040 - accuracy: 0.8780\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2946 - accuracy: 0.8820\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2872 - accuracy: 0.8864\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2739 - accuracy: 0.8938\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2650 - accuracy: 0.8950\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2687 - accuracy: 0.8947\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2525 - accuracy: 0.9015\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2536 - accuracy: 0.9013\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2494 - accuracy: 0.9019\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2411 - accuracy: 0.9073\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2216 - accuracy: 0.9142\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2187 - accuracy: 0.9128\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2134 - accuracy: 0.9167\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 18s 119ms/step - loss: 0.2074 - accuracy: 0.9173\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 18s 119ms/step - loss: 0.2006 - accuracy: 0.9222\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2006 - accuracy: 0.9224\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1964 - accuracy: 0.9250\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1908 - accuracy: 0.9269\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1844 - accuracy: 0.9272\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1842 - accuracy: 0.9290\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1757 - accuracy: 0.9318\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1759 - accuracy: 0.9314\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1662 - accuracy: 0.9366\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1670 - accuracy: 0.9351\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1697 - accuracy: 0.9331\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 18s 119ms/step - loss: 0.1629 - accuracy: 0.9367\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1543 - accuracy: 0.9401\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1518 - accuracy: 0.9409\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1466 - accuracy: 0.9426\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1540 - accuracy: 0.9398\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1480 - accuracy: 0.9417\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1438 - accuracy: 0.9428\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1409 - accuracy: 0.9459\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1335 - accuracy: 0.9474\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1334 - accuracy: 0.9468\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1353 - accuracy: 0.9464\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1290 - accuracy: 0.9492\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1281 - accuracy: 0.9502\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1339 - accuracy: 0.9465\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1230 - accuracy: 0.9508\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1300 - accuracy: 0.9493\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 18s 119ms/step - loss: 0.1288 - accuracy: 0.9496\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1193 - accuracy: 0.9532\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1208 - accuracy: 0.9515\n",
      "76/76 [==============================] - 3s 41ms/step - loss: 0.3228 - accuracy: 0.9171\n",
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_152 (LSTM)              (None, 90, 128)           67584     \n",
      "_________________________________________________________________\n",
      "lstm_153 (LSTM)              (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_154 (LSTM)              (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_155 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_38 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_114 (Dense)            (None, 256)               5898496   \n",
      "_________________________________________________________________\n",
      "dropout_76 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_116 (Dense)            (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 6,690,822\n",
      "Trainable params: 6,690,822\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 128, n_lstm_2 = 128, n_lstm_3 = 128, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.1,\n",
      "          n_dense_1 = 256, dropout_1 = 0.3,\n",
      "          n_dense_2 = 256, dropout_2 = 0.3, activation_conv=  elu,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 14s 91ms/step - loss: 1.2452 - accuracy: 0.4692\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.9582 - accuracy: 0.5873\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.8457 - accuracy: 0.6375\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.7719 - accuracy: 0.6778\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.6931 - accuracy: 0.71391s - l\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.6306 - accuracy: 0.73721s - loss: 0\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.5865 - accuracy: 0.7615\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.5575 - accuracy: 0.7762\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.5079 - accuracy: 0.7966\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.4764 - accuracy: 0.8086\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.4497 - accuracy: 0.8233\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.4324 - accuracy: 0.8289\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.4040 - accuracy: 0.8407\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.3861 - accuracy: 0.8480\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.3721 - accuracy: 0.8538\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.3717 - accuracy: 0.8541\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.3450 - accuracy: 0.8635\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.3337 - accuracy: 0.8697\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.3345 - accuracy: 0.8668\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.3158 - accuracy: 0.8759\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.3067 - accuracy: 0.8776\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2960 - accuracy: 0.8787\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2913 - accuracy: 0.8835\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2877 - accuracy: 0.8847\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2775 - accuracy: 0.8901\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2684 - accuracy: 0.8928\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2651 - accuracy: 0.8944\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2630 - accuracy: 0.8956\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 14s 92ms/step - loss: 0.2549 - accuracy: 0.8979\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 14s 92ms/step - loss: 0.2470 - accuracy: 0.8996\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2491 - accuracy: 0.8994\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2367 - accuracy: 0.9058\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2451 - accuracy: 0.9013\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2386 - accuracy: 0.9053\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2282 - accuracy: 0.9080\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2177 - accuracy: 0.9127\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2245 - accuracy: 0.9094\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2107 - accuracy: 0.9162\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2162 - accuracy: 0.9125\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2043 - accuracy: 0.9179\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2098 - accuracy: 0.9160\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1979 - accuracy: 0.9208\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1974 - accuracy: 0.9196\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1886 - accuracy: 0.9227\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1951 - accuracy: 0.9218\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 14s 92ms/step - loss: 0.1862 - accuracy: 0.9251\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 14s 92ms/step - loss: 0.1861 - accuracy: 0.9266\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1813 - accuracy: 0.9281\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1816 - accuracy: 0.9280\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 14s 92ms/step - loss: 0.1851 - accuracy: 0.9254\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1756 - accuracy: 0.9289\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1746 - accuracy: 0.93062s - loss: 0.172\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1703 - accuracy: 0.9332\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1708 - accuracy: 0.9325\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1725 - accuracy: 0.9310\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1607 - accuracy: 0.9359\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1711 - accuracy: 0.9323\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1589 - accuracy: 0.9353\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1645 - accuracy: 0.9332\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1646 - accuracy: 0.9347\n",
      "76/76 [==============================] - 3s 33ms/step - loss: 0.3350 - accuracy: 0.9020\n",
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_156 (LSTM)              (None, 90, 128)           67584     \n",
      "_________________________________________________________________\n",
      "lstm_157 (LSTM)              (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_158 (LSTM)              (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_159 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_39 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (None, 256)               5898496   \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_118 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_79 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 6,690,822\n",
      "Trainable params: 6,690,822\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 128, n_lstm_2 = 128, n_lstm_3 = 128, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.1,\n",
      "          n_dense_1 = 256, dropout_1 = 0.3,\n",
      "          n_dense_2 = 256, dropout_2 = 0.3, activation_conv=  elu,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 14s 91ms/step - loss: 1.2722 - accuracy: 0.4584\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.9952 - accuracy: 0.5742\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.8514 - accuracy: 0.6355\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.7806 - accuracy: 0.6701\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.7099 - accuracy: 0.7035\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.6513 - accuracy: 0.7337\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.5874 - accuracy: 0.7618\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.5431 - accuracy: 0.7845\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.5143 - accuracy: 0.7949\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.4774 - accuracy: 0.8122\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.4405 - accuracy: 0.8308\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.4229 - accuracy: 0.8348\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.3974 - accuracy: 0.8456\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.3786 - accuracy: 0.8526\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.3632 - accuracy: 0.8582\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.3568 - accuracy: 0.85980s - loss: 0.3568 - \n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.3352 - accuracy: 0.8682\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.3329 - accuracy: 0.8688\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.3173 - accuracy: 0.8751\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.3035 - accuracy: 0.8775\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2968 - accuracy: 0.8819\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2876 - accuracy: 0.8870\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2782 - accuracy: 0.8878\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2806 - accuracy: 0.8873\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2727 - accuracy: 0.8908\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2647 - accuracy: 0.8926\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2683 - accuracy: 0.8897\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2501 - accuracy: 0.8990\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2516 - accuracy: 0.8991\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2399 - accuracy: 0.9024\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2475 - accuracy: 0.9014\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2473 - accuracy: 0.9014\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2333 - accuracy: 0.9059\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 14s 92ms/step - loss: 0.2392 - accuracy: 0.9011\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2292 - accuracy: 0.9077\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2203 - accuracy: 0.9128\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2132 - accuracy: 0.9128\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2053 - accuracy: 0.9178\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2034 - accuracy: 0.9185\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2030 - accuracy: 0.9176\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1952 - accuracy: 0.9211\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1932 - accuracy: 0.9211\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1961 - accuracy: 0.9203\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1945 - accuracy: 0.9214\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1908 - accuracy: 0.9216\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1911 - accuracy: 0.9252\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1813 - accuracy: 0.9285\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1723 - accuracy: 0.9306\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1729 - accuracy: 0.9308\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1788 - accuracy: 0.9282\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 14s 92ms/step - loss: 0.1771 - accuracy: 0.9285\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1679 - accuracy: 0.9339\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1652 - accuracy: 0.9340\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1668 - accuracy: 0.9334\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1625 - accuracy: 0.9351\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 14s 92ms/step - loss: 0.1720 - accuracy: 0.9310\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1565 - accuracy: 0.9372\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1631 - accuracy: 0.9335\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1571 - accuracy: 0.9363\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1601 - accuracy: 0.9358\n",
      "76/76 [==============================] - 3s 33ms/step - loss: 0.3553 - accuracy: 0.8922\n",
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_160 (LSTM)              (None, 90, 128)           67584     \n",
      "_________________________________________________________________\n",
      "lstm_161 (LSTM)              (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_162 (LSTM)              (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_163 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_40 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_120 (Dense)            (None, 256)               5898496   \n",
      "_________________________________________________________________\n",
      "dropout_80 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_121 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_81 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_122 (Dense)            (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 6,690,822\n",
      "Trainable params: 6,690,822\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 128, n_lstm_2 = 128, n_lstm_3 = 128, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.1,\n",
      "          n_dense_1 = 256, dropout_1 = 0.3,\n",
      "          n_dense_2 = 256, dropout_2 = 0.3, activation_conv=  elu,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 14s 91ms/step - loss: 1.2773 - accuracy: 0.4558\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.9665 - accuracy: 0.5828\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.8534 - accuracy: 0.6327\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.7787 - accuracy: 0.6695\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.7075 - accuracy: 0.7044\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.6524 - accuracy: 0.7284\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.6034 - accuracy: 0.7566\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.5576 - accuracy: 0.7753\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.5252 - accuracy: 0.7906\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.4938 - accuracy: 0.8050\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.4672 - accuracy: 0.8186\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.4368 - accuracy: 0.8294\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.4173 - accuracy: 0.8378\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.3996 - accuracy: 0.8452\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.3878 - accuracy: 0.8502\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.3725 - accuracy: 0.8540\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.3594 - accuracy: 0.8587\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.3409 - accuracy: 0.8655\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.3280 - accuracy: 0.8720\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.3197 - accuracy: 0.8741\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.3117 - accuracy: 0.8762\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.3063 - accuracy: 0.8774\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2946 - accuracy: 0.8836\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2902 - accuracy: 0.8846\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2797 - accuracy: 0.8893\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2772 - accuracy: 0.8892\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2693 - accuracy: 0.8910\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2643 - accuracy: 0.8938\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2518 - accuracy: 0.8995\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2521 - accuracy: 0.9000\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2479 - accuracy: 0.9004\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2472 - accuracy: 0.9000\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2356 - accuracy: 0.9063\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 14s 92ms/step - loss: 0.2376 - accuracy: 0.9054\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2275 - accuracy: 0.9097\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2250 - accuracy: 0.9110\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2162 - accuracy: 0.9139\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2172 - accuracy: 0.9133\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2107 - accuracy: 0.9158\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2160 - accuracy: 0.9136\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2017 - accuracy: 0.9190\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2001 - accuracy: 0.9211\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2042 - accuracy: 0.9189\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.2075 - accuracy: 0.9178\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1925 - accuracy: 0.9213\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1873 - accuracy: 0.9258\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1868 - accuracy: 0.9253\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1971 - accuracy: 0.9222\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1856 - accuracy: 0.9258\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1778 - accuracy: 0.9299\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1845 - accuracy: 0.9264\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1749 - accuracy: 0.9304\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1750 - accuracy: 0.9281\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1733 - accuracy: 0.9309\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1728 - accuracy: 0.9306\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1684 - accuracy: 0.93290s - loss: 0.1684 - accuracy: 0.93\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1656 - accuracy: 0.9333\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1721 - accuracy: 0.9311\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1774 - accuracy: 0.9290\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 14s 91ms/step - loss: 0.1681 - accuracy: 0.9335\n",
      "76/76 [==============================] - 3s 33ms/step - loss: 0.4094 - accuracy: 0.8735\n",
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_164 (LSTM)              (None, 90, 128)           67584     \n",
      "_________________________________________________________________\n",
      "lstm_165 (LSTM)              (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_166 (LSTM)              (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_167 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_41 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_123 (Dense)            (None, 384)               8847744   \n",
      "_________________________________________________________________\n",
      "dropout_82 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_124 (Dense)            (None, 512)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_83 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_125 (Dense)            (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 9,772,934\n",
      "Trainable params: 9,772,934\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 128, n_lstm_2 = 128, n_lstm_3 = 128, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.1,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 384, dropout_1 = 0.3,\n",
      "          n_dense_2 = 512, dropout_2 = 0.2, activation_conv=  elu,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 14s 93ms/step - loss: 1.2647 - accuracy: 0.4630\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.9572 - accuracy: 0.5869\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.8399 - accuracy: 0.6399\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.7749 - accuracy: 0.6739\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.6861 - accuracy: 0.7162\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 14s 93ms/step - loss: 0.6145 - accuracy: 0.7501\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 14s 93ms/step - loss: 0.5615 - accuracy: 0.7749\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.5347 - accuracy: 0.7893\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.4811 - accuracy: 0.8115\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 14s 93ms/step - loss: 0.4520 - accuracy: 0.8228\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.4207 - accuracy: 0.8365\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.4130 - accuracy: 0.8376\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.3891 - accuracy: 0.8471\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.3673 - accuracy: 0.8568\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 14s 93ms/step - loss: 0.3573 - accuracy: 0.8576\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - ETA: 0s - loss: 0.3431 - accuracy: 0.86 - 14s 94ms/step - loss: 0.3431 - accuracy: 0.8630\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.3284 - accuracy: 0.8693\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.3179 - accuracy: 0.8735\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.3077 - accuracy: 0.8771\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2969 - accuracy: 0.8819\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2903 - accuracy: 0.8806\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2824 - accuracy: 0.8881\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2757 - accuracy: 0.8910\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2601 - accuracy: 0.8978\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2606 - accuracy: 0.8956\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2558 - accuracy: 0.8970\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2507 - accuracy: 0.8994\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2470 - accuracy: 0.9006\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2339 - accuracy: 0.9070\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2342 - accuracy: 0.9069\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2240 - accuracy: 0.9088\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2300 - accuracy: 0.9089\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2250 - accuracy: 0.9107\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2171 - accuracy: 0.9145\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2117 - accuracy: 0.91492s - ETA: 0s - loss: 0.2114 - \n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2144 - accuracy: 0.9149\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2082 - accuracy: 0.9166\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2181 - accuracy: 0.9117\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2095 - accuracy: 0.9154\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1972 - accuracy: 0.9207\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1930 - accuracy: 0.9227\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1900 - accuracy: 0.9249\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1972 - accuracy: 0.9201\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1844 - accuracy: 0.9263\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1820 - accuracy: 0.9252\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1769 - accuracy: 0.9286\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1793 - accuracy: 0.9293\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1759 - accuracy: 0.9284\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1765 - accuracy: 0.9298\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1733 - accuracy: 0.9313\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1709 - accuracy: 0.9314\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1578 - accuracy: 0.9378\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1676 - accuracy: 0.9343\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1664 - accuracy: 0.9350\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1649 - accuracy: 0.9340\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1564 - accuracy: 0.9365\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1620 - accuracy: 0.93540s - loss: 0.1632 - accu - ETA: 0s - loss: 0.1621 - accuracy: \n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1532 - accuracy: 0.9377\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1532 - accuracy: 0.9397\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1571 - accuracy: 0.9375\n",
      "76/76 [==============================] - 3s 34ms/step - loss: 0.3290 - accuracy: 0.8935\n",
      "Model: \"sequential_42\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_168 (LSTM)              (None, 90, 128)           67584     \n",
      "_________________________________________________________________\n",
      "lstm_169 (LSTM)              (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_170 (LSTM)              (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_171 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_42 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_126 (Dense)            (None, 384)               8847744   \n",
      "_________________________________________________________________\n",
      "dropout_84 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_127 (Dense)            (None, 512)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_85 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_128 (Dense)            (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 9,772,934\n",
      "Trainable params: 9,772,934\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 128, n_lstm_2 = 128, n_lstm_3 = 128, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.1,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 384, dropout_1 = 0.3,\n",
      "          n_dense_2 = 512, dropout_2 = 0.2, activation_conv=  elu,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 14s 93ms/step - loss: 1.2557 - accuracy: 0.4656\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 14s 93ms/step - loss: 0.9583 - accuracy: 0.5843\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.8422 - accuracy: 0.6361\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 14s 93ms/step - loss: 0.7500 - accuracy: 0.6809\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.6787 - accuracy: 0.7189\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.6092 - accuracy: 0.74951s - los\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.5547 - accuracy: 0.7781\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.5062 - accuracy: 0.7983\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.4748 - accuracy: 0.8125\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.4394 - accuracy: 0.8291\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.4190 - accuracy: 0.8380\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.4000 - accuracy: 0.8428\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.3739 - accuracy: 0.8543\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.3662 - accuracy: 0.85650s - loss: 0.367\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.3409 - accuracy: 0.8673\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.3383 - accuracy: 0.8646\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.3210 - accuracy: 0.8739\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.3096 - accuracy: 0.8756\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2965 - accuracy: 0.8825\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2921 - accuracy: 0.8827\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2804 - accuracy: 0.8870\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2737 - accuracy: 0.8909\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2676 - accuracy: 0.8930\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2580 - accuracy: 0.8947\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2604 - accuracy: 0.8949\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2463 - accuracy: 0.9011\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2459 - accuracy: 0.9006\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2351 - accuracy: 0.9053\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2335 - accuracy: 0.9056\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2294 - accuracy: 0.9077\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2215 - accuracy: 0.9084\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2180 - accuracy: 0.9113\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2100 - accuracy: 0.9143\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2088 - accuracy: 0.9162\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1985 - accuracy: 0.9200\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2006 - accuracy: 0.9171\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1991 - accuracy: 0.9204\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1977 - accuracy: 0.9193\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1943 - accuracy: 0.9220\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1851 - accuracy: 0.9240\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1857 - accuracy: 0.9234\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1801 - accuracy: 0.9250\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1843 - accuracy: 0.9256\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1727 - accuracy: 0.9288\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1732 - accuracy: 0.9274\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1808 - accuracy: 0.9262\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1745 - accuracy: 0.9288\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1708 - accuracy: 0.9317\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1700 - accuracy: 0.9304\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1626 - accuracy: 0.9339\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1695 - accuracy: 0.9313\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1653 - accuracy: 0.9332\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1543 - accuracy: 0.9373\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1525 - accuracy: 0.9375\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1585 - accuracy: 0.9363\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1574 - accuracy: 0.9364\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1600 - accuracy: 0.9343\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1529 - accuracy: 0.9374\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1536 - accuracy: 0.9362\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1493 - accuracy: 0.9401\n",
      "76/76 [==============================] - 3s 34ms/step - loss: 0.3901 - accuracy: 0.8859 0s -\n",
      "Model: \"sequential_43\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_172 (LSTM)              (None, 90, 128)           67584     \n",
      "_________________________________________________________________\n",
      "lstm_173 (LSTM)              (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_174 (LSTM)              (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_175 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_43 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_129 (Dense)            (None, 384)               8847744   \n",
      "_________________________________________________________________\n",
      "dropout_86 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_130 (Dense)            (None, 512)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_87 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_131 (Dense)            (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 9,772,934\n",
      "Trainable params: 9,772,934\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 128, n_lstm_2 = 128, n_lstm_3 = 128, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.1,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 384, dropout_1 = 0.3,\n",
      "          n_dense_2 = 512, dropout_2 = 0.2, activation_conv=  elu,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 14s 93ms/step - loss: 1.2733 - accuracy: 0.4578\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 14s 93ms/step - loss: 0.9654 - accuracy: 0.5837\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.8474 - accuracy: 0.63751s - loss:\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 14s 93ms/step - loss: 0.7573 - accuracy: 0.68030s - loss: 0.7595 - accura\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 14s 93ms/step - loss: 0.6784 - accuracy: 0.7165\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.6212 - accuracy: 0.7449\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 14s 93ms/step - loss: 0.5660 - accuracy: 0.7723\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.5133 - accuracy: 0.7932\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.4805 - accuracy: 0.8077\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.4545 - accuracy: 0.8181\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.4356 - accuracy: 0.8265\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.3990 - accuracy: 0.8409\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.3913 - accuracy: 0.8453\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.3623 - accuracy: 0.8571\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.3573 - accuracy: 0.8585\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.3319 - accuracy: 0.8666\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.3298 - accuracy: 0.8667\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.3184 - accuracy: 0.8714\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.3154 - accuracy: 0.8740\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2979 - accuracy: 0.8800\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2937 - accuracy: 0.8818\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2863 - accuracy: 0.8847\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2730 - accuracy: 0.8899\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2638 - accuracy: 0.8927\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2633 - accuracy: 0.8931\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2510 - accuracy: 0.8987\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2486 - accuracy: 0.8984\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2523 - accuracy: 0.8977\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2341 - accuracy: 0.9074\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2334 - accuracy: 0.9040\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2348 - accuracy: 0.9033\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2303 - accuracy: 0.9066\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2183 - accuracy: 0.9124\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2159 - accuracy: 0.9119\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2101 - accuracy: 0.9152\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2089 - accuracy: 0.9142\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2083 - accuracy: 0.9168\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.2029 - accuracy: 0.9174\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1962 - accuracy: 0.9207\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1956 - accuracy: 0.9219\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1933 - accuracy: 0.9207\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1902 - accuracy: 0.9214\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1937 - accuracy: 0.9223\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1874 - accuracy: 0.9236\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1878 - accuracy: 0.9240\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1826 - accuracy: 0.9266\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1739 - accuracy: 0.9290\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1734 - accuracy: 0.9283\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1711 - accuracy: 0.9310\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1773 - accuracy: 0.9290\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1878 - accuracy: 0.9238\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1721 - accuracy: 0.9314\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1654 - accuracy: 0.9335\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1626 - accuracy: 0.9335\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1686 - accuracy: 0.9308\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1610 - accuracy: 0.9336\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1576 - accuracy: 0.9352\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1569 - accuracy: 0.9354\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1563 - accuracy: 0.9362\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 14s 94ms/step - loss: 0.1620 - accuracy: 0.9327\n",
      "76/76 [==============================] - 3s 34ms/step - loss: 0.3540 - accuracy: 0.8887\n",
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_176 (LSTM)              (None, 90, 128)           67584     \n",
      "_________________________________________________________________\n",
      "lstm_177 (LSTM)              (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_178 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_179 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_44 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_132 (Dense)            (None, 512)               11796992  \n",
      "_________________________________________________________________\n",
      "dropout_88 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_133 (Dense)            (None, 384)               196992    \n",
      "_________________________________________________________________\n",
      "dropout_89 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_134 (Dense)            (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 13,115,014\n",
      "Trainable params: 13,115,014\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 128, n_lstm_2 = 128, n_lstm_3 = 256, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 512, dropout_1 = 0.3,\n",
      "          n_dense_2 = 384, dropout_2 = 0.2, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510688>,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 19s 122ms/step - loss: 1.2502 - accuracy: 0.4642\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.9575 - accuracy: 0.5868\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.8218 - accuracy: 0.6502\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.7228 - accuracy: 0.6992\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.6537 - accuracy: 0.7347\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.5931 - accuracy: 0.7602\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.5433 - accuracy: 0.7810\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.5080 - accuracy: 0.7992\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.4639 - accuracy: 0.8158\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.4358 - accuracy: 0.8271\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.4056 - accuracy: 0.8390\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3861 - accuracy: 0.8482\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3710 - accuracy: 0.8520\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3513 - accuracy: 0.8600\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3501 - accuracy: 0.8603\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3295 - accuracy: 0.8656\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3226 - accuracy: 0.8682\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3056 - accuracy: 0.8756\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2997 - accuracy: 0.8806\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2917 - accuracy: 0.8799\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2868 - accuracy: 0.8821\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2796 - accuracy: 0.8871\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2657 - accuracy: 0.8904\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2681 - accuracy: 0.8899\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2561 - accuracy: 0.8966\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2585 - accuracy: 0.8934\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2438 - accuracy: 0.8997\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2492 - accuracy: 0.8981\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2358 - accuracy: 0.9036\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.2361 - accuracy: 0.9014\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2350 - accuracy: 0.9040\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2202 - accuracy: 0.9086\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.2199 - accuracy: 0.9116\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.2174 - accuracy: 0.9114\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2165 - accuracy: 0.9114\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2131 - accuracy: 0.9131\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2093 - accuracy: 0.9129\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2111 - accuracy: 0.9153\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2000 - accuracy: 0.9181\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2070 - accuracy: 0.9130\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.1876 - accuracy: 0.9236\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1932 - accuracy: 0.9218\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1917 - accuracy: 0.9211\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1840 - accuracy: 0.9245\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.1804 - accuracy: 0.9275\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1823 - accuracy: 0.9254\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1729 - accuracy: 0.9311\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1745 - accuracy: 0.9278\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1772 - accuracy: 0.9274\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1659 - accuracy: 0.9317\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1618 - accuracy: 0.9334\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1663 - accuracy: 0.9334\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.1710 - accuracy: 0.9314\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1641 - accuracy: 0.9332\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1544 - accuracy: 0.9378\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1662 - accuracy: 0.9331\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1585 - accuracy: 0.9363\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1548 - accuracy: 0.9386\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.1593 - accuracy: 0.9368\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1537 - accuracy: 0.9387\n",
      "76/76 [==============================] - 3s 44ms/step - loss: 0.3715 - accuracy: 0.8918\n",
      "Model: \"sequential_45\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_180 (LSTM)              (None, 90, 128)           67584     \n",
      "_________________________________________________________________\n",
      "lstm_181 (LSTM)              (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_182 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_183 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_45 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_135 (Dense)            (None, 512)               11796992  \n",
      "_________________________________________________________________\n",
      "dropout_90 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_136 (Dense)            (None, 384)               196992    \n",
      "_________________________________________________________________\n",
      "dropout_91 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_137 (Dense)            (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 13,115,014\n",
      "Trainable params: 13,115,014\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 128, n_lstm_2 = 128, n_lstm_3 = 256, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 512, dropout_1 = 0.3,\n",
      "          n_dense_2 = 384, dropout_2 = 0.2, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510688>,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 19s 122ms/step - loss: 1.2891 - accuracy: 0.4534\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.9835 - accuracy: 0.5701\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.8569 - accuracy: 0.6281\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.7694 - accuracy: 0.6726\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.6979 - accuracy: 0.7098\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.6268 - accuracy: 0.7444\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.5663 - accuracy: 0.7713\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.5242 - accuracy: 0.7877\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.4799 - accuracy: 0.8077\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.4453 - accuracy: 0.8244\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.4173 - accuracy: 0.8364\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3906 - accuracy: 0.8477\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3720 - accuracy: 0.8522\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3546 - accuracy: 0.8586\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3392 - accuracy: 0.8669\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3419 - accuracy: 0.8646\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3187 - accuracy: 0.8707\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3073 - accuracy: 0.8781\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3028 - accuracy: 0.8785\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2874 - accuracy: 0.8821\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2882 - accuracy: 0.8832\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2739 - accuracy: 0.8886\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2654 - accuracy: 0.8924\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2587 - accuracy: 0.8940\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.2545 - accuracy: 0.8958\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2569 - accuracy: 0.8973\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2438 - accuracy: 0.8994\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2385 - accuracy: 0.9014\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2366 - accuracy: 0.9017\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.2238 - accuracy: 0.9098\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2203 - accuracy: 0.9093\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2218 - accuracy: 0.9098\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2123 - accuracy: 0.9149\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.2064 - accuracy: 0.9137\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2039 - accuracy: 0.9165\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2042 - accuracy: 0.9165\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1953 - accuracy: 0.9201\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2029 - accuracy: 0.9170\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1889 - accuracy: 0.9220\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1957 - accuracy: 0.9187\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.1913 - accuracy: 0.9217\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1889 - accuracy: 0.9240\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1825 - accuracy: 0.9254\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1746 - accuracy: 0.9293\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.1769 - accuracy: 0.9278\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1767 - accuracy: 0.9285\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1729 - accuracy: 0.9310\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.1703 - accuracy: 0.9308\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1688 - accuracy: 0.9308\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.1717 - accuracy: 0.9318\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1634 - accuracy: 0.9337\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.1677 - accuracy: 0.9315\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.1621 - accuracy: 0.9342\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1596 - accuracy: 0.9352\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1598 - accuracy: 0.9366\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1563 - accuracy: 0.9377\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1667 - accuracy: 0.9344\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1517 - accuracy: 0.9385\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1509 - accuracy: 0.9397\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.1414 - accuracy: 0.9426\n",
      "76/76 [==============================] - 3s 44ms/step - loss: 0.3761 - accuracy: 0.9021\n",
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_184 (LSTM)              (None, 90, 128)           67584     \n",
      "_________________________________________________________________\n",
      "lstm_185 (LSTM)              (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_186 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_187 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_46 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_138 (Dense)            (None, 512)               11796992  \n",
      "_________________________________________________________________\n",
      "dropout_92 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_139 (Dense)            (None, 384)               196992    \n",
      "_________________________________________________________________\n",
      "dropout_93 (Dropout)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_140 (Dense)            (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 13,115,014\n",
      "Trainable params: 13,115,014\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 128, n_lstm_2 = 128, n_lstm_3 = 256, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 512, dropout_1 = 0.3,\n",
      "          n_dense_2 = 384, dropout_2 = 0.2, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510688>,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 19s 122ms/step - loss: 1.2747 - accuracy: 0.4571\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.9813 - accuracy: 0.5786\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.8372 - accuracy: 0.6417\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.7492 - accuracy: 0.6870\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.6693 - accuracy: 0.7261\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.5981 - accuracy: 0.7605\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.5538 - accuracy: 0.7801\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.5035 - accuracy: 0.7992\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.4761 - accuracy: 0.8128\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.4466 - accuracy: 0.8241\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.4232 - accuracy: 0.8325\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.4033 - accuracy: 0.8446\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3835 - accuracy: 0.8487\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3686 - accuracy: 0.8536\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3489 - accuracy: 0.8627\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3374 - accuracy: 0.8659\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3393 - accuracy: 0.8647\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3163 - accuracy: 0.8734\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3115 - accuracy: 0.8741\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.3008 - accuracy: 0.8811\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.2914 - accuracy: 0.8838\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2906 - accuracy: 0.8844\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.2761 - accuracy: 0.8894\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2670 - accuracy: 0.8923\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2677 - accuracy: 0.8926\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.2502 - accuracy: 0.8999\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2489 - accuracy: 0.8994\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.2521 - accuracy: 0.8973\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2308 - accuracy: 0.9055\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.2309 - accuracy: 0.9069\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.2253 - accuracy: 0.9092\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2392 - accuracy: 0.9040\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.2163 - accuracy: 0.9131\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.2132 - accuracy: 0.9134\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2105 - accuracy: 0.9150\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2071 - accuracy: 0.9174\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.2035 - accuracy: 0.9173\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2066 - accuracy: 0.9165\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1948 - accuracy: 0.9200\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.2097 - accuracy: 0.9151\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1958 - accuracy: 0.9214\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1840 - accuracy: 0.9260\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.1835 - accuracy: 0.9267\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1836 - accuracy: 0.9253\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.1795 - accuracy: 0.9281\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.1757 - accuracy: 0.9297\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.1797 - accuracy: 0.9295\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1690 - accuracy: 0.9325\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.1642 - accuracy: 0.9343\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.1705 - accuracy: 0.9301\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1624 - accuracy: 0.9362\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.1670 - accuracy: 0.9327\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.1649 - accuracy: 0.9337\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1561 - accuracy: 0.9365\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1626 - accuracy: 0.9365\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.1553 - accuracy: 0.9375\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.1519 - accuracy: 0.9385\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1519 - accuracy: 0.9388\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 19s 124ms/step - loss: 0.1522 - accuracy: 0.9399\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 19s 123ms/step - loss: 0.1463 - accuracy: 0.9412\n",
      "76/76 [==============================] - 3s 44ms/step - loss: 0.3609 - accuracy: 0.8983\n",
      "Model: \"sequential_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_188 (LSTM)              (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_189 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_190 (LSTM)              (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_191 (LSTM)              (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_47 (Flatten)         (None, 11520)             0         \n",
      "_________________________________________________________________\n",
      "dense_141 (Dense)            (None, 256)               2949376   \n",
      "_________________________________________________________________\n",
      "dropout_94 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_142 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_95 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_143 (Dense)            (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 4,204,294\n",
      "Trainable params: 4,204,294\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256, n_lstm_3 = 128, n_lstm_4 = 128,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 512, dropout_2 = 0.2, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510688>,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 18s 120ms/step - loss: 1.2590 - accuracy: 0.4662\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.8929 - accuracy: 0.6191\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.7793 - accuracy: 0.6750\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.6969 - accuracy: 0.7053\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.6270 - accuracy: 0.7403\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.5613 - accuracy: 0.7694\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.4991 - accuracy: 0.7997\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.4704 - accuracy: 0.8122\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.4387 - accuracy: 0.8247\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.4012 - accuracy: 0.8409\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.3734 - accuracy: 0.8530\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.3525 - accuracy: 0.8606\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.3419 - accuracy: 0.8651\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.3278 - accuracy: 0.8698\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.3100 - accuracy: 0.8748\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.3011 - accuracy: 0.8796\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2946 - accuracy: 0.8827\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2802 - accuracy: 0.8885\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2798 - accuracy: 0.8887\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2770 - accuracy: 0.8885\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2648 - accuracy: 0.8918\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2568 - accuracy: 0.8943\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2552 - accuracy: 0.8970\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2387 - accuracy: 0.9022\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2395 - accuracy: 0.9041\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2299 - accuracy: 0.9065\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2183 - accuracy: 0.9107\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2092 - accuracy: 0.9139\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2140 - accuracy: 0.9143\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1972 - accuracy: 0.9211\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2011 - accuracy: 0.9189\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1894 - accuracy: 0.9237\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1923 - accuracy: 0.9225\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1782 - accuracy: 0.9289\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1775 - accuracy: 0.9288\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1828 - accuracy: 0.9273\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1670 - accuracy: 0.9327\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1680 - accuracy: 0.9338\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1653 - accuracy: 0.9337\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1696 - accuracy: 0.9311\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1804 - accuracy: 0.9254\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1654 - accuracy: 0.9320\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1534 - accuracy: 0.9380\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1515 - accuracy: 0.9397\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1434 - accuracy: 0.9425\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1471 - accuracy: 0.9415\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1481 - accuracy: 0.9417\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1380 - accuracy: 0.9440\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1396 - accuracy: 0.9444\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1342 - accuracy: 0.9465\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1270 - accuracy: 0.9488\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1377 - accuracy: 0.9457\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1249 - accuracy: 0.9506\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1360 - accuracy: 0.9469\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1268 - accuracy: 0.9489\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1294 - accuracy: 0.9482\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1304 - accuracy: 0.9483\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1200 - accuracy: 0.9530\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1143 - accuracy: 0.9536\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1200 - accuracy: 0.9522\n",
      "76/76 [==============================] - 3s 43ms/step - loss: 0.3860 - accuracy: 0.8971\n",
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_192 (LSTM)              (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_193 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_194 (LSTM)              (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_195 (LSTM)              (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_48 (Flatten)         (None, 11520)             0         \n",
      "_________________________________________________________________\n",
      "dense_144 (Dense)            (None, 256)               2949376   \n",
      "_________________________________________________________________\n",
      "dropout_96 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_145 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_97 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_146 (Dense)            (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 4,204,294\n",
      "Trainable params: 4,204,294\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256, n_lstm_3 = 128, n_lstm_4 = 128,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 512, dropout_2 = 0.2, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510688>,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 18s 120ms/step - loss: 1.2641 - accuracy: 0.4655\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.9162 - accuracy: 0.6067\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.8048 - accuracy: 0.6571\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.6943 - accuracy: 0.7080\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.6194 - accuracy: 0.7425\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.5469 - accuracy: 0.7765\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.4921 - accuracy: 0.8023\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.4625 - accuracy: 0.8135\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.4208 - accuracy: 0.8330\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.3999 - accuracy: 0.8376\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.3724 - accuracy: 0.8524\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.3524 - accuracy: 0.8590\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.3382 - accuracy: 0.8641\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.3256 - accuracy: 0.8701\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.3140 - accuracy: 0.8734\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.3007 - accuracy: 0.8777\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2916 - accuracy: 0.8817\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2794 - accuracy: 0.8854\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2765 - accuracy: 0.8867\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2670 - accuracy: 0.8919\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2599 - accuracy: 0.8932\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2509 - accuracy: 0.8969\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2426 - accuracy: 0.9013\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2378 - accuracy: 0.9038\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2316 - accuracy: 0.9050\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2198 - accuracy: 0.9105\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2152 - accuracy: 0.9118\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2279 - accuracy: 0.9045\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2057 - accuracy: 0.9167\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2024 - accuracy: 0.9206\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1981 - accuracy: 0.9211\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1937 - accuracy: 0.9231\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1769 - accuracy: 0.9291\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1827 - accuracy: 0.9263\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1689 - accuracy: 0.9335\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1665 - accuracy: 0.9321\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1700 - accuracy: 0.9313\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1621 - accuracy: 0.9344\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1616 - accuracy: 0.9361\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1557 - accuracy: 0.9376\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1529 - accuracy: 0.9378\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1522 - accuracy: 0.9368\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1485 - accuracy: 0.9409\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1452 - accuracy: 0.9406\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1521 - accuracy: 0.9398\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1552 - accuracy: 0.9372\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1379 - accuracy: 0.9441\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1383 - accuracy: 0.9455\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1306 - accuracy: 0.9470\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1384 - accuracy: 0.9445\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1278 - accuracy: 0.9465\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1265 - accuracy: 0.9491\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1288 - accuracy: 0.9482\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1202 - accuracy: 0.9523\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1189 - accuracy: 0.9518\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1202 - accuracy: 0.9507\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1281 - accuracy: 0.9485\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1188 - accuracy: 0.9525\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1140 - accuracy: 0.9537\n",
      "76/76 [==============================] - 3s 43ms/step - loss: 0.5175 - accuracy: 0.8410\n",
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_196 (LSTM)              (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_197 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_198 (LSTM)              (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_199 (LSTM)              (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_49 (Flatten)         (None, 11520)             0         \n",
      "_________________________________________________________________\n",
      "dense_147 (Dense)            (None, 256)               2949376   \n",
      "_________________________________________________________________\n",
      "dropout_98 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_148 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_99 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_149 (Dense)            (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 4,204,294\n",
      "Trainable params: 4,204,294\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256, n_lstm_3 = 128, n_lstm_4 = 128,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 512, dropout_2 = 0.2, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510688>,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 18s 120ms/step - loss: 1.2529 - accuracy: 0.4724\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.9036 - accuracy: 0.6137\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.7906 - accuracy: 0.6651\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.7141 - accuracy: 0.6982\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.6387 - accuracy: 0.7380\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.5753 - accuracy: 0.7689\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.5362 - accuracy: 0.7870\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.4781 - accuracy: 0.8113\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.4425 - accuracy: 0.8273\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.4106 - accuracy: 0.8403\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.3911 - accuracy: 0.8466\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.3568 - accuracy: 0.8596\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.3468 - accuracy: 0.8647\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.3284 - accuracy: 0.8710\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.3230 - accuracy: 0.8752\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.3112 - accuracy: 0.8755\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2978 - accuracy: 0.8817\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2871 - accuracy: 0.8845\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2740 - accuracy: 0.8910\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2678 - accuracy: 0.8912\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2788 - accuracy: 0.8886\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2582 - accuracy: 0.8966\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2526 - accuracy: 0.8988\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2406 - accuracy: 0.9043\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2380 - accuracy: 0.9044\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2291 - accuracy: 0.9093\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2196 - accuracy: 0.9128\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2211 - accuracy: 0.9134\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2122 - accuracy: 0.9156\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.2084 - accuracy: 0.9173\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1982 - accuracy: 0.9215\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1942 - accuracy: 0.9228\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1908 - accuracy: 0.9236\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1930 - accuracy: 0.9247\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1832 - accuracy: 0.9283\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1802 - accuracy: 0.9285\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1770 - accuracy: 0.9299\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1762 - accuracy: 0.9296\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1719 - accuracy: 0.9341\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1626 - accuracy: 0.9361\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1559 - accuracy: 0.9374\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1537 - accuracy: 0.9390\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1524 - accuracy: 0.9409\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1530 - accuracy: 0.9406\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1448 - accuracy: 0.9431\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1444 - accuracy: 0.9430\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1454 - accuracy: 0.9432\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1398 - accuracy: 0.9456\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1331 - accuracy: 0.9472\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1347 - accuracy: 0.9463\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1371 - accuracy: 0.9450\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1386 - accuracy: 0.9450\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1315 - accuracy: 0.9473\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1327 - accuracy: 0.9471\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1299 - accuracy: 0.9485\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1246 - accuracy: 0.9497\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1191 - accuracy: 0.9521\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1196 - accuracy: 0.9524\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1218 - accuracy: 0.9506\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 18s 120ms/step - loss: 0.1182 - accuracy: 0.9532\n",
      "76/76 [==============================] - 3s 43ms/step - loss: 0.4813 - accuracy: 0.8675\n",
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_200 (LSTM)              (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_201 (LSTM)              (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_202 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_203 (LSTM)              (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "flatten_50 (Flatten)         (None, 11520)             0         \n",
      "_________________________________________________________________\n",
      "dense_150 (Dense)            (None, 256)               2949376   \n",
      "_________________________________________________________________\n",
      "dropout_100 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_151 (Dense)            (None, 384)               98688     \n",
      "_________________________________________________________________\n",
      "dropout_101 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_152 (Dense)            (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 4,105,094\n",
      "Trainable params: 4,105,094\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128, n_lstm_3 = 256, n_lstm_4 = 128,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.1,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  elu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 18s 117ms/step - loss: 1.2438 - accuracy: 0.4738\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.8395 - accuracy: 0.6472\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.7028 - accuracy: 0.7115\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.6358 - accuracy: 0.7383\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.5668 - accuracy: 0.7672\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.5095 - accuracy: 0.7937\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.4671 - accuracy: 0.8147\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.4409 - accuracy: 0.8264\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.4091 - accuracy: 0.8392\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.3870 - accuracy: 0.8497\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3678 - accuracy: 0.8557\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3562 - accuracy: 0.8590\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3456 - accuracy: 0.8633\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3203 - accuracy: 0.8729\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3102 - accuracy: 0.8768\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.2975 - accuracy: 0.8827\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2852 - accuracy: 0.8865\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2739 - accuracy: 0.8927\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2753 - accuracy: 0.8909\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2625 - accuracy: 0.8960\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2555 - accuracy: 0.8979\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2444 - accuracy: 0.9043\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.2308 - accuracy: 0.9084\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2330 - accuracy: 0.9078\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2228 - accuracy: 0.9118\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.2120 - accuracy: 0.9157\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2047 - accuracy: 0.9184\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2035 - accuracy: 0.9198\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1949 - accuracy: 0.9234\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1836 - accuracy: 0.9285\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1840 - accuracy: 0.9296\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1829 - accuracy: 0.9256\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1735 - accuracy: 0.9312\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1712 - accuracy: 0.9330\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1604 - accuracy: 0.9364\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1731 - accuracy: 0.9313\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1711 - accuracy: 0.9338\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1566 - accuracy: 0.9377\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1505 - accuracy: 0.9395\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.1481 - accuracy: 0.9405\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1486 - accuracy: 0.9416\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1426 - accuracy: 0.9436\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1479 - accuracy: 0.9407\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1470 - accuracy: 0.9424\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1339 - accuracy: 0.9463\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1360 - accuracy: 0.9458\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1250 - accuracy: 0.9493\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1379 - accuracy: 0.9449\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1371 - accuracy: 0.9460\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1237 - accuracy: 0.9502\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1223 - accuracy: 0.9515\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1254 - accuracy: 0.9496\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1180 - accuracy: 0.9537\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1250 - accuracy: 0.9499\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1186 - accuracy: 0.9529\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1213 - accuracy: 0.9516\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1123 - accuracy: 0.9539\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1186 - accuracy: 0.9533\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1069 - accuracy: 0.9570\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1133 - accuracy: 0.9556\n",
      "76/76 [==============================] - 3s 41ms/step - loss: 0.2821 - accuracy: 0.9260\n",
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_204 (LSTM)              (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_205 (LSTM)              (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_206 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_207 (LSTM)              (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "flatten_51 (Flatten)         (None, 11520)             0         \n",
      "_________________________________________________________________\n",
      "dense_153 (Dense)            (None, 256)               2949376   \n",
      "_________________________________________________________________\n",
      "dropout_102 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_154 (Dense)            (None, 384)               98688     \n",
      "_________________________________________________________________\n",
      "dropout_103 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_155 (Dense)            (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 4,105,094\n",
      "Trainable params: 4,105,094\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128, n_lstm_3 = 256, n_lstm_4 = 128,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.1,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  elu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 18s 117ms/step - loss: 1.2362 - accuracy: 0.4798\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.8631 - accuracy: 0.6398\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.7403 - accuracy: 0.6926\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.6417 - accuracy: 0.7341\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.5820 - accuracy: 0.7637\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.5251 - accuracy: 0.7889\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.4742 - accuracy: 0.8081\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.4447 - accuracy: 0.8235\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.4112 - accuracy: 0.8374\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3969 - accuracy: 0.8445\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3654 - accuracy: 0.8576\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3513 - accuracy: 0.8611\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.3386 - accuracy: 0.8679\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.3259 - accuracy: 0.8718\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3164 - accuracy: 0.8762\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3056 - accuracy: 0.8795\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2950 - accuracy: 0.8828\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2873 - accuracy: 0.8865\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2732 - accuracy: 0.8901\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.2625 - accuracy: 0.8949\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.2644 - accuracy: 0.8933\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2511 - accuracy: 0.9002\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2502 - accuracy: 0.9006\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.2340 - accuracy: 0.9060\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.2289 - accuracy: 0.9087\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.2261 - accuracy: 0.9116\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.2242 - accuracy: 0.9113\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.2176 - accuracy: 0.9128\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2042 - accuracy: 0.9215\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.2020 - accuracy: 0.9204\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2008 - accuracy: 0.9206\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1909 - accuracy: 0.9251\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1853 - accuracy: 0.9275\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.1808 - accuracy: 0.9289\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1739 - accuracy: 0.9304\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1697 - accuracy: 0.9349\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.1699 - accuracy: 0.9329\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1656 - accuracy: 0.9353\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1630 - accuracy: 0.9357\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1547 - accuracy: 0.9390\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1461 - accuracy: 0.9426\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.1468 - accuracy: 0.9404\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1504 - accuracy: 0.9406\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.1555 - accuracy: 0.9394\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1513 - accuracy: 0.9408\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1365 - accuracy: 0.9443\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.1439 - accuracy: 0.9419\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.1345 - accuracy: 0.9467\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1390 - accuracy: 0.9447\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.1326 - accuracy: 0.9466\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1352 - accuracy: 0.9470\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1253 - accuracy: 0.9505\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.1298 - accuracy: 0.9486\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1245 - accuracy: 0.9501\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.1270 - accuracy: 0.9485\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.1206 - accuracy: 0.9514\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.1266 - accuracy: 0.9504\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.1193 - accuracy: 0.9528\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.1219 - accuracy: 0.9497\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.1141 - accuracy: 0.9552\n",
      "76/76 [==============================] - 3s 42ms/step - loss: 0.3155 - accuracy: 0.9245\n",
      "Model: \"sequential_52\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_208 (LSTM)              (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_209 (LSTM)              (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_210 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_211 (LSTM)              (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "flatten_52 (Flatten)         (None, 11520)             0         \n",
      "_________________________________________________________________\n",
      "dense_156 (Dense)            (None, 256)               2949376   \n",
      "_________________________________________________________________\n",
      "dropout_104 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_157 (Dense)            (None, 384)               98688     \n",
      "_________________________________________________________________\n",
      "dropout_105 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_158 (Dense)            (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 4,105,094\n",
      "Trainable params: 4,105,094\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128, n_lstm_3 = 256, n_lstm_4 = 128,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.1,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  elu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 18s 117ms/step - loss: 1.2512 - accuracy: 0.4750\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.8650 - accuracy: 0.6368\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.7163 - accuracy: 0.7027\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.6264 - accuracy: 0.7414\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.5638 - accuracy: 0.7691\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.5036 - accuracy: 0.7982\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.4644 - accuracy: 0.8162\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.4359 - accuracy: 0.8287\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.4096 - accuracy: 0.8389\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.3941 - accuracy: 0.8443\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.3693 - accuracy: 0.8541\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.3568 - accuracy: 0.8593\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.3351 - accuracy: 0.8680\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.3248 - accuracy: 0.8695\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.3212 - accuracy: 0.8732\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2977 - accuracy: 0.8809\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2967 - accuracy: 0.8823\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.2795 - accuracy: 0.8889\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.2704 - accuracy: 0.8938\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.2657 - accuracy: 0.8956\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2599 - accuracy: 0.8971\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2459 - accuracy: 0.9025\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2503 - accuracy: 0.9021\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2464 - accuracy: 0.9019\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2231 - accuracy: 0.9126\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.2202 - accuracy: 0.9154\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2133 - accuracy: 0.9168\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.2132 - accuracy: 0.9134\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.2085 - accuracy: 0.9156\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.1973 - accuracy: 0.9212\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.1899 - accuracy: 0.9242\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.2038 - accuracy: 0.9186\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1856 - accuracy: 0.9266\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1824 - accuracy: 0.9282\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1728 - accuracy: 0.9335\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1681 - accuracy: 0.9346\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.1769 - accuracy: 0.9317\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1665 - accuracy: 0.9349\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1628 - accuracy: 0.9365\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1630 - accuracy: 0.9359\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1569 - accuracy: 0.9379\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1504 - accuracy: 0.9418\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 18s 117ms/step - loss: 0.1449 - accuracy: 0.9440\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1481 - accuracy: 0.9417\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1479 - accuracy: 0.9431\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1422 - accuracy: 0.9431\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1397 - accuracy: 0.9454\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1429 - accuracy: 0.9438\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1326 - accuracy: 0.9481\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1386 - accuracy: 0.9459\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1279 - accuracy: 0.9500\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1273 - accuracy: 0.9493\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1221 - accuracy: 0.9512\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1193 - accuracy: 0.9527\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1262 - accuracy: 0.9498\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1239 - accuracy: 0.9510\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1228 - accuracy: 0.9523\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1170 - accuracy: 0.9523\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1103 - accuracy: 0.9560\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 18s 118ms/step - loss: 0.1150 - accuracy: 0.9539\n",
      "76/76 [==============================] - 3s 41ms/step - loss: 0.3138 - accuracy: 0.9224\n",
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_212 (LSTM)              (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_213 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_214 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_215 (LSTM)              (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "flatten_53 (Flatten)         (None, 11520)             0         \n",
      "_________________________________________________________________\n",
      "dense_159 (Dense)            (None, 384)               4424064   \n",
      "_________________________________________________________________\n",
      "dropout_106 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_160 (Dense)            (None, 512)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_107 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_161 (Dense)            (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 6,138,246\n",
      "Trainable params: 6,138,246\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256, n_lstm_3 = 256, n_lstm_4 = 128,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.1,\n",
      "          n_dense_1 = 384, dropout_1 = 0.2,\n",
      "          n_dense_2 = 512, dropout_2 = 0.2, activation_conv=  elu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 22s 146ms/step - loss: 1.2803 - accuracy: 0.4580\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.9095 - accuracy: 0.6109\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.8028 - accuracy: 0.6631\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.7236 - accuracy: 0.6972\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.6658 - accuracy: 0.7217\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.5860 - accuracy: 0.7603\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.5325 - accuracy: 0.7853\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.4968 - accuracy: 0.8019\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.4623 - accuracy: 0.8176\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.4277 - accuracy: 0.8290\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.4000 - accuracy: 0.8424\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.3769 - accuracy: 0.8498\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.3602 - accuracy: 0.8594\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.3435 - accuracy: 0.8656\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.3341 - accuracy: 0.8690\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.3201 - accuracy: 0.8717\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.3083 - accuracy: 0.8792\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.2995 - accuracy: 0.8809\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.2910 - accuracy: 0.8843\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2822 - accuracy: 0.8874\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.2717 - accuracy: 0.8913\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.2627 - accuracy: 0.8948\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2592 - accuracy: 0.8959\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.2566 - accuracy: 0.8968\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 23s 148ms/step - loss: 0.2403 - accuracy: 0.9056\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.2377 - accuracy: 0.9048\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2317 - accuracy: 0.9077\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2285 - accuracy: 0.9076\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.2185 - accuracy: 0.9117\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2169 - accuracy: 0.9148\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2000 - accuracy: 0.9194\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.1984 - accuracy: 0.9220\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.1945 - accuracy: 0.9218\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.1861 - accuracy: 0.9270\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1855 - accuracy: 0.9268\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1878 - accuracy: 0.9258\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.1708 - accuracy: 0.9319\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.1705 - accuracy: 0.9321\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1681 - accuracy: 0.9320\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.1723 - accuracy: 0.9312\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1659 - accuracy: 0.9340\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1574 - accuracy: 0.9374\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.1519 - accuracy: 0.9392\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1537 - accuracy: 0.9378\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.1510 - accuracy: 0.9387\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1473 - accuracy: 0.9410\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1449 - accuracy: 0.9416\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.1469 - accuracy: 0.9402\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1412 - accuracy: 0.9442\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.1447 - accuracy: 0.9418\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1373 - accuracy: 0.9450\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1306 - accuracy: 0.9479\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1354 - accuracy: 0.9464\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1271 - accuracy: 0.9477\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1271 - accuracy: 0.9488\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.1260 - accuracy: 0.9485\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1249 - accuracy: 0.9507\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1230 - accuracy: 0.9518\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1176 - accuracy: 0.9524\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.1197 - accuracy: 0.9520\n",
      "76/76 [==============================] - 4s 53ms/step - loss: 0.4530 - accuracy: 0.8861\n",
      "Model: \"sequential_54\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_216 (LSTM)              (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_217 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_218 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_219 (LSTM)              (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "flatten_54 (Flatten)         (None, 11520)             0         \n",
      "_________________________________________________________________\n",
      "dense_162 (Dense)            (None, 384)               4424064   \n",
      "_________________________________________________________________\n",
      "dropout_108 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_163 (Dense)            (None, 512)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_109 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_164 (Dense)            (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 6,138,246\n",
      "Trainable params: 6,138,246\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256, n_lstm_3 = 256, n_lstm_4 = 128,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.1,\n",
      "          n_dense_1 = 384, dropout_1 = 0.2,\n",
      "          n_dense_2 = 512, dropout_2 = 0.2, activation_conv=  elu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 22s 147ms/step - loss: 1.2741 - accuracy: 0.4657\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.9177 - accuracy: 0.6068\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.8161 - accuracy: 0.6534\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.7287 - accuracy: 0.6911\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.6517 - accuracy: 0.7286\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.5855 - accuracy: 0.7597\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.5262 - accuracy: 0.7877\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.4923 - accuracy: 0.7985\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.4472 - accuracy: 0.8215\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.4216 - accuracy: 0.8320\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.3990 - accuracy: 0.8427\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.3773 - accuracy: 0.8508\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.3589 - accuracy: 0.8578\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.3399 - accuracy: 0.8655\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.3261 - accuracy: 0.8711\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.3217 - accuracy: 0.8727\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.3058 - accuracy: 0.8780\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2992 - accuracy: 0.8817\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.2834 - accuracy: 0.8853\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2746 - accuracy: 0.8896\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2646 - accuracy: 0.8913\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.2566 - accuracy: 0.8962\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2522 - accuracy: 0.8991\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2356 - accuracy: 0.9050\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.2361 - accuracy: 0.9041\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.2356 - accuracy: 0.9048\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2334 - accuracy: 0.9069\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2293 - accuracy: 0.9076\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2144 - accuracy: 0.9123\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2080 - accuracy: 0.9149\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2011 - accuracy: 0.9190\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1996 - accuracy: 0.9182\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.1983 - accuracy: 0.9199\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1906 - accuracy: 0.9225\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1877 - accuracy: 0.9235\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.1727 - accuracy: 0.9308\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1892 - accuracy: 0.9245\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1727 - accuracy: 0.9321\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1774 - accuracy: 0.9294\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1706 - accuracy: 0.9311\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1618 - accuracy: 0.9340\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1715 - accuracy: 0.9302\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1558 - accuracy: 0.9395\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1633 - accuracy: 0.9352\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1511 - accuracy: 0.9399\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1500 - accuracy: 0.9414\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1466 - accuracy: 0.9420\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1387 - accuracy: 0.9462\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1413 - accuracy: 0.9428\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.1365 - accuracy: 0.9455\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.1427 - accuracy: 0.9437\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1290 - accuracy: 0.9485\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1333 - accuracy: 0.9459\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1449 - accuracy: 0.9425\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1312 - accuracy: 0.9468\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1263 - accuracy: 0.9493\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1178 - accuracy: 0.9517\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1187 - accuracy: 0.9515\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1147 - accuracy: 0.9527\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1127 - accuracy: 0.9554\n",
      "76/76 [==============================] - 4s 53ms/step - loss: 0.4284 - accuracy: 0.8834\n",
      "Model: \"sequential_55\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_220 (LSTM)              (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_221 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_222 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_223 (LSTM)              (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "flatten_55 (Flatten)         (None, 11520)             0         \n",
      "_________________________________________________________________\n",
      "dense_165 (Dense)            (None, 384)               4424064   \n",
      "_________________________________________________________________\n",
      "dropout_110 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_166 (Dense)            (None, 512)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_111 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_167 (Dense)            (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 6,138,246\n",
      "Trainable params: 6,138,246\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256, n_lstm_3 = 256, n_lstm_4 = 128,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.1,\n",
      "          n_dense_1 = 384, dropout_1 = 0.2,\n",
      "          n_dense_2 = 512, dropout_2 = 0.2, activation_conv=  elu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 22s 147ms/step - loss: 1.2566 - accuracy: 0.4709\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.9220 - accuracy: 0.6056\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.8217 - accuracy: 0.6511\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.7296 - accuracy: 0.6947\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.6558 - accuracy: 0.7243\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.6019 - accuracy: 0.7519\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.5518 - accuracy: 0.7768\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.4991 - accuracy: 0.8006\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.4585 - accuracy: 0.8190\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.4297 - accuracy: 0.8304\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.3997 - accuracy: 0.8431\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 22s 147ms/step - loss: 0.3770 - accuracy: 0.8524\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.3668 - accuracy: 0.8567\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.3409 - accuracy: 0.8662\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.3306 - accuracy: 0.8694\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.3238 - accuracy: 0.8710\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.3035 - accuracy: 0.8808\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2928 - accuracy: 0.8819\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2886 - accuracy: 0.8836\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2728 - accuracy: 0.8899\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2735 - accuracy: 0.8907\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2601 - accuracy: 0.8967\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2515 - accuracy: 0.8999\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2392 - accuracy: 0.9034\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2381 - accuracy: 0.9055\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2319 - accuracy: 0.9074\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2245 - accuracy: 0.9094\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2238 - accuracy: 0.9098\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2157 - accuracy: 0.9118\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.2125 - accuracy: 0.9150\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1934 - accuracy: 0.9228\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1988 - accuracy: 0.9200\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1907 - accuracy: 0.9240\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1802 - accuracy: 0.9286\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1826 - accuracy: 0.9270\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1799 - accuracy: 0.9274\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1745 - accuracy: 0.9300\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1679 - accuracy: 0.9334\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1638 - accuracy: 0.9341\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1614 - accuracy: 0.9342\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1696 - accuracy: 0.9319\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1590 - accuracy: 0.9365\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1524 - accuracy: 0.9394\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1531 - accuracy: 0.9384\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1434 - accuracy: 0.9431\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1462 - accuracy: 0.9404\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1444 - accuracy: 0.9418\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1397 - accuracy: 0.9431\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1375 - accuracy: 0.9449\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1357 - accuracy: 0.9459\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1357 - accuracy: 0.9454\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1340 - accuracy: 0.9462\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1282 - accuracy: 0.9489\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1295 - accuracy: 0.9486\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1311 - accuracy: 0.9481\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1196 - accuracy: 0.9512\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1245 - accuracy: 0.9493\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1250 - accuracy: 0.9485\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1202 - accuracy: 0.9510\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 22s 148ms/step - loss: 0.1152 - accuracy: 0.9530\n",
      "76/76 [==============================] - 4s 53ms/step - loss: 0.4254 - accuracy: 0.8846\n",
      "Model: \"sequential_56\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_224 (LSTM)              (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_225 (LSTM)              (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_226 (LSTM)              (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_227 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_56 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_168 (Dense)            (None, 384)               8847744   \n",
      "_________________________________________________________________\n",
      "dropout_112 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_169 (Dense)            (None, 384)               147840    \n",
      "_________________________________________________________________\n",
      "dropout_113 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_170 (Dense)            (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 9,987,078\n",
      "Trainable params: 9,987,078\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128, n_lstm_3 = 128, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 384, dropout_1 = 0.3,\n",
      "          n_dense_2 = 384, dropout_2 = 0.2, activation_conv=  relu,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 17s 115ms/step - loss: 1.2255 - accuracy: 0.4783\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.8604 - accuracy: 0.6361\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.7232 - accuracy: 0.7021\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.6175 - accuracy: 0.7489\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.5631 - accuracy: 0.7748\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.4946 - accuracy: 0.8047\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.4561 - accuracy: 0.8200\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.4354 - accuracy: 0.8295\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.4048 - accuracy: 0.8421\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.3815 - accuracy: 0.8499\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.3642 - accuracy: 0.8584\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.3503 - accuracy: 0.8633\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.3405 - accuracy: 0.8636\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.3218 - accuracy: 0.8705\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.3109 - accuracy: 0.8739\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.3069 - accuracy: 0.8773\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.2957 - accuracy: 0.8813\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2823 - accuracy: 0.8855\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2890 - accuracy: 0.8843\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2636 - accuracy: 0.8922\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2782 - accuracy: 0.8878\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2521 - accuracy: 0.8995\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2596 - accuracy: 0.8942\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2521 - accuracy: 0.8984\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.2469 - accuracy: 0.9014\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2352 - accuracy: 0.9058\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2254 - accuracy: 0.9095\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2220 - accuracy: 0.9112\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2143 - accuracy: 0.9146\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2116 - accuracy: 0.9158\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2081 - accuracy: 0.9174\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2066 - accuracy: 0.9160\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2071 - accuracy: 0.9183\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1897 - accuracy: 0.9237\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1903 - accuracy: 0.9248\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1946 - accuracy: 0.9235\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1848 - accuracy: 0.9257\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1820 - accuracy: 0.9280\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1836 - accuracy: 0.9271\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1736 - accuracy: 0.9312\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1705 - accuracy: 0.9317\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1636 - accuracy: 0.9354\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1680 - accuracy: 0.9322\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1695 - accuracy: 0.9329\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1602 - accuracy: 0.9368\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1646 - accuracy: 0.9351\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1526 - accuracy: 0.9386\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.1662 - accuracy: 0.9348\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1519 - accuracy: 0.9388\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1478 - accuracy: 0.9395\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1566 - accuracy: 0.9395\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1561 - accuracy: 0.9390\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1519 - accuracy: 0.9400\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.1391 - accuracy: 0.9447\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1415 - accuracy: 0.9446\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1366 - accuracy: 0.9458\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1307 - accuracy: 0.9472\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1352 - accuracy: 0.9465\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1361 - accuracy: 0.9463\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1370 - accuracy: 0.9456\n",
      "76/76 [==============================] - 3s 41ms/step - loss: 0.2995 - accuracy: 0.9153\n",
      "Model: \"sequential_57\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_228 (LSTM)              (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_229 (LSTM)              (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_230 (LSTM)              (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_231 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_57 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_171 (Dense)            (None, 384)               8847744   \n",
      "_________________________________________________________________\n",
      "dropout_114 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_172 (Dense)            (None, 384)               147840    \n",
      "_________________________________________________________________\n",
      "dropout_115 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_173 (Dense)            (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 9,987,078\n",
      "Trainable params: 9,987,078\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128, n_lstm_3 = 128, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 384, dropout_1 = 0.3,\n",
      "          n_dense_2 = 384, dropout_2 = 0.2, activation_conv=  relu,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 17s 115ms/step - loss: 1.2371 - accuracy: 0.4806\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.8665 - accuracy: 0.6290\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.7187 - accuracy: 0.7030\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.6114 - accuracy: 0.7523\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.5626 - accuracy: 0.7736\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.4933 - accuracy: 0.8031\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.4624 - accuracy: 0.8169\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.4341 - accuracy: 0.8298\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.4054 - accuracy: 0.8406\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.3757 - accuracy: 0.8529\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.3648 - accuracy: 0.8585\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.3542 - accuracy: 0.8601\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.3433 - accuracy: 0.8629\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.3246 - accuracy: 0.8707\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.3142 - accuracy: 0.8735\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.3070 - accuracy: 0.8752\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2956 - accuracy: 0.8799\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.2818 - accuracy: 0.8835\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2832 - accuracy: 0.8846\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2720 - accuracy: 0.8887\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2625 - accuracy: 0.8903\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2534 - accuracy: 0.8950\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2467 - accuracy: 0.8996\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2546 - accuracy: 0.8959\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2447 - accuracy: 0.9002\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2350 - accuracy: 0.9023\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2330 - accuracy: 0.9031\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2299 - accuracy: 0.9066\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2223 - accuracy: 0.9090\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2076 - accuracy: 0.9149\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2102 - accuracy: 0.9132\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2099 - accuracy: 0.9142\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2117 - accuracy: 0.9139\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2047 - accuracy: 0.9164\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1907 - accuracy: 0.9217\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1967 - accuracy: 0.9214\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2018 - accuracy: 0.9183\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1844 - accuracy: 0.9238\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1750 - accuracy: 0.9286\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1874 - accuracy: 0.9257\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1702 - accuracy: 0.9324\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1646 - accuracy: 0.9321\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1697 - accuracy: 0.9318\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1709 - accuracy: 0.9300\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.1649 - accuracy: 0.9343\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1584 - accuracy: 0.9352\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1532 - accuracy: 0.9403\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1548 - accuracy: 0.9392\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1491 - accuracy: 0.9400\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1487 - accuracy: 0.9403\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1505 - accuracy: 0.9385\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1586 - accuracy: 0.9366\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1455 - accuracy: 0.9401\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1460 - accuracy: 0.9412\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1391 - accuracy: 0.9454\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1387 - accuracy: 0.9436\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1371 - accuracy: 0.9454\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1374 - accuracy: 0.9450\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1350 - accuracy: 0.9432\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1345 - accuracy: 0.9461\n",
      "76/76 [==============================] - 3s 41ms/step - loss: 0.3344 - accuracy: 0.9194\n",
      "Model: \"sequential_58\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_232 (LSTM)              (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_233 (LSTM)              (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_234 (LSTM)              (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_235 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "flatten_58 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_174 (Dense)            (None, 384)               8847744   \n",
      "_________________________________________________________________\n",
      "dropout_116 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_175 (Dense)            (None, 384)               147840    \n",
      "_________________________________________________________________\n",
      "dropout_117 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_176 (Dense)            (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 9,987,078\n",
      "Trainable params: 9,987,078\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128, n_lstm_3 = 128, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 384, dropout_1 = 0.3,\n",
      "          n_dense_2 = 384, dropout_2 = 0.2, activation_conv=  relu,\n",
      "          activation_dense=  relu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 17s 115ms/step - loss: 1.2491 - accuracy: 0.4687\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.8646 - accuracy: 0.6346\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.7226 - accuracy: 0.7017\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.6276 - accuracy: 0.7426\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.5569 - accuracy: 0.7757\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.5030 - accuracy: 0.8008\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.4654 - accuracy: 0.8169\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.4363 - accuracy: 0.8258\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.4132 - accuracy: 0.8364\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.3915 - accuracy: 0.8452\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.3719 - accuracy: 0.8521\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.3572 - accuracy: 0.8582\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.3422 - accuracy: 0.8646\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.3322 - accuracy: 0.8673\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.3179 - accuracy: 0.8727\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.3184 - accuracy: 0.8738\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2903 - accuracy: 0.8827\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 18s 115ms/step - loss: 0.2975 - accuracy: 0.8816\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2994 - accuracy: 0.8801\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2802 - accuracy: 0.8863\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2713 - accuracy: 0.8905\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2549 - accuracy: 0.8990\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2551 - accuracy: 0.8959\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2476 - accuracy: 0.8972\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2442 - accuracy: 0.9015\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2347 - accuracy: 0.9069\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2209 - accuracy: 0.9114\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2248 - accuracy: 0.9109\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2175 - accuracy: 0.9134\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2106 - accuracy: 0.9159\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2119 - accuracy: 0.9164\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.2085 - accuracy: 0.9167\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1967 - accuracy: 0.9209\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1936 - accuracy: 0.9222\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1870 - accuracy: 0.9247\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1804 - accuracy: 0.9272\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1805 - accuracy: 0.9276\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1841 - accuracy: 0.9270\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1750 - accuracy: 0.9313\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1734 - accuracy: 0.9295\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1725 - accuracy: 0.9313\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1717 - accuracy: 0.9339\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1710 - accuracy: 0.9319\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1583 - accuracy: 0.9364\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1632 - accuracy: 0.9343\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1526 - accuracy: 0.9399\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1596 - accuracy: 0.9371\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1572 - accuracy: 0.9362\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1484 - accuracy: 0.9411\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1504 - accuracy: 0.9379\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1482 - accuracy: 0.9420\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1462 - accuracy: 0.9421\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1384 - accuracy: 0.9456\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1385 - accuracy: 0.9445\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1371 - accuracy: 0.9446\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1390 - accuracy: 0.9442\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1360 - accuracy: 0.9455\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1311 - accuracy: 0.9478\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1343 - accuracy: 0.9474\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 18s 116ms/step - loss: 0.1316 - accuracy: 0.9478\n",
      "76/76 [==============================] - 3s 40ms/step - loss: 0.2796 - accuracy: 0.9235\n",
      "Model: \"sequential_59\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_236 (LSTM)              (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_237 (LSTM)              (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_238 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_239 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_59 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_177 (Dense)            (None, 256)               5898496   \n",
      "_________________________________________________________________\n",
      "dropout_118 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_178 (Dense)            (None, 384)               98688     \n",
      "_________________________________________________________________\n",
      "dropout_119 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_179 (Dense)            (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 7,382,406\n",
      "Trainable params: 7,382,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128, n_lstm_3 = 256, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.3,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510688>,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 21s 140ms/step - loss: 1.2888 - accuracy: 0.4564\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.8906 - accuracy: 0.6284\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.7536 - accuracy: 0.6892\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.6658 - accuracy: 0.7265\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.5908 - accuracy: 0.7598\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.5218 - accuracy: 0.7900\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.4815 - accuracy: 0.8063\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.4498 - accuracy: 0.8221\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.4169 - accuracy: 0.8362\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3991 - accuracy: 0.8418\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3729 - accuracy: 0.8528\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3592 - accuracy: 0.8576\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3420 - accuracy: 0.8647\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3316 - accuracy: 0.8688\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3209 - accuracy: 0.8734\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3204 - accuracy: 0.8719\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2991 - accuracy: 0.8811\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2875 - accuracy: 0.8862\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2851 - accuracy: 0.8835\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2793 - accuracy: 0.8909\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2798 - accuracy: 0.8895\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2557 - accuracy: 0.8978\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2506 - accuracy: 0.9007\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2514 - accuracy: 0.8994\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2447 - accuracy: 0.9020\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2303 - accuracy: 0.9090\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2231 - accuracy: 0.9127\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2201 - accuracy: 0.9116\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2193 - accuracy: 0.9120\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2071 - accuracy: 0.9192\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2053 - accuracy: 0.9193\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1997 - accuracy: 0.9206\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1891 - accuracy: 0.9244\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1818 - accuracy: 0.9286\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1789 - accuracy: 0.9286\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1750 - accuracy: 0.9323\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1797 - accuracy: 0.9308\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1682 - accuracy: 0.9326\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1626 - accuracy: 0.9353\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1605 - accuracy: 0.9351\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1606 - accuracy: 0.9357\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1645 - accuracy: 0.9355\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1603 - accuracy: 0.9369\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1512 - accuracy: 0.9396\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1551 - accuracy: 0.9370\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1583 - accuracy: 0.9378\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1419 - accuracy: 0.9439\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1458 - accuracy: 0.9427\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1418 - accuracy: 0.9435\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1314 - accuracy: 0.9461\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1316 - accuracy: 0.9470\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1320 - accuracy: 0.9479\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1254 - accuracy: 0.9496\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1243 - accuracy: 0.9497\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1278 - accuracy: 0.9491\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1217 - accuracy: 0.9512\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1264 - accuracy: 0.9492\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1170 - accuracy: 0.9539\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1173 - accuracy: 0.9539\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1178 - accuracy: 0.9540\n",
      "76/76 [==============================] - 4s 51ms/step - loss: 0.3014 - accuracy: 0.9178\n",
      "Model: \"sequential_60\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_240 (LSTM)              (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_241 (LSTM)              (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_242 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_243 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_60 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_180 (Dense)            (None, 256)               5898496   \n",
      "_________________________________________________________________\n",
      "dropout_120 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_181 (Dense)            (None, 384)               98688     \n",
      "_________________________________________________________________\n",
      "dropout_121 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_182 (Dense)            (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 7,382,406\n",
      "Trainable params: 7,382,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128, n_lstm_3 = 256, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.3,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510688>,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 21s 140ms/step - loss: 1.2660 - accuracy: 0.4636\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.8793 - accuracy: 0.6287\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.7380 - accuracy: 0.6920\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.6482 - accuracy: 0.7343\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.5857 - accuracy: 0.7620\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.5108 - accuracy: 0.7930\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.4678 - accuracy: 0.8101\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.4465 - accuracy: 0.8215\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.4096 - accuracy: 0.8386\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3934 - accuracy: 0.8402\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3809 - accuracy: 0.8503\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3558 - accuracy: 0.8602\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3427 - accuracy: 0.8653\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3287 - accuracy: 0.8705\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3169 - accuracy: 0.8735\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3101 - accuracy: 0.8784\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2955 - accuracy: 0.8826\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2919 - accuracy: 0.8834\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2819 - accuracy: 0.8901\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2640 - accuracy: 0.8944\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2649 - accuracy: 0.8910\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2515 - accuracy: 0.8992\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2462 - accuracy: 0.9017\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2378 - accuracy: 0.9036\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2322 - accuracy: 0.9055\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2299 - accuracy: 0.9051\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2213 - accuracy: 0.9088\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2236 - accuracy: 0.9069\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2180 - accuracy: 0.9115\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2037 - accuracy: 0.9175\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2040 - accuracy: 0.9180\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1912 - accuracy: 0.9209\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 21s 141ms/step - loss: 0.1912 - accuracy: 0.9224\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1763 - accuracy: 0.9289\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1792 - accuracy: 0.9272\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1729 - accuracy: 0.9302\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1723 - accuracy: 0.9307\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1638 - accuracy: 0.9333\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1648 - accuracy: 0.9334\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1555 - accuracy: 0.9363\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1548 - accuracy: 0.9374\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1501 - accuracy: 0.9405\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1480 - accuracy: 0.9406\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1441 - accuracy: 0.9426\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1386 - accuracy: 0.9432\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1381 - accuracy: 0.9442\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1367 - accuracy: 0.9442\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 22s 144ms/step - loss: 0.1367 - accuracy: 0.9440\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 22s 146ms/step - loss: 0.1455 - accuracy: 0.9406\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1312 - accuracy: 0.9459\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 22s 145ms/step - loss: 0.1304 - accuracy: 0.9468\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 22s 142ms/step - loss: 0.1338 - accuracy: 0.9457\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1355 - accuracy: 0.9448\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1244 - accuracy: 0.9498\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1232 - accuracy: 0.9495\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1230 - accuracy: 0.9495\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1175 - accuracy: 0.9525\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1232 - accuracy: 0.9489\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1182 - accuracy: 0.9509\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1102 - accuracy: 0.9553\n",
      "76/76 [==============================] - 4s 51ms/step - loss: 0.3144 - accuracy: 0.9222\n",
      "Model: \"sequential_61\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_244 (LSTM)              (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_245 (LSTM)              (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_246 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_247 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_61 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_183 (Dense)            (None, 256)               5898496   \n",
      "_________________________________________________________________\n",
      "dropout_122 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_184 (Dense)            (None, 384)               98688     \n",
      "_________________________________________________________________\n",
      "dropout_123 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_185 (Dense)            (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 7,382,406\n",
      "Trainable params: 7,382,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128, n_lstm_3 = 256, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.3,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510688>,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 21s 140ms/step - loss: 1.2674 - accuracy: 0.4629\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.8902 - accuracy: 0.6267\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.7479 - accuracy: 0.6901\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.6591 - accuracy: 0.7309\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.5836 - accuracy: 0.7652\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.5223 - accuracy: 0.7908\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.4849 - accuracy: 0.8071\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.4440 - accuracy: 0.8261\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.4278 - accuracy: 0.8349\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3967 - accuracy: 0.8468\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3710 - accuracy: 0.8573\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3640 - accuracy: 0.8591\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3371 - accuracy: 0.8694\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3192 - accuracy: 0.8753\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3244 - accuracy: 0.8761\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3057 - accuracy: 0.8813\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.3033 - accuracy: 0.8809\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2831 - accuracy: 0.8889\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2794 - accuracy: 0.8911\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2698 - accuracy: 0.8927\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2662 - accuracy: 0.8949\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2498 - accuracy: 0.9031\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2483 - accuracy: 0.9026\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2389 - accuracy: 0.9059\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2340 - accuracy: 0.9080\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2265 - accuracy: 0.9115\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2202 - accuracy: 0.9144\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2132 - accuracy: 0.9159\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2035 - accuracy: 0.9201\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.2073 - accuracy: 0.9197\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1979 - accuracy: 0.9211\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1887 - accuracy: 0.9252\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1961 - accuracy: 0.9235\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1841 - accuracy: 0.9264\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1731 - accuracy: 0.9335\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1737 - accuracy: 0.9323\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1721 - accuracy: 0.9324\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1684 - accuracy: 0.9331\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1672 - accuracy: 0.9350\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1619 - accuracy: 0.9365\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1555 - accuracy: 0.9392\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1539 - accuracy: 0.9404\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1555 - accuracy: 0.9395\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1544 - accuracy: 0.9394\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1455 - accuracy: 0.9432\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1482 - accuracy: 0.9422\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1455 - accuracy: 0.9430\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1576 - accuracy: 0.9388\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1395 - accuracy: 0.9442\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1325 - accuracy: 0.9476\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1332 - accuracy: 0.9463\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1272 - accuracy: 0.9497\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1324 - accuracy: 0.9496\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1230 - accuracy: 0.9503\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1269 - accuracy: 0.9497\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 21s 141ms/step - loss: 0.1240 - accuracy: 0.9504\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1133 - accuracy: 0.9555\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1267 - accuracy: 0.9513\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1177 - accuracy: 0.9546\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 21s 140ms/step - loss: 0.1202 - accuracy: 0.9540\n",
      "76/76 [==============================] - 4s 51ms/step - loss: 0.3082 - accuracy: 0.9177\n",
      "Model: \"sequential_62\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_248 (LSTM)              (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_249 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_250 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_251 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_62 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_186 (Dense)            (None, 512)               11796992  \n",
      "_________________________________________________________________\n",
      "dropout_124 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_187 (Dense)            (None, 384)               196992    \n",
      "_________________________________________________________________\n",
      "dropout_125 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_188 (Dense)            (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 13,838,470\n",
      "Trainable params: 13,838,470\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256, n_lstm_3 = 256, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 512, dropout_1 = 0.2,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 26s 173ms/step - loss: 1.2753 - accuracy: 0.4615\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 26s 173ms/step - loss: 0.9377 - accuracy: 0.5977\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 26s 173ms/step - loss: 0.8229 - accuracy: 0.6489\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.7417 - accuracy: 0.6913\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.6569 - accuracy: 0.7266\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.5922 - accuracy: 0.7578\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.5449 - accuracy: 0.7801\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.5083 - accuracy: 0.7985\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 26s 173ms/step - loss: 0.4745 - accuracy: 0.8128\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.4368 - accuracy: 0.8273\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.4103 - accuracy: 0.8392\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.3910 - accuracy: 0.8481\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.3704 - accuracy: 0.8548\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.3491 - accuracy: 0.8656\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.3339 - accuracy: 0.8667\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.3196 - accuracy: 0.8734\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.3110 - accuracy: 0.8757\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.3047 - accuracy: 0.8804\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2937 - accuracy: 0.8838\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2759 - accuracy: 0.8897\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2792 - accuracy: 0.8889\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2618 - accuracy: 0.8973\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2536 - accuracy: 0.8985\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2470 - accuracy: 0.9003\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2407 - accuracy: 0.9029\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2403 - accuracy: 0.9047\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2321 - accuracy: 0.9073\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2241 - accuracy: 0.9113\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2173 - accuracy: 0.9139\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2115 - accuracy: 0.9154\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2061 - accuracy: 0.9182\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2016 - accuracy: 0.9191\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1994 - accuracy: 0.9193\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1932 - accuracy: 0.9220\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1837 - accuracy: 0.9259\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1805 - accuracy: 0.9292\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1785 - accuracy: 0.9281\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1690 - accuracy: 0.9327\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1723 - accuracy: 0.9315\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1630 - accuracy: 0.9350\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1723 - accuracy: 0.9310\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1614 - accuracy: 0.9356\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1508 - accuracy: 0.9400\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1566 - accuracy: 0.9372\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1537 - accuracy: 0.9388\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1459 - accuracy: 0.9412\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1465 - accuracy: 0.9400\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1419 - accuracy: 0.9430\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1353 - accuracy: 0.9458\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1352 - accuracy: 0.9443\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1353 - accuracy: 0.9455\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1356 - accuracy: 0.9468\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1248 - accuracy: 0.9490\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1324 - accuracy: 0.9483\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1290 - accuracy: 0.9482\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1248 - accuracy: 0.9500\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1281 - accuracy: 0.9500\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1212 - accuracy: 0.9519\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1186 - accuracy: 0.9522\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1192 - accuracy: 0.9526\n",
      "76/76 [==============================] - 5s 62ms/step - loss: 0.4201 - accuracy: 0.8858\n",
      "Model: \"sequential_63\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_252 (LSTM)              (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_253 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_254 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_255 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_63 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_189 (Dense)            (None, 512)               11796992  \n",
      "_________________________________________________________________\n",
      "dropout_126 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_190 (Dense)            (None, 384)               196992    \n",
      "_________________________________________________________________\n",
      "dropout_127 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_191 (Dense)            (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 13,838,470\n",
      "Trainable params: 13,838,470\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256, n_lstm_3 = 256, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 512, dropout_1 = 0.2,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 26s 173ms/step - loss: 1.2509 - accuracy: 0.4738\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 26s 173ms/step - loss: 0.9244 - accuracy: 0.6033\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 26s 173ms/step - loss: 0.8114 - accuracy: 0.6570\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 26s 173ms/step - loss: 0.7233 - accuracy: 0.6971\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 26s 173ms/step - loss: 0.6482 - accuracy: 0.7341\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 26s 173ms/step - loss: 0.5703 - accuracy: 0.7653\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 26s 173ms/step - loss: 0.5212 - accuracy: 0.7886\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 26s 173ms/step - loss: 0.4879 - accuracy: 0.8042\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 26s 173ms/step - loss: 0.4470 - accuracy: 0.8241\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 26s 173ms/step - loss: 0.4241 - accuracy: 0.8310\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.3982 - accuracy: 0.8446\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.3881 - accuracy: 0.8471\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.3670 - accuracy: 0.8546\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.3490 - accuracy: 0.8617\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.3284 - accuracy: 0.8699\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.3243 - accuracy: 0.8695\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.3059 - accuracy: 0.8792\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 26s 173ms/step - loss: 0.2954 - accuracy: 0.8822\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2948 - accuracy: 0.8806\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2862 - accuracy: 0.8836\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2755 - accuracy: 0.8880\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2632 - accuracy: 0.8934\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2490 - accuracy: 0.9003\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2421 - accuracy: 0.9023\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2465 - accuracy: 0.8999\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2269 - accuracy: 0.9074\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2321 - accuracy: 0.9053\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2207 - accuracy: 0.9085\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2129 - accuracy: 0.9146\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2033 - accuracy: 0.9157\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2011 - accuracy: 0.9191\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1882 - accuracy: 0.9220\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1994 - accuracy: 0.9202\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1827 - accuracy: 0.9250\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1851 - accuracy: 0.9239\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1754 - accuracy: 0.9280\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1717 - accuracy: 0.9326\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1649 - accuracy: 0.9343\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1690 - accuracy: 0.9313\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1635 - accuracy: 0.9325\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1589 - accuracy: 0.9381\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1553 - accuracy: 0.9359\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1627 - accuracy: 0.9353\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1524 - accuracy: 0.9380\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1439 - accuracy: 0.9413\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1407 - accuracy: 0.9439\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1434 - accuracy: 0.9426\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1361 - accuracy: 0.9460\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1379 - accuracy: 0.9457\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1320 - accuracy: 0.9454\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1383 - accuracy: 0.9443\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1350 - accuracy: 0.9463\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1272 - accuracy: 0.9487\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1274 - accuracy: 0.9494\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1234 - accuracy: 0.9497\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1173 - accuracy: 0.9528\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1188 - accuracy: 0.9517\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1187 - accuracy: 0.9526s - loss:\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1164 - accuracy: 0.9518\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1146 - accuracy: 0.9538\n",
      "76/76 [==============================] - 5s 62ms/step - loss: 0.3945 - accuracy: 0.8888\n",
      "Model: \"sequential_64\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_256 (LSTM)              (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_257 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_258 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_259 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_64 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_192 (Dense)            (None, 512)               11796992  \n",
      "_________________________________________________________________\n",
      "dropout_128 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_193 (Dense)            (None, 384)               196992    \n",
      "_________________________________________________________________\n",
      "dropout_129 (Dropout)        (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_194 (Dense)            (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 13,838,470\n",
      "Trainable params: 13,838,470\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 256, n_lstm_3 = 256, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 512, dropout_1 = 0.2,\n",
      "          n_dense_2 = 384, dropout_2 = 0.3, activation_conv=  relu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 26s 173ms/step - loss: 1.3018 - accuracy: 0.4490\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 26s 173ms/step - loss: 0.9460 - accuracy: 0.5990\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 26s 173ms/step - loss: 0.8217 - accuracy: 0.6504\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.7426 - accuracy: 0.6901\n",
      "Epoch 5/60\n",
      "152/152 [==============================] - 26s 173ms/step - loss: 0.6459 - accuracy: 0.7364\n",
      "Epoch 6/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.5821 - accuracy: 0.7646\n",
      "Epoch 7/60\n",
      "152/152 [==============================] - 26s 173ms/step - loss: 0.5418 - accuracy: 0.7809\n",
      "Epoch 8/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.5023 - accuracy: 0.7990\n",
      "Epoch 9/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.4729 - accuracy: 0.8120\n",
      "Epoch 10/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.4373 - accuracy: 0.8254\n",
      "Epoch 11/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.4161 - accuracy: 0.8367\n",
      "Epoch 12/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.4028 - accuracy: 0.8416\n",
      "Epoch 13/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.3779 - accuracy: 0.8491\n",
      "Epoch 14/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.3580 - accuracy: 0.8576\n",
      "Epoch 15/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.3461 - accuracy: 0.8639\n",
      "Epoch 16/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.3258 - accuracy: 0.8681\n",
      "Epoch 17/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.3168 - accuracy: 0.8746\n",
      "Epoch 18/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.3069 - accuracy: 0.8764\n",
      "Epoch 19/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2967 - accuracy: 0.8799\n",
      "Epoch 20/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2898 - accuracy: 0.8838\n",
      "Epoch 21/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2831 - accuracy: 0.8859\n",
      "Epoch 22/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2725 - accuracy: 0.8934\n",
      "Epoch 23/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2589 - accuracy: 0.8956\n",
      "Epoch 24/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2489 - accuracy: 0.9007\n",
      "Epoch 25/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2443 - accuracy: 0.9025\n",
      "Epoch 26/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2392 - accuracy: 0.9064\n",
      "Epoch 27/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2292 - accuracy: 0.9092\n",
      "Epoch 28/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2232 - accuracy: 0.9097\n",
      "Epoch 29/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2233 - accuracy: 0.9109\n",
      "Epoch 30/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2185 - accuracy: 0.9137\n",
      "Epoch 31/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2047 - accuracy: 0.9174\n",
      "Epoch 32/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.2073 - accuracy: 0.9181\n",
      "Epoch 33/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1936 - accuracy: 0.9226\n",
      "Epoch 34/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1871 - accuracy: 0.9264\n",
      "Epoch 35/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1881 - accuracy: 0.9245\n",
      "Epoch 36/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1874 - accuracy: 0.9246\n",
      "Epoch 37/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1853 - accuracy: 0.9262\n",
      "Epoch 38/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1809 - accuracy: 0.9271\n",
      "Epoch 39/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1716 - accuracy: 0.9326\n",
      "Epoch 40/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1606 - accuracy: 0.9362\n",
      "Epoch 41/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1669 - accuracy: 0.9350\n",
      "Epoch 42/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1591 - accuracy: 0.9372\n",
      "Epoch 43/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1542 - accuracy: 0.9385\n",
      "Epoch 44/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1542 - accuracy: 0.9384\n",
      "Epoch 45/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1536 - accuracy: 0.9398\n",
      "Epoch 46/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1493 - accuracy: 0.9405\n",
      "Epoch 47/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1447 - accuracy: 0.9428s - loss: 0.1440 - accu\n",
      "Epoch 48/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1443 - accuracy: 0.9431\n",
      "Epoch 49/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1438 - accuracy: 0.9427\n",
      "Epoch 50/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1372 - accuracy: 0.9446\n",
      "Epoch 51/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1363 - accuracy: 0.9439\n",
      "Epoch 52/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1279 - accuracy: 0.9482\n",
      "Epoch 53/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1344 - accuracy: 0.9480\n",
      "Epoch 54/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1378 - accuracy: 0.9457\n",
      "Epoch 55/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1339 - accuracy: 0.9478\n",
      "Epoch 56/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1258 - accuracy: 0.9509\n",
      "Epoch 57/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1159 - accuracy: 0.9533\n",
      "Epoch 58/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1255 - accuracy: 0.9501\n",
      "Epoch 59/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1168 - accuracy: 0.9544\n",
      "Epoch 60/60\n",
      "152/152 [==============================] - 26s 174ms/step - loss: 0.1196 - accuracy: 0.9523\n",
      "76/76 [==============================] - 5s 62ms/step - loss: 0.4402 - accuracy: 0.8751\n",
      "Model: \"sequential_65\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_260 (LSTM)              (None, 90, 128)           67584     \n",
      "_________________________________________________________________\n",
      "lstm_261 (LSTM)              (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_262 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_263 (LSTM)              (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_65 (Flatten)         (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_195 (Dense)            (None, 512)               11796992  \n",
      "_________________________________________________________________\n",
      "dropout_130 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_196 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_131 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_197 (Dense)            (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 13,575,174\n",
      "Trainable params: 13,575,174\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 128, n_lstm_2 = 256, n_lstm_3 = 256, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.1, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 512, dropout_1 = 0.3,\n",
      "          n_dense_2 = 512, dropout_2 = 0.2, activation_conv=  elu,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228/228 [==============================] - 34s 150ms/step - loss: 1.1938 - accuracy: 0.4917\n",
      "Epoch 2/60\n",
      "228/228 [==============================] - 34s 150ms/step - loss: 0.8386 - accuracy: 0.6489\n",
      "Epoch 3/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.6632 - accuracy: 0.7276\n",
      "Epoch 4/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.5632 - accuracy: 0.7726\n",
      "Epoch 5/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.4945 - accuracy: 0.8043\n",
      "Epoch 6/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.4502 - accuracy: 0.8256\n",
      "Epoch 7/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.4088 - accuracy: 0.8403\n",
      "Epoch 8/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.3757 - accuracy: 0.8546\n",
      "Epoch 9/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.3596 - accuracy: 0.8609\n",
      "Epoch 10/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.3356 - accuracy: 0.8710\n",
      "Epoch 11/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.3217 - accuracy: 0.8748\n",
      "Epoch 12/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.3084 - accuracy: 0.8808\n",
      "Epoch 13/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.2880 - accuracy: 0.8882\n",
      "Epoch 14/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.2762 - accuracy: 0.8949\n",
      "Epoch 15/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.2679 - accuracy: 0.8946\n",
      "Epoch 16/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.2595 - accuracy: 0.8987\n",
      "Epoch 17/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.2493 - accuracy: 0.9040\n",
      "Epoch 18/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.2384 - accuracy: 0.9074\n",
      "Epoch 19/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.2346 - accuracy: 0.9100\n",
      "Epoch 20/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.2234 - accuracy: 0.9128\n",
      "Epoch 21/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.2122 - accuracy: 0.9174\n",
      "Epoch 22/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.2060 - accuracy: 0.9196\n",
      "Epoch 23/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1931 - accuracy: 0.9264\n",
      "Epoch 24/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1906 - accuracy: 0.9272\n",
      "Epoch 25/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1874 - accuracy: 0.9286\n",
      "Epoch 26/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1814 - accuracy: 0.9308\n",
      "Epoch 27/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1712 - accuracy: 0.9323\n",
      "Epoch 28/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1653 - accuracy: 0.9351\n",
      "Epoch 29/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1668 - accuracy: 0.9353\n",
      "Epoch 30/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1572 - accuracy: 0.9389\n",
      "Epoch 31/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1533 - accuracy: 0.9406\n",
      "Epoch 32/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1543 - accuracy: 0.9402\n",
      "Epoch 33/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1450 - accuracy: 0.9437\n",
      "Epoch 34/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1465 - accuracy: 0.9426\n",
      "Epoch 35/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1446 - accuracy: 0.9429\n",
      "Epoch 36/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1431 - accuracy: 0.9444\n",
      "Epoch 37/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1347 - accuracy: 0.9464\n",
      "Epoch 38/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1300 - accuracy: 0.9479\n",
      "Epoch 39/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1270 - accuracy: 0.9492\n",
      "Epoch 40/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1244 - accuracy: 0.9519\n",
      "Epoch 41/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1239 - accuracy: 0.9511\n",
      "Epoch 42/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1230 - accuracy: 0.9506\n",
      "Epoch 43/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1198 - accuracy: 0.9530\n",
      "Epoch 44/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1189 - accuracy: 0.9529\n",
      "Epoch 45/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1134 - accuracy: 0.9554\n",
      "Epoch 46/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1172 - accuracy: 0.9542\n",
      "Epoch 47/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1079 - accuracy: 0.9569\n",
      "Epoch 48/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1114 - accuracy: 0.9566\n",
      "Epoch 49/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1115 - accuracy: 0.9558\n",
      "Epoch 50/60\n",
      "228/228 [==============================] - 35s 151ms/step - loss: 0.1039 - accuracy: 0.9592\n",
      "Epoch 51/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1032 - accuracy: 0.9588\n",
      "Epoch 52/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1001 - accuracy: 0.9609\n",
      "Epoch 53/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1043 - accuracy: 0.9589\n",
      "Epoch 54/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1019 - accuracy: 0.9606\n",
      "Epoch 55/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.1022 - accuracy: 0.9596\n",
      "Epoch 56/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.0987 - accuracy: 0.9604\n",
      "Epoch 57/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.0941 - accuracy: 0.9626\n",
      "Epoch 58/60\n",
      "228/228 [==============================] - 35s 151ms/step - loss: 0.0972 - accuracy: 0.9623\n",
      "Epoch 59/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.0963 - accuracy: 0.9621\n",
      "Epoch 60/60\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.0914 - accuracy: 0.9643\n",
      "Wall time: 20h 3min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rscv = RandomizedSearchCV(model, param_distributions=params, cv=cv, n_iter=n_tune_iter)\n",
    "rscv_results = rscv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score is: 0.9249288042386373 using {'n_lstm_4': 256, 'n_lstm_3': 256, 'n_lstm_2': 256, 'n_lstm_1': 128, 'n_dense_2': 512, 'n_dense_1': 512, 'dropout_2': 0.2, 'dropout_1': 0.3, 'drop_lstm_4': 0.02, 'drop_lstm_3': 0.1, 'drop_lstm_2': 0.02, 'drop_lstm_1': 0.02, 'activation_dense': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>, 'activation_conv': 'elu'}\n"
     ]
    }
   ],
   "source": [
    "print('Best score is: {} using {}'.format(rscv_results.best_score_,\n",
    "rscv_results.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = rscv_results.best_estimator_\n",
    "best_param = rscv_results.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JkbhJMox8Jl9"
   },
   "source": [
    "### Test the best model based on the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZwGk5dR8JmS"
   },
   "outputs": [],
   "source": [
    "y_hat = best_model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E_GNq-R_8JmZ",
    "outputId": "0fc16efe-7739-4394-f1a3-46da92e79584"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = to_categorical(y_hat)\n",
    "y_hat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFqRQ5XB8Jmc",
    "outputId": "a957fbed-92f8-4bdd-d24b-5df3361a8bac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FTWf6Cf-8Jme",
    "outputId": "1256b170-33dc-4171-f2ff-7a3d7ddd74e3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_hat)\n",
    "_ = plt.axvline(x=0.5, color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "93.79246448424954"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import  accuracy_score as score\n",
    "y_pred_classes = y_hat.round()\n",
    "print(y_valid[0], y_hat[0], y_pred_classes[0])\n",
    "acc = score(y_valid, y_pred_classes) * 100\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RNljSx3v8Jmh"
   },
   "outputs": [],
   "source": [
    "pct_auc = roc_auc_score(y_valid, y_hat)*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SO_N9lJG8Jmj",
    "outputId": "20968a52-8ca0-44d3-abf6-a1382bba5cde"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'96.28'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{:0.2f}\".format(pct_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vU3hnA298Jmo"
   },
   "outputs": [],
   "source": [
    "float_y_hat = []\n",
    "for y in y_hat:\n",
    "    float_y_hat.append(y[0:6].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zrTf-aQl8Jmr"
   },
   "outputs": [],
   "source": [
    "ydf = pd.DataFrame(list(zip(float_y_hat, y_valid)), columns=['y_hat', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zAyKh_mX8Jmu",
    "outputId": "611224b8-f980-47df-d596-9aa3a5811009"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_hat</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            y_hat                               y\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
       "1  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n",
       "2  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
       "3  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "4  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
       "5  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
       "6  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
       "7  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
       "8  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "9  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ydf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_lstm_4</th>\n",
       "      <th>n_lstm_3</th>\n",
       "      <th>n_lstm_2</th>\n",
       "      <th>n_lstm_1</th>\n",
       "      <th>n_dense_2</th>\n",
       "      <th>n_dense_1</th>\n",
       "      <th>dropout_2</th>\n",
       "      <th>dropout_1</th>\n",
       "      <th>drop_lstm_4</th>\n",
       "      <th>drop_lstm_3</th>\n",
       "      <th>drop_lstm_2</th>\n",
       "      <th>drop_lstm_1</th>\n",
       "      <th>activation_dense</th>\n",
       "      <th>activation_conv</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.924929</td>\n",
       "      <td>0.000526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>384</td>\n",
       "      <td>256</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.924311</td>\n",
       "      <td>0.001459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>384</td>\n",
       "      <td>256</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>elu</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>0.923316</td>\n",
       "      <td>0.002471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>elu</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.922218</td>\n",
       "      <td>0.000799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>512</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>elu</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.921497</td>\n",
       "      <td>0.003644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>512</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.920382</td>\n",
       "      <td>0.002315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>384</td>\n",
       "      <td>384</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.919386</td>\n",
       "      <td>0.003320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>384</td>\n",
       "      <td>256</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>0.919232</td>\n",
       "      <td>0.002112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>384</td>\n",
       "      <td>512</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>relu</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>0.914359</td>\n",
       "      <td>0.004164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>384</td>\n",
       "      <td>512</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>relu</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>0.897440</td>\n",
       "      <td>0.004250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.893802</td>\n",
       "      <td>0.016487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>elu</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>0.893682</td>\n",
       "      <td>0.007980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>0.892309</td>\n",
       "      <td>0.012539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>512</td>\n",
       "      <td>384</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>relu</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.889358</td>\n",
       "      <td>0.003129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>relu</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.889238</td>\n",
       "      <td>0.011824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>512</td>\n",
       "      <td>384</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.884725</td>\n",
       "      <td>0.001115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>384</td>\n",
       "      <td>512</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.883249</td>\n",
       "      <td>0.005879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>384</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>elu</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.883112</td>\n",
       "      <td>0.007276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>384</td>\n",
       "      <td>384</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.880710</td>\n",
       "      <td>0.003098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>0.868544</td>\n",
       "      <td>0.022940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_lstm_4  n_lstm_3  n_lstm_2  n_lstm_1  n_dense_2  n_dense_1  dropout_2  \\\n",
       "5        256       256       256       128        512        512        0.2   \n",
       "15       128       256       128       256        384        256        0.3   \n",
       "6        256       128       128       256        384        256        0.2   \n",
       "9        256       128       128       256        256        256        0.3   \n",
       "1        256       256       256       128        256        512        0.3   \n",
       "10       256       128       128       256        256        512        0.2   \n",
       "17       256       128       128       256        384        384        0.2   \n",
       "18       256       256       128       256        384        256        0.3   \n",
       "2        256       256       128       256        384        512        0.3   \n",
       "13       256       256       128       128        384        512        0.2   \n",
       "3        128       256       256       128        512        256        0.3   \n",
       "4        128       128       128       128        256        256        0.2   \n",
       "0        256       256       128       256        256        256        0.3   \n",
       "12       256       128       128       128        512        384        0.2   \n",
       "11       256       128       128       128        256        256        0.3   \n",
       "16       128       256       256       256        512        384        0.2   \n",
       "19       256       256       256       256        384        512        0.3   \n",
       "8        256       256       256       128        256        384        0.3   \n",
       "7        256       128       256       256        384        384        0.3   \n",
       "14       128       128       256       256        512        256        0.2   \n",
       "\n",
       "    dropout_1  drop_lstm_4  drop_lstm_3  drop_lstm_2  drop_lstm_1  \\\n",
       "5         0.3         0.02         0.10         0.02         0.02   \n",
       "15        0.2         0.10         0.10         0.02         0.02   \n",
       "6         0.3         0.02         0.10         0.10         0.02   \n",
       "9         0.2         0.10         0.10         0.10         0.02   \n",
       "1         0.2         0.02         0.02         0.10         0.02   \n",
       "10        0.3         0.02         0.10         0.10         0.02   \n",
       "17        0.3         0.02         0.02         0.02         0.02   \n",
       "18        0.3         0.02         0.02         0.02         0.02   \n",
       "2         0.2         0.10         0.10         0.02         0.02   \n",
       "13        0.3         0.02         0.10         0.02         0.10   \n",
       "3         0.2         0.10         0.10         0.02         0.10   \n",
       "4         0.3         0.10         0.02         0.10         0.10   \n",
       "0         0.2         0.02         0.02         0.02         0.10   \n",
       "12        0.3         0.02         0.02         0.10         0.10   \n",
       "11        0.3         0.10         0.10         0.02         0.10   \n",
       "16        0.2         0.10         0.10         0.02         0.10   \n",
       "19        0.2         0.02         0.02         0.02         0.10   \n",
       "8         0.3         0.10         0.02         0.10         0.10   \n",
       "7         0.2         0.02         0.02         0.02         0.10   \n",
       "14        0.2         0.02         0.10         0.02         0.10   \n",
       "\n",
       "                                     activation_dense  \\\n",
       "5   <tensorflow.python.keras.layers.advanced_activ...   \n",
       "15  <tensorflow.python.keras.layers.advanced_activ...   \n",
       "6                                                 elu   \n",
       "9                                                 elu   \n",
       "1                                                 elu   \n",
       "10  <tensorflow.python.keras.layers.advanced_activ...   \n",
       "17                                               relu   \n",
       "18  <tensorflow.python.keras.layers.advanced_activ...   \n",
       "2                                                relu   \n",
       "13                                               relu   \n",
       "3   <tensorflow.python.keras.layers.advanced_activ...   \n",
       "4                                                 elu   \n",
       "0   <tensorflow.python.keras.layers.advanced_activ...   \n",
       "12                                               relu   \n",
       "11                                               relu   \n",
       "16  <tensorflow.python.keras.layers.advanced_activ...   \n",
       "19  <tensorflow.python.keras.layers.advanced_activ...   \n",
       "8                                                 elu   \n",
       "7   <tensorflow.python.keras.layers.advanced_activ...   \n",
       "14  <tensorflow.python.keras.layers.advanced_activ...   \n",
       "\n",
       "                                      activation_conv      mean       std  \n",
       "5                                                 elu  0.924929  0.000526  \n",
       "15                                                elu  0.924311  0.001459  \n",
       "6   <tensorflow.python.keras.layers.advanced_activ...  0.923316  0.002471  \n",
       "9                                                relu  0.922218  0.000799  \n",
       "1                                                relu  0.921497  0.003644  \n",
       "10                                               relu  0.920382  0.002315  \n",
       "17                                               relu  0.919386  0.003320  \n",
       "18  <tensorflow.python.keras.layers.advanced_activ...  0.919232  0.002112  \n",
       "2   <tensorflow.python.keras.layers.advanced_activ...  0.914359  0.004164  \n",
       "13  <tensorflow.python.keras.layers.advanced_activ...  0.897440  0.004250  \n",
       "3                                                 elu  0.893802  0.016487  \n",
       "4   <tensorflow.python.keras.layers.advanced_activ...  0.893682  0.007980  \n",
       "0   <tensorflow.python.keras.layers.advanced_activ...  0.892309  0.012539  \n",
       "12                                                elu  0.889358  0.003129  \n",
       "11                                                elu  0.889238  0.011824  \n",
       "16                                                elu  0.884725  0.001115  \n",
       "19                                               relu  0.883249  0.005879  \n",
       "8                                                relu  0.883112  0.007276  \n",
       "7                                                relu  0.880710  0.003098  \n",
       "14  <tensorflow.python.keras.layers.advanced_activ...  0.868544  0.022940  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(rscv_results.cv_results_['params'])\n",
    "results_df['mean'] = rscv_results.cv_results_['mean_test_score']\n",
    "results_df['std'] = rscv_results.cv_results_['std_test_score']\n",
    "results_df.sort_values('mean', ascending=False, ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating best model from the best param and train it for 60 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_lstm_4': 256,\n",
       " 'n_lstm_3': 256,\n",
       " 'n_lstm_2': 256,\n",
       " 'n_lstm_1': 128,\n",
       " 'n_dense_2': 512,\n",
       " 'n_dense_1': 512,\n",
       " 'dropout_2': 0.2,\n",
       " 'dropout_1': 0.3,\n",
       " 'drop_lstm_4': 0.02,\n",
       " 'drop_lstm_3': 0.1,\n",
       " 'drop_lstm_2': 0.02,\n",
       " 'drop_lstm_1': 0.02,\n",
       " 'activation_dense': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU at 0x1b6fb510788>,\n",
       " 'activation_conv': 'elu'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rscv_results.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lstm_4 = 256\n",
    "n_lstm_3 = 256\n",
    "n_lstm_2 = 256\n",
    "n_lstm_1 = 128\n",
    "n_dense_2 = 512\n",
    "n_dense_1 = 512\n",
    "dropout_2 = 0.2\n",
    "dropout_1 = 0.3\n",
    "drop_lstm_4 = 0.02\n",
    "drop_lstm_3 = 0.1\n",
    "drop_lstm_2 = 0.02\n",
    "drop_lstm_1 = 0.02\n",
    "activation_dense = LeakyReLU()\n",
    "activation_conv = 'elu'\n",
    "model = Sequential()\n",
    "# LSTM layers\n",
    "model.add(LSTM(n_lstm_1, dropout=drop_lstm_1, \n",
    "                         return_sequences=True, input_shape=input_shape[1:])) \n",
    "model.add(LSTM(n_lstm_2, dropout=drop_lstm_2, \n",
    "                         return_sequences=True))\n",
    "model.add(LSTM(n_lstm_3, dropout=drop_lstm_3, \n",
    "                         return_sequences=True)) \n",
    "model.add(LSTM(n_lstm_4, dropout=drop_lstm_4, \n",
    "                         return_sequences=True))\n",
    "\n",
    "# Dense layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(n_dense_1, activation=activation_dense))\n",
    "model.add(Dropout(dropout_1))\n",
    "model.add(Dense(n_dense_2, activation=activation_dense))\n",
    "model.add(Dropout(dropout_2))\n",
    "model.add(Dense(n_class, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir= 'model_ouput/Final-LSTM'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "modelcheckpoint = ModelCheckpoint(filepath=output_dir+\n",
    "                                  \"/weights.{epoch:02d}.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "228/228 [==============================] - 36s 157ms/step - loss: 1.1734 - accuracy: 0.4994 - val_loss: 0.9245 - val_accuracy: 0.6177\n",
      "Epoch 2/120\n",
      "228/228 [==============================] - 35s 156ms/step - loss: 0.7879 - accuracy: 0.6698 - val_loss: 0.6506 - val_accuracy: 0.7326\n",
      "Epoch 3/120\n",
      "228/228 [==============================] - 36s 156ms/step - loss: 0.6340 - accuracy: 0.7362 - val_loss: 0.5323 - val_accuracy: 0.7925\n",
      "Epoch 4/120\n",
      "228/228 [==============================] - 36s 157ms/step - loss: 0.5332 - accuracy: 0.7819 - val_loss: 0.4690 - val_accuracy: 0.8150\n",
      "Epoch 5/120\n",
      "228/228 [==============================] - 36s 157ms/step - loss: 0.4703 - accuracy: 0.8114 - val_loss: 0.4224 - val_accuracy: 0.8380\n",
      "Epoch 6/120\n",
      "228/228 [==============================] - 36s 158ms/step - loss: 0.4292 - accuracy: 0.8309 - val_loss: 0.4012 - val_accuracy: 0.8400\n",
      "Epoch 7/120\n",
      "228/228 [==============================] - 36s 158ms/step - loss: 0.3956 - accuracy: 0.8467 - val_loss: 0.3715 - val_accuracy: 0.8528\n",
      "Epoch 8/120\n",
      "223/228 [============================>.] - ETA: 0s - loss: 0.3656 - accuracy: 0.8573"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-36c0d513cf16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m          \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m          \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m          callbacks=[modelcheckpoint])\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 120\n",
    "model.fit(X_train, y_train, \n",
    "         batch_size=batch_size, epochs=epochs, verbose=1, \n",
    "         validation_data=(X_valid, y_valid), \n",
    "         callbacks=[modelcheckpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z8t0nVCw8JmP"
   },
   "outputs": [],
   "source": [
    "model.load_weights(output_dir+\"/weights.100.hdf5\") # 96.32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZwGk5dR8JmS"
   },
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EPc5_h6K8JmW",
    "outputId": "cad99da9-9f89-437f-854b-a315616ed50f"
   },
   "outputs": [],
   "source": [
    "len(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E_GNq-R_8JmZ",
    "outputId": "0fc16efe-7739-4394-f1a3-46da92e79584"
   },
   "outputs": [],
   "source": [
    "y_hat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFqRQ5XB8Jmc",
    "outputId": "a957fbed-92f8-4bdd-d24b-5df3361a8bac"
   },
   "outputs": [],
   "source": [
    "y_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FTWf6Cf-8Jme",
    "outputId": "1256b170-33dc-4171-f2ff-7a3d7ddd74e3"
   },
   "outputs": [],
   "source": [
    "plt.hist(y_hat)\n",
    "_ = plt.axvline(x=0.5, color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RNljSx3v8Jmh"
   },
   "outputs": [],
   "source": [
    "pct_auc = roc_auc_score(y_valid, y_hat)*100.0\n",
    "\"{:0.2f}\".format(pct_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SO_N9lJG8Jmj",
    "outputId": "20968a52-8ca0-44d3-abf6-a1382bba5cde"
   },
   "outputs": [],
   "source": [
    "y_pred_classes = y_hat.round()\n",
    "acc = score(y_valid, y_pred_classes) * 100\n",
    "\"{:0.2f}\".format(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vU3hnA298Jmo"
   },
   "outputs": [],
   "source": [
    "float_y_hat = []\n",
    "for y in y_hat:\n",
    "    float_y_hat.append(y[0:6].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zrTf-aQl8Jmr"
   },
   "outputs": [],
   "source": [
    "ydf = pd.DataFrame(list(zip(float_y_hat, y_valid)), columns=['y_hat', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zAyKh_mX8Jmu",
    "outputId": "611224b8-f980-47df-d596-9aa3a5811009"
   },
   "outputs": [],
   "source": [
    "ydf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "dense_sentiment_classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
