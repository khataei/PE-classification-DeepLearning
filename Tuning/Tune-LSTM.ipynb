{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ahUZuAfz8Jje"
   },
   "source": [
    "# Sklearn Tuner for LSTM Activity Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Re0ecmVI8Jjk"
   },
   "source": [
    "In this notebook, we use SKlearn  to tune a LSTM neural net to classify PE activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ixsGb9tY8Jjm"
   },
   "source": [
    "#### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5At1PKQp8Jjp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1276608612805494057\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 12984453876963605300\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4022219571\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 4408072045768815224\n",
      "physical_device_desc: \"device: 0, name: Quadro P2000, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 13974246354186393395\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n",
      "# GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv1D, GlobalMaxPooling1D, LSTM\n",
    "from tensorflow.keras.layers import AveragePooling1D, LeakyReLU , MaxPool1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint \n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "import tensorflow as tf\n",
    "print(\"# GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "import sklearn\n",
    "# bug in sklearn wrapper\n",
    "#https://stackoverflow.com/questions/59746974/cannot-clone-object-tensorflow-python-keras-wrappers-scikit-learn-kerasclassifi\n",
    "if sklearn.__version__ != '0.21.2':\n",
    "    print(\"updating sklearn ...\")\n",
    "    !pip install --user scikit-learn==0.21.2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hg5wX2Dd8Jjz"
   },
   "source": [
    "#### Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lYLOM99-8Jj2"
   },
   "outputs": [],
   "source": [
    "# output directory name:\n",
    "output_dir = 'model_output/tune-lstm'\n",
    "input_dir =  'Z:/Research/dfuller/Walkabilly/studies/smarphone_accel/data/Ethica_Jaeger_Merged/pocket/'\n",
    "input_file_name = 'pocket-NN-data.npz'\n",
    "\n",
    "# from the data preparation section we have:\n",
    "window_size_second = 3\n",
    "frequency = 30\n",
    "lenght_of_each_seq = window_size_second * frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn hyperparams\n",
    "params = {\n",
    "    \n",
    "    # LSTM layers\n",
    "    'n_lstm_1' : [128, 256],\n",
    "    'n_lstm_2' : [128, 256],\n",
    "    'n_lstm_3' : [128, 256],\n",
    "    'n_lstm_4' : [128, 256],\n",
    "    'drop_lstm_1' : [0.02,0.1],\n",
    "    'drop_lstm_2' : [0.02,0.1],\n",
    "    'drop_lstm_3' : [0.02,0.1],\n",
    "    'drop_lstm_4' : [0.02,0.1],\n",
    "\n",
    "    \n",
    "    # Dense layers\n",
    "    'n_dense_1':[256, 384, 512],\n",
    "    'dropout_1':[0.2, 0.3],\n",
    "    'n_dense_2':[256, 384, 512],\n",
    "    'dropout_2':[0.2, 0.3],\n",
    "    'activation_conv':['elu', 'relu', LeakyReLU()],\n",
    "    'activation_dense':['elu', 'relu', LeakyReLU()]\n",
    "}\n",
    "\n",
    "# training:\n",
    "n_tune_iter = 20\n",
    "cv = 3\n",
    "epochs = 60\n",
    "batch_size = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to get reproducable results\n",
    "from numpy.random import seed\n",
    "seed(85)\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DaLx4yZ48Jj9"
   },
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CD9X0yJg8Jj_"
   },
   "source": [
    "##### For this notebook we use the acceleration data gathered from the pocket location. It was prepared in the DataPrep-Deep notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6r44JKj-8JkA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acceleration_data\n",
      "metadata\n",
      "labels\n"
     ]
    }
   ],
   "source": [
    "# read the raw file and get the keys:\n",
    "raw_data = np.load(file=input_dir+input_file_name,allow_pickle=True)\n",
    "for k in raw_data.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "\n",
    "accel_array = raw_data['acceleration_data']\n",
    "meta_array = raw_data['metadata']\n",
    "labels_array = raw_data['labels']\n",
    "input_shape = list(accel_array.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WWq3ipbu8JlG"
   },
   "source": [
    "#### Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the  labels to integer.\n",
    "In the raw data format of the labels is String and there are 6 classes. 'Lying', 'Sitting', 'Self Pace walk', 'Running 3 METs',\n",
    "       'Running 5 METs', 'Running 7 METs' <br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LyReiequ8Jln"
   },
   "outputs": [],
   "source": [
    "n_class = len(np.unique(labels_array))\n",
    "class_list, labels_array_int = np.unique(labels_array,return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels_array_int, num_classes=n_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "40x3s0KY8Jlv",
    "outputId": "d3ac3e31-3c47-4b01-c2f9-ada0684b31b7"
   },
   "source": [
    "### Splitting and shuffeling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f5grc2H78Jlz",
    "outputId": "3f7eeb55-f851-4796-de4e-0c68eb9df88d"
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "     accel_array, y, test_size=0.1, random_state=65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "GscSnCpk8Jlm"
   },
   "source": [
    "\n",
    "#### Design neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_lstm_1': [128, 256],\n",
       " 'n_lstm_2': [128, 256],\n",
       " 'n_lstm_3': [128, 256],\n",
       " 'n_lstm_4': [128, 256],\n",
       " 'drop_lstm_1': [0.02, 0.1],\n",
       " 'drop_lstm_2': [0.02, 0.1],\n",
       " 'drop_lstm_3': [0.02, 0.1],\n",
       " 'drop_lstm_4': [0.02, 0.1],\n",
       " 'n_dense_1': [256, 384, 512],\n",
       " 'dropout_1': [0.2, 0.3],\n",
       " 'n_dense_2': [256, 384, 512],\n",
       " 'dropout_2': [0.2, 0.3],\n",
       " 'activation_conv': ['elu',\n",
       "  'relu',\n",
       "  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU at 0x1b6fb510688>],\n",
       " 'activation_dense': ['elu',\n",
       "  'relu',\n",
       "  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU at 0x1b6fb510788>]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model( n_lstm_1=128, n_lstm_2=128, n_lstm_3=128, n_lstm_4=128, drop_lstm_1=0.02, drop_lstm_2=0.02,\n",
    "                 drop_lstm_3=0.02,drop_lstm_4=0.02, n_dense_1=256, dropout_1=0.2,\n",
    "                 n_dense_2=256, dropout_2=0.2, activation_conv= 'relu', activation_dense= 'elu'\n",
    "                ):\n",
    "    model = Sequential()\n",
    "    # LSTM layers\n",
    "    model.add(LSTM(n_lstm_1, dropout=drop_lstm_1, \n",
    "                             return_sequences=True, input_shape=input_shape[1:])) \n",
    "    model.add(LSTM(n_lstm_2, dropout=drop_lstm_2, \n",
    "                             return_sequences=True))\n",
    "    model.add(LSTM(n_lstm_3, dropout=drop_lstm_3, \n",
    "                             return_sequences=True)) \n",
    "    model.add(LSTM(n_lstm_4, dropout=drop_lstm_4, \n",
    "                             return_sequences=True))\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(n_dense_1, activation=activation_dense))\n",
    "    model.add(Dropout(dropout_1))\n",
    "    model.add(Dense(n_dense_2, activation=activation_dense))\n",
    "    model.add(Dropout(dropout_2))\n",
    "    model.add(Dense(n_class, activation='softmax'))\n",
    "    model.summary()\n",
    "    print(f\"\"\"\n",
    "          n_lstm_1 = {n_lstm_1}, n_lstm_2 = {n_lstm_2}, n_lstm_3 = {n_lstm_3}, n_lstm_4 = {n_lstm_4},\n",
    "          drop_lstm_1 = {drop_lstm_1}, drop_lstm_2 = {drop_lstm_2},\n",
    "          drop_lstm_3 = {drop_lstm_3}, drop_lstm_4 = {drop_lstm_4},\n",
    "          n_dense_1 = {n_dense_1}, dropout_1 = {dropout_1},\n",
    "          n_dense_2 = {n_dense_2}, dropout_2 = {dropout_2}, activation_conv=  {activation_conv},\n",
    "          activation_dense=  {activation_dense}\"\"\")\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_16 (LSTM)               (None, 90, 128)           67584     \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 90, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 11520)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 256)               2949376   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 3,479,046\n",
      "Trainable params: 3,479,046\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 128, n_lstm_2 = 128, n_lstm_3 = 128, n_lstm_4 = 128,\n",
      "          drop_lstm_1 = 0.02, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 256, dropout_2 = 0.2, activation_conv=  relu,\n",
      "          activation_dense=  elu\n"
     ]
    }
   ],
   "source": [
    "model_default = create_model()\n",
    "# model_default.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_20 (LSTM)               (None, 90, 256)           266240    \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 90, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 90, 256)           394240    \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, 90, 256)           525312    \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 256)               5898496   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 7,348,742\n",
      "Trainable params: 7,348,742\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "          n_lstm_1 = 256, n_lstm_2 = 128, n_lstm_3 = 256, n_lstm_4 = 256,\n",
      "          drop_lstm_1 = 0.1, drop_lstm_2 = 0.02,\n",
      "          drop_lstm_3 = 0.02, drop_lstm_4 = 0.02,\n",
      "          n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 256, dropout_2 = 0.3, activation_conv=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510688>,\n",
      "          activation_dense=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B6FB510788>\n",
      "Epoch 1/60\n",
      "152/152 [==============================] - 21s 139ms/step - loss: 1.2610 - accuracy: 0.4689\n",
      "Epoch 2/60\n",
      "152/152 [==============================] - 21s 139ms/step - loss: 0.9199 - accuracy: 0.6081\n",
      "Epoch 3/60\n",
      "152/152 [==============================] - 21s 139ms/step - loss: 0.8157 - accuracy: 0.6527\n",
      "Epoch 4/60\n",
      "152/152 [==============================] - 22s 142ms/step - loss: 0.7361 - accuracy: 0.6924\n",
      "Epoch 5/60\n",
      " 42/152 [=======>......................] - ETA: 15s - loss: 0.6813 - accuracy: 0.7148"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rscv = RandomizedSearchCV(model, param_distributions=params, cv=cv, n_iter=n_tune_iter)\n",
    "rscv_results = rscv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best score is: {} using {}'.format(rscv_results.best_score_,\n",
    "rscv_results.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = rscv_results.best_estimator_\n",
    "best_param = rscv_results.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JkbhJMox8Jl9"
   },
   "source": [
    "### Test the best model based on the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZwGk5dR8JmS"
   },
   "outputs": [],
   "source": [
    "y_hat = best_model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E_GNq-R_8JmZ",
    "outputId": "0fc16efe-7739-4394-f1a3-46da92e79584"
   },
   "outputs": [],
   "source": [
    "y_hat = to_categorical(y_hat)\n",
    "y_hat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFqRQ5XB8Jmc",
    "outputId": "a957fbed-92f8-4bdd-d24b-5df3361a8bac"
   },
   "outputs": [],
   "source": [
    "y_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FTWf6Cf-8Jme",
    "outputId": "1256b170-33dc-4171-f2ff-7a3d7ddd74e3"
   },
   "outputs": [],
   "source": [
    "plt.hist(y_hat)\n",
    "_ = plt.axvline(x=0.5, color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  accuracy_score as score\n",
    "y_pred_classes = y_hat.round()\n",
    "print(y_valid[0], y_hat[0], y_pred_classes[0])\n",
    "acc = score(y_valid, y_pred_classes) * 100\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RNljSx3v8Jmh"
   },
   "outputs": [],
   "source": [
    "pct_auc = roc_auc_score(y_valid, y_hat)*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SO_N9lJG8Jmj",
    "outputId": "20968a52-8ca0-44d3-abf6-a1382bba5cde"
   },
   "outputs": [],
   "source": [
    "\"{:0.2f}\".format(pct_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vU3hnA298Jmo"
   },
   "outputs": [],
   "source": [
    "float_y_hat = []\n",
    "for y in y_hat:\n",
    "    float_y_hat.append(y[0:6].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zrTf-aQl8Jmr"
   },
   "outputs": [],
   "source": [
    "ydf = pd.DataFrame(list(zip(float_y_hat, y_valid)), columns=['y_hat', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zAyKh_mX8Jmu",
    "outputId": "611224b8-f980-47df-d596-9aa3a5811009"
   },
   "outputs": [],
   "source": [
    "ydf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(rscv_results.cv_results_['params'])\n",
    "results_df['mean'] = rscv_results.cv_results_['mean_test_score']\n",
    "results_df['std'] = rscv_results.cv_results_['std_test_score']\n",
    "results_df.sort_values('mean', ascending=False, ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating best model from the best param and train it for 60 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rscv_results.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lstm_2 = 128\n",
    "n_lstm_1 = 128\n",
    "n_dense_2= 384\n",
    "n_dense_1= 384\n",
    "n_conv_3= 512\n",
    "n_conv_2= 512\n",
    "n_conv_1= 768\n",
    "maxpooling_pool_size= 2\n",
    "k_conv_3= 2\n",
    "k_conv_2= 2\n",
    "k_conv_1= 2\n",
    "dropout_2= 0.2\n",
    "dropout_1= 0.2\n",
    "drop_lstm_2 = 0.02\n",
    "drop_lstm_1 = 0.1\n",
    "avepooling_pool_size= 2\n",
    "activation_dense = LeakyReLU()\n",
    "activation_conv = 'relu'\n",
    "model = Sequential()\n",
    "model.add(Conv1D(n_conv_1, k_conv_1, activation=activation_conv, input_shape=input_shape[1:]))\n",
    "model.add(MaxPool1D(pool_size = maxpooling_pool_size))\n",
    "model.add(Conv1D(n_conv_2, k_conv_2, activation=activation_conv))\n",
    "model.add(AveragePooling1D(pool_size = avepooling_pool_size))\n",
    "model.add(Conv1D(n_conv_3, k_conv_3, activation=activation_conv))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# LSTM layers\n",
    "model.add(LSTM(n_lstm_1, dropout=drop_lstm_1, return_sequences=True)) \n",
    "model.add(LSTM(n_lstm_2, dropout=drop_lstm_2, return_sequences=True))\n",
    "\n",
    "# Dense layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(n_dense_1, activation=activation_dense))\n",
    "model.add(Dropout(dropout_1))\n",
    "model.add(Dense(n_dense_2, activation=activation_dense))\n",
    "model.add(Dropout(dropout_2))\n",
    "model.add(Dense(n_class, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir= 'model_ouput/Final-LSTM'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "modelcheckpoint = ModelCheckpoint(filepath=output_dir+\n",
    "                                  \"/weights.{epoch:02d}.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "228/228 [==============================] - 17s 76ms/step - loss: 0.9483 - accuracy: 0.5963 - val_loss: 0.6727 - val_accuracy: 0.7288\n",
      "Epoch 2/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.5682 - accuracy: 0.7747 - val_loss: 0.4335 - val_accuracy: 0.8223\n",
      "Epoch 3/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.4172 - accuracy: 0.8399 - val_loss: 0.3706 - val_accuracy: 0.8562\n",
      "Epoch 4/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.3481 - accuracy: 0.8662 - val_loss: 0.3244 - val_accuracy: 0.8780\n",
      "Epoch 5/120\n",
      "228/228 [==============================] - 16s 71ms/step - loss: 0.3166 - accuracy: 0.8762 - val_loss: 0.3082 - val_accuracy: 0.8779\n",
      "Epoch 6/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.2983 - accuracy: 0.8836 - val_loss: 0.2678 - val_accuracy: 0.9015\n",
      "Epoch 7/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.2806 - accuracy: 0.8920 - val_loss: 0.2774 - val_accuracy: 0.8843\n",
      "Epoch 8/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.2595 - accuracy: 0.9011 - val_loss: 0.2631 - val_accuracy: 0.8968\n",
      "Epoch 9/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.2495 - accuracy: 0.9045 - val_loss: 0.2591 - val_accuracy: 0.8989\n",
      "Epoch 10/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.2394 - accuracy: 0.9090 - val_loss: 0.2438 - val_accuracy: 0.8999\n",
      "Epoch 11/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.2255 - accuracy: 0.9136 - val_loss: 0.2386 - val_accuracy: 0.9035\n",
      "Epoch 12/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.2172 - accuracy: 0.9177 - val_loss: 0.2402 - val_accuracy: 0.9155\n",
      "Epoch 13/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.2083 - accuracy: 0.9213 - val_loss: 0.2091 - val_accuracy: 0.9209\n",
      "Epoch 14/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.2079 - accuracy: 0.9215 - val_loss: 0.2394 - val_accuracy: 0.9143\n",
      "Epoch 15/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.1950 - accuracy: 0.9250 - val_loss: 0.2063 - val_accuracy: 0.9209\n",
      "Epoch 16/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.1813 - accuracy: 0.9301 - val_loss: 0.2063 - val_accuracy: 0.9237\n",
      "Epoch 17/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.1816 - accuracy: 0.9317 - val_loss: 0.2138 - val_accuracy: 0.9219\n",
      "Epoch 18/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.1715 - accuracy: 0.9330 - val_loss: 0.2032 - val_accuracy: 0.9250\n",
      "Epoch 19/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.1702 - accuracy: 0.9342 - val_loss: 0.2266 - val_accuracy: 0.9229\n",
      "Epoch 20/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.1648 - accuracy: 0.9358 - val_loss: 0.2009 - val_accuracy: 0.9296\n",
      "Epoch 21/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.1564 - accuracy: 0.9389 - val_loss: 0.1808 - val_accuracy: 0.9322\n",
      "Epoch 22/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.1502 - accuracy: 0.9417 - val_loss: 0.1840 - val_accuracy: 0.9330\n",
      "Epoch 23/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.1441 - accuracy: 0.9443 - val_loss: 0.1960 - val_accuracy: 0.9338\n",
      "Epoch 24/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.1387 - accuracy: 0.9463 - val_loss: 0.1998 - val_accuracy: 0.9287\n",
      "Epoch 25/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.1410 - accuracy: 0.9457 - val_loss: 0.1743 - val_accuracy: 0.9381\n",
      "Epoch 26/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.1295 - accuracy: 0.9512 - val_loss: 0.1902 - val_accuracy: 0.9348\n",
      "Epoch 27/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.1310 - accuracy: 0.9501 - val_loss: 0.1927 - val_accuracy: 0.9367\n",
      "Epoch 28/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.1210 - accuracy: 0.9539 - val_loss: 0.1776 - val_accuracy: 0.9484\n",
      "Epoch 29/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.1200 - accuracy: 0.9552 - val_loss: 0.1889 - val_accuracy: 0.9407\n",
      "Epoch 30/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.1190 - accuracy: 0.9547 - val_loss: 0.2013 - val_accuracy: 0.9402\n",
      "Epoch 31/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.1091 - accuracy: 0.9589 - val_loss: 0.1715 - val_accuracy: 0.9478\n",
      "Epoch 32/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.1042 - accuracy: 0.9608 - val_loss: 0.1715 - val_accuracy: 0.9455\n",
      "Epoch 33/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.1044 - accuracy: 0.9612 - val_loss: 0.1766 - val_accuracy: 0.9483\n",
      "Epoch 34/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0925 - accuracy: 0.9661 - val_loss: 0.1864 - val_accuracy: 0.9492\n",
      "Epoch 35/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0957 - accuracy: 0.9640 - val_loss: 0.1754 - val_accuracy: 0.9504\n",
      "Epoch 36/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0902 - accuracy: 0.9658 - val_loss: 0.1852 - val_accuracy: 0.9520\n",
      "Epoch 37/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0943 - accuracy: 0.9650 - val_loss: 0.1861 - val_accuracy: 0.9464\n",
      "Epoch 38/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0917 - accuracy: 0.9665 - val_loss: 0.1943 - val_accuracy: 0.9416\n",
      "Epoch 39/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0812 - accuracy: 0.9703 - val_loss: 0.2083 - val_accuracy: 0.9507\n",
      "Epoch 40/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0854 - accuracy: 0.9694 - val_loss: 0.2089 - val_accuracy: 0.9507\n",
      "Epoch 41/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0821 - accuracy: 0.9694 - val_loss: 0.1982 - val_accuracy: 0.9518\n",
      "Epoch 42/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0831 - accuracy: 0.9685 - val_loss: 0.1893 - val_accuracy: 0.9506\n",
      "Epoch 43/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0802 - accuracy: 0.9710 - val_loss: 0.1925 - val_accuracy: 0.9526\n",
      "Epoch 44/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0761 - accuracy: 0.9721 - val_loss: 0.1979 - val_accuracy: 0.9532\n",
      "Epoch 45/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0740 - accuracy: 0.9731 - val_loss: 0.2072 - val_accuracy: 0.9412\n",
      "Epoch 46/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0748 - accuracy: 0.9726 - val_loss: 0.2193 - val_accuracy: 0.9527\n",
      "Epoch 47/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0661 - accuracy: 0.9750 - val_loss: 0.2375 - val_accuracy: 0.9470\n",
      "Epoch 48/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0676 - accuracy: 0.9753 - val_loss: 0.2191 - val_accuracy: 0.9540\n",
      "Epoch 49/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0706 - accuracy: 0.9741 - val_loss: 0.1953 - val_accuracy: 0.9531\n",
      "Epoch 50/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0662 - accuracy: 0.9755 - val_loss: 0.2222 - val_accuracy: 0.9509\n",
      "Epoch 51/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0706 - accuracy: 0.9738 - val_loss: 0.1913 - val_accuracy: 0.9535\n",
      "Epoch 52/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0726 - accuracy: 0.9731 - val_loss: 0.2042 - val_accuracy: 0.9552\n",
      "Epoch 53/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0605 - accuracy: 0.9775 - val_loss: 0.2448 - val_accuracy: 0.9460\n",
      "Epoch 54/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0588 - accuracy: 0.9785 - val_loss: 0.2127 - val_accuracy: 0.9557\n",
      "Epoch 55/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0625 - accuracy: 0.9772 - val_loss: 0.2181 - val_accuracy: 0.9552\n",
      "Epoch 56/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0608 - accuracy: 0.9773 - val_loss: 0.2111 - val_accuracy: 0.9523\n",
      "Epoch 57/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0578 - accuracy: 0.9786 - val_loss: 0.2178 - val_accuracy: 0.9565\n",
      "Epoch 58/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0519 - accuracy: 0.9804 - val_loss: 0.2299 - val_accuracy: 0.9557\n",
      "Epoch 59/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0546 - accuracy: 0.9808 - val_loss: 0.2110 - val_accuracy: 0.9565\n",
      "Epoch 60/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0543 - accuracy: 0.9801 - val_loss: 0.2321 - val_accuracy: 0.9546\n",
      "Epoch 61/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0536 - accuracy: 0.9807 - val_loss: 0.2144 - val_accuracy: 0.9577\n",
      "Epoch 62/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0532 - accuracy: 0.9810 - val_loss: 0.2299 - val_accuracy: 0.9506\n",
      "Epoch 63/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0539 - accuracy: 0.9802 - val_loss: 0.2168 - val_accuracy: 0.9543\n",
      "Epoch 64/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.0556 - accuracy: 0.9805 - val_loss: 0.2102 - val_accuracy: 0.9580\n",
      "Epoch 65/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0499 - accuracy: 0.9820 - val_loss: 0.2262 - val_accuracy: 0.9599\n",
      "Epoch 66/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0470 - accuracy: 0.9819 - val_loss: 0.2279 - val_accuracy: 0.9554\n",
      "Epoch 67/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0476 - accuracy: 0.9819 - val_loss: 0.2566 - val_accuracy: 0.9527\n",
      "Epoch 68/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0567 - accuracy: 0.9793 - val_loss: 0.2197 - val_accuracy: 0.9589\n",
      "Epoch 69/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0497 - accuracy: 0.9819 - val_loss: 0.2410 - val_accuracy: 0.9558\n",
      "Epoch 70/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0428 - accuracy: 0.9840 - val_loss: 0.2516 - val_accuracy: 0.9569\n",
      "Epoch 71/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0499 - accuracy: 0.9829 - val_loss: 0.2203 - val_accuracy: 0.9560\n",
      "Epoch 72/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0419 - accuracy: 0.9843 - val_loss: 0.2488 - val_accuracy: 0.9619\n",
      "Epoch 73/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0484 - accuracy: 0.9830 - val_loss: 0.2079 - val_accuracy: 0.9578\n",
      "Epoch 74/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0433 - accuracy: 0.9844 - val_loss: 0.2202 - val_accuracy: 0.9608\n",
      "Epoch 75/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0401 - accuracy: 0.9855 - val_loss: 0.2522 - val_accuracy: 0.9551\n",
      "Epoch 76/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0476 - accuracy: 0.9832 - val_loss: 0.2305 - val_accuracy: 0.9582\n",
      "Epoch 77/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0429 - accuracy: 0.9841 - val_loss: 0.2350 - val_accuracy: 0.9565\n",
      "Epoch 78/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0417 - accuracy: 0.9850 - val_loss: 0.2299 - val_accuracy: 0.9609\n",
      "Epoch 79/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0422 - accuracy: 0.9845 - val_loss: 0.2334 - val_accuracy: 0.9577\n",
      "Epoch 80/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0424 - accuracy: 0.9846 - val_loss: 0.2443 - val_accuracy: 0.9548\n",
      "Epoch 81/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0456 - accuracy: 0.9840 - val_loss: 0.2202 - val_accuracy: 0.9586\n",
      "Epoch 82/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0383 - accuracy: 0.9863 - val_loss: 0.2507 - val_accuracy: 0.9565\n",
      "Epoch 83/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0410 - accuracy: 0.9854 - val_loss: 0.2387 - val_accuracy: 0.9555\n",
      "Epoch 84/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0457 - accuracy: 0.9844 - val_loss: 0.2194 - val_accuracy: 0.9608\n",
      "Epoch 85/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0360 - accuracy: 0.9872 - val_loss: 0.2385 - val_accuracy: 0.9591\n",
      "Epoch 86/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0359 - accuracy: 0.9869 - val_loss: 0.2499 - val_accuracy: 0.9540\n",
      "Epoch 87/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0393 - accuracy: 0.9858 - val_loss: 0.2385 - val_accuracy: 0.9551\n",
      "Epoch 88/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0363 - accuracy: 0.9873 - val_loss: 0.2362 - val_accuracy: 0.9586\n",
      "Epoch 89/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0436 - accuracy: 0.9846 - val_loss: 0.2398 - val_accuracy: 0.9591\n",
      "Epoch 90/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0330 - accuracy: 0.9883 - val_loss: 0.2629 - val_accuracy: 0.9582\n",
      "Epoch 91/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0354 - accuracy: 0.9868 - val_loss: 0.2589 - val_accuracy: 0.9586\n",
      "Epoch 92/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0362 - accuracy: 0.9873 - val_loss: 0.2473 - val_accuracy: 0.9588\n",
      "Epoch 93/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0390 - accuracy: 0.9865 - val_loss: 0.2449 - val_accuracy: 0.9605\n",
      "Epoch 94/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0364 - accuracy: 0.9869 - val_loss: 0.2631 - val_accuracy: 0.9606\n",
      "Epoch 95/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0332 - accuracy: 0.9879 - val_loss: 0.2685 - val_accuracy: 0.9603\n",
      "Epoch 96/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0385 - accuracy: 0.9872 - val_loss: 0.2206 - val_accuracy: 0.9606\n",
      "Epoch 97/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.0332 - accuracy: 0.9880 - val_loss: 0.2560 - val_accuracy: 0.9580\n",
      "Epoch 98/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0389 - accuracy: 0.9866 - val_loss: 0.2303 - val_accuracy: 0.9578\n",
      "Epoch 99/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0340 - accuracy: 0.9873 - val_loss: 0.2926 - val_accuracy: 0.9527\n",
      "Epoch 100/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.0331 - accuracy: 0.9880 - val_loss: 0.2311 - val_accuracy: 0.9632\n",
      "Epoch 101/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0297 - accuracy: 0.9895 - val_loss: 0.2462 - val_accuracy: 0.9629\n",
      "Epoch 102/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.0342 - accuracy: 0.9881 - val_loss: 0.2753 - val_accuracy: 0.9591\n",
      "Epoch 103/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0323 - accuracy: 0.9886 - val_loss: 0.2463 - val_accuracy: 0.9619\n",
      "Epoch 104/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0329 - accuracy: 0.9883 - val_loss: 0.2729 - val_accuracy: 0.9612\n",
      "Epoch 105/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0302 - accuracy: 0.9890 - val_loss: 0.2690 - val_accuracy: 0.9612\n",
      "Epoch 106/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0332 - accuracy: 0.9883 - val_loss: 0.2667 - val_accuracy: 0.9575\n",
      "Epoch 107/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.0307 - accuracy: 0.9892 - val_loss: 0.2456 - val_accuracy: 0.9614\n",
      "Epoch 108/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.0271 - accuracy: 0.9898 - val_loss: 0.2922 - val_accuracy: 0.9606\n",
      "Epoch 109/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.0356 - accuracy: 0.9877 - val_loss: 0.2592 - val_accuracy: 0.9631\n",
      "Epoch 110/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.0267 - accuracy: 0.9909 - val_loss: 0.2925 - val_accuracy: 0.9606\n",
      "Epoch 111/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.0293 - accuracy: 0.9894 - val_loss: 0.2832 - val_accuracy: 0.9546\n",
      "Epoch 112/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.0333 - accuracy: 0.9890 - val_loss: 0.2531 - val_accuracy: 0.9589\n",
      "Epoch 113/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0315 - accuracy: 0.9888 - val_loss: 0.2509 - val_accuracy: 0.9617\n",
      "Epoch 114/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.0257 - accuracy: 0.9910 - val_loss: 0.2644 - val_accuracy: 0.9616\n",
      "Epoch 115/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0275 - accuracy: 0.9905 - val_loss: 0.2529 - val_accuracy: 0.9589\n",
      "Epoch 116/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.0288 - accuracy: 0.9901 - val_loss: 0.2535 - val_accuracy: 0.9575\n",
      "Epoch 117/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.0299 - accuracy: 0.9899 - val_loss: 0.2432 - val_accuracy: 0.9622\n",
      "Epoch 118/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.0291 - accuracy: 0.9897 - val_loss: 0.2481 - val_accuracy: 0.9597\n",
      "Epoch 119/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.0300 - accuracy: 0.9899 - val_loss: 0.2594 - val_accuracy: 0.9595\n",
      "Epoch 120/120\n",
      "228/228 [==============================] - 17s 72ms/step - loss: 0.0330 - accuracy: 0.9886 - val_loss: 0.2466 - val_accuracy: 0.9586\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x253ccab2488>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 120\n",
    "model.fit(X_train, y_train, \n",
    "         batch_size=batch_size, epochs=epochs, verbose=1, \n",
    "         validation_data=(X_valid, y_valid), \n",
    "         callbacks=[modelcheckpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z8t0nVCw8JmP"
   },
   "outputs": [],
   "source": [
    "model.load_weights(output_dir+\"/weights.100.hdf5\") # 96.32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZwGk5dR8JmS"
   },
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EPc5_h6K8JmW",
    "outputId": "cad99da9-9f89-437f-854b-a315616ed50f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6476"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E_GNq-R_8JmZ",
    "outputId": "0fc16efe-7739-4394-f1a3-46da92e79584"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.1243720e-06, 1.6568372e-06, 7.3366004e-07, 3.5447638e-06,\n",
       "       2.3228047e-06, 9.9999070e-01], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFqRQ5XB8Jmc",
    "outputId": "a957fbed-92f8-4bdd-d24b-5df3361a8bac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FTWf6Cf-8Jme",
    "outputId": "1256b170-33dc-4171-f2ff-7a3d7ddd74e3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_hat)\n",
    "_ = plt.axvline(x=0.5, color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RNljSx3v8Jmh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'99.66'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pct_auc = roc_auc_score(y_valid, y_hat)*100.0\n",
    "\"{:0.2f}\".format(pct_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SO_N9lJG8Jmj",
    "outputId": "20968a52-8ca0-44d3-abf6-a1382bba5cde"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'96.22'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_classes = y_hat.round()\n",
    "acc = score(y_valid, y_pred_classes) * 100\n",
    "\"{:0.2f}\".format(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vU3hnA298Jmo"
   },
   "outputs": [],
   "source": [
    "float_y_hat = []\n",
    "for y in y_hat:\n",
    "    float_y_hat.append(y[0:6].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zrTf-aQl8Jmr"
   },
   "outputs": [],
   "source": [
    "ydf = pd.DataFrame(list(zip(float_y_hat, y_valid)), columns=['y_hat', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zAyKh_mX8Jmu",
    "outputId": "611224b8-f980-47df-d596-9aa3a5811009"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_hat</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            y_hat                               y\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
       "1  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n",
       "2  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
       "3  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "4  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
       "5  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
       "6  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
       "7  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
       "8  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "9  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ydf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "dense_sentiment_classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
