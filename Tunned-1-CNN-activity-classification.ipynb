{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/khataei/PE-classification-DeepLearning/blob/master/Tunned-1-CNN-activity-classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ahUZuAfz8Jje"
   },
   "source": [
    "# CNN Activity Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Re0ecmVI8Jjk"
   },
   "source": [
    "In this notebook, we build a CNN neural net to classify PE activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ixsGb9tY8Jjm"
   },
   "source": [
    "#### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5At1PKQp8Jjp"
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv1D, GlobalMaxPooling1D, MaxPool1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import AveragePooling1D, LeakyReLU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint \n",
    "import os  \n",
    "from sklearn.metrics import roc_auc_score, roc_curve \n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "import kerastuner as kt\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hg5wX2Dd8Jjz"
   },
   "source": [
    "#### Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lYLOM99-8Jj2"
   },
   "outputs": [],
   "source": [
    "# output directory name:\n",
    "output_dir = 'model_output/cnn2'\n",
    "input_dir =  'Z:/Research/dfuller/Walkabilly/studies/smarphone_accel/data/Ethica_Jaeger_Merged/pocket/'\n",
    "input_file_name = 'pocket-NN-data.npz'\n",
    "\n",
    "# from the data preparation section we have:\n",
    "window_size_second = 3\n",
    "frequency = 30\n",
    "lenght_of_each_seq = window_size_second * frequency\n",
    "\n",
    "# pooling layer parameters\n",
    "maxpooling_pool_size = 2\n",
    "avepooling_pool_size = 2\n",
    "\n",
    "\n",
    "# convolutional layer architecture:\n",
    "n_conv_1 = 256 # filters, a.k.a. kernels\n",
    "k_conv_1 = 3 # kernel length\n",
    "n_conv_2 = 256\n",
    "k_conv_2 = 3 # kernel length\n",
    "n_conv_3 = 256 # filters, a.k.a. kernels\n",
    "k_conv_3 = 2 # kernel length\n",
    "\n",
    "# dense layer architecture: \n",
    "n_dense_1 = 512\n",
    "dropout_1 = 0.3\n",
    "n_dense_2 = 256\n",
    "dropout_2 = 0.25\n",
    "\n",
    "# training:\n",
    "epochs = 60\n",
    "batch_size = 256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DaLx4yZ48Jj9"
   },
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CD9X0yJg8Jj_"
   },
   "source": [
    "##### For this notebook we use the acceleration data gathered from the pocket location. It was prepared in the DataPrep-Deep notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6r44JKj-8JkA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acceleration_data\n",
      "metadata\n",
      "labels\n"
     ]
    }
   ],
   "source": [
    "# read the raw file and get the keys:\n",
    "raw_data = np.load(file=input_dir+input_file_name,allow_pickle=True)\n",
    "for k in raw_data.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "\n",
    "accel_array = raw_data['acceleration_data']\n",
    "meta_array = raw_data['metadata']\n",
    "labels_array = raw_data['labels']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WWq3ipbu8JlG"
   },
   "source": [
    "#### Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the  labels to integer.\n",
    "In the raw data format of the labels is String and there are 6 classes. 'Lying', 'Sitting', 'Self Pace walk', 'Running 3 METs',\n",
    "       'Running 5 METs', 'Running 7 METs' <br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LyReiequ8Jln"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       ...,\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change from string to integer so keras.to_categorical can consume it\n",
    "\n",
    "# could do with factorize method as well\n",
    "n_class = len(np.unique(labels_array))\n",
    "class_list, labels_array_int = np.unique(labels_array,return_inverse=True)\n",
    "labels_array_int\n",
    "\n",
    "# check if the result is consistant with the original input\n",
    "class_list[labels_array_int].reshape(len(labels_array_int), 1) == labels_array\n",
    "\n",
    "# Note: to get the reverse, i.e converting integer array to string use class_list[labels_array_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64754, 6)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels to categorical\n",
    "\n",
    "y = to_categorical(labels_array_int, num_classes=n_class)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[64754, 90, 3]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = list(accel_array.shape)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "40x3s0KY8Jlv",
    "outputId": "d3ac3e31-3c47-4b01-c2f9-ada0684b31b7"
   },
   "source": [
    "### Splitting and shuffeling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f5grc2H78Jlz",
    "outputId": "3f7eeb55-f851-4796-de4e-0c68eb9df88d"
   },
   "outputs": [],
   "source": [
    "# split and shuffle\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "     accel_array, y, test_size=0.1, random_state=65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "GscSnCpk8Jlm"
   },
   "source": [
    "\n",
    "#### Design neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_44 (Conv1D)           (None, 88, 256)           2560      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 44, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 42, 256)           196864    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_8 (Average (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 20, 256)           131328    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_10  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 595,206\n",
      "Trainable params: 595,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# a conv model!\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(n_conv_1, k_conv_1, activation='relu', input_shape=input_shape[1:]))\n",
    "model.add(MaxPool1D(pool_size = maxpooling_pool_size))\n",
    "model.add(Conv1D(n_conv_2, k_conv_2, activation='relu'))\n",
    "model.add(AveragePooling1D(pool_size = avepooling_pool_size))\n",
    "model.add(Conv1D(n_conv_3, k_conv_3, activation='relu'))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(n_dense_1, activation=LeakyReLU(alpha=0.1)))\n",
    "model.add(Dropout(dropout_1))\n",
    "model.add(Dense(n_dense_2, activation='relu'))\n",
    "model.add(Dropout(dropout_2))\n",
    "model.add(Dense(n_class, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JkbhJMox8Jl9"
   },
   "source": [
    "#### Configure model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QodbQvQh8Jl_"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zHjvYe288JmE"
   },
   "outputs": [],
   "source": [
    "modelcheckpoint = ModelCheckpoint(filepath=output_dir+\n",
    "                                  \"/weights.{epoch:02d}.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "esUwodZA8JmI"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEYpX7968JmL"
   },
   "source": [
    "#### Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "QaD1W7Ka8JmM",
    "outputId": "f0c30141-0962-48f6-a000-d136af50af79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "228/228 [==============================] - 6s 25ms/step - loss: 1.0732 - accuracy: 0.5505 - val_loss: 0.8005 - val_accuracy: 0.6805\n",
      "Epoch 2/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.7005 - accuracy: 0.7145 - val_loss: 0.5832 - val_accuracy: 0.7648\n",
      "Epoch 3/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.4800 - accuracy: 0.8212 - val_loss: 0.6195 - val_accuracy: 0.7773\n",
      "Epoch 4/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.3663 - accuracy: 0.8636 - val_loss: 0.3177 - val_accuracy: 0.8885\n",
      "Epoch 5/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.3118 - accuracy: 0.8855 - val_loss: 0.2939 - val_accuracy: 0.8931\n",
      "Epoch 6/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.2833 - accuracy: 0.8956 - val_loss: 0.3149 - val_accuracy: 0.8792\n",
      "Epoch 7/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.2762 - accuracy: 0.9000 - val_loss: 0.2445 - val_accuracy: 0.9070\n",
      "Epoch 8/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.2468 - accuracy: 0.9110 - val_loss: 0.2336 - val_accuracy: 0.9141\n",
      "Epoch 9/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.2295 - accuracy: 0.9166 - val_loss: 0.2303 - val_accuracy: 0.9143\n",
      "Epoch 10/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.2222 - accuracy: 0.9191 - val_loss: 0.2336 - val_accuracy: 0.9165\n",
      "Epoch 11/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.2206 - accuracy: 0.9212 - val_loss: 0.2123 - val_accuracy: 0.9197\n",
      "Epoch 12/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.2425 - accuracy: 0.9164 - val_loss: 0.2086 - val_accuracy: 0.9259\n",
      "Epoch 13/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.1970 - accuracy: 0.9291 - val_loss: 0.2036 - val_accuracy: 0.9316\n",
      "Epoch 14/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.1942 - accuracy: 0.9299 - val_loss: 0.1896 - val_accuracy: 0.9293\n",
      "Epoch 15/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.1822 - accuracy: 0.9338 - val_loss: 0.2001 - val_accuracy: 0.9317\n",
      "Epoch 16/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.1771 - accuracy: 0.9353 - val_loss: 0.1979 - val_accuracy: 0.9302\n",
      "Epoch 17/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.1822 - accuracy: 0.9357 - val_loss: 0.1879 - val_accuracy: 0.9336\n",
      "Epoch 18/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.1649 - accuracy: 0.9405 - val_loss: 0.1816 - val_accuracy: 0.9384\n",
      "Epoch 19/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.1627 - accuracy: 0.9418 - val_loss: 0.1670 - val_accuracy: 0.9424\n",
      "Epoch 20/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.1559 - accuracy: 0.9442 - val_loss: 0.1672 - val_accuracy: 0.9396\n",
      "Epoch 21/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.1488 - accuracy: 0.9466 - val_loss: 0.1615 - val_accuracy: 0.9398\n",
      "Epoch 22/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.1477 - accuracy: 0.9458 - val_loss: 0.1746 - val_accuracy: 0.9389\n",
      "Epoch 23/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.1431 - accuracy: 0.9486 - val_loss: 0.1907 - val_accuracy: 0.9345\n",
      "Epoch 24/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.1380 - accuracy: 0.9498 - val_loss: 0.1648 - val_accuracy: 0.9481\n",
      "Epoch 25/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.1323 - accuracy: 0.9521 - val_loss: 0.1671 - val_accuracy: 0.9472\n",
      "Epoch 26/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.1332 - accuracy: 0.9520 - val_loss: 0.1606 - val_accuracy: 0.9492\n",
      "Epoch 27/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.1321 - accuracy: 0.9530 - val_loss: 0.1458 - val_accuracy: 0.9518\n",
      "Epoch 28/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.1222 - accuracy: 0.9552 - val_loss: 0.1757 - val_accuracy: 0.9412\n",
      "Epoch 29/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.1226 - accuracy: 0.9551 - val_loss: 0.1515 - val_accuracy: 0.9487\n",
      "Epoch 30/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.1191 - accuracy: 0.9567 - val_loss: 0.1662 - val_accuracy: 0.9452\n",
      "Epoch 31/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.1157 - accuracy: 0.9581 - val_loss: 0.1546 - val_accuracy: 0.9484\n",
      "Epoch 32/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.1151 - accuracy: 0.9575 - val_loss: 0.1555 - val_accuracy: 0.9497\n",
      "Epoch 33/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.1101 - accuracy: 0.9600 - val_loss: 0.1393 - val_accuracy: 0.9560\n",
      "Epoch 34/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.1091 - accuracy: 0.9595 - val_loss: 0.1573 - val_accuracy: 0.9486\n",
      "Epoch 35/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.1081 - accuracy: 0.9603 - val_loss: 0.1608 - val_accuracy: 0.9494\n",
      "Epoch 36/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.1034 - accuracy: 0.9617 - val_loss: 0.1484 - val_accuracy: 0.9549\n",
      "Epoch 37/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.1006 - accuracy: 0.9625 - val_loss: 0.1487 - val_accuracy: 0.9526\n",
      "Epoch 38/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.1000 - accuracy: 0.9627 - val_loss: 0.1457 - val_accuracy: 0.9507\n",
      "Epoch 39/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.0970 - accuracy: 0.9635 - val_loss: 0.1557 - val_accuracy: 0.9506\n",
      "Epoch 40/60\n",
      "228/228 [==============================] - 5s 24ms/step - loss: 0.0996 - accuracy: 0.9637 - val_loss: 0.1490 - val_accuracy: 0.9560\n",
      "Epoch 41/60\n",
      "228/228 [==============================] - 5s 24ms/step - loss: 0.0993 - accuracy: 0.9629 - val_loss: 0.1462 - val_accuracy: 0.9549\n",
      "Epoch 42/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.0934 - accuracy: 0.9652 - val_loss: 0.1435 - val_accuracy: 0.9554\n",
      "Epoch 43/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.0916 - accuracy: 0.9660 - val_loss: 0.1437 - val_accuracy: 0.9588\n",
      "Epoch 44/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.0882 - accuracy: 0.9666 - val_loss: 0.1413 - val_accuracy: 0.9575\n",
      "Epoch 45/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.1011 - accuracy: 0.9653 - val_loss: 0.1478 - val_accuracy: 0.9575\n",
      "Epoch 46/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.0882 - accuracy: 0.9669 - val_loss: 0.1381 - val_accuracy: 0.9582\n",
      "Epoch 47/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.0877 - accuracy: 0.9678 - val_loss: 0.1435 - val_accuracy: 0.9574\n",
      "Epoch 48/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.0836 - accuracy: 0.9689 - val_loss: 0.1652 - val_accuracy: 0.9514\n",
      "Epoch 49/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.0822 - accuracy: 0.9690 - val_loss: 0.1425 - val_accuracy: 0.9619\n",
      "Epoch 50/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.0846 - accuracy: 0.9688 - val_loss: 0.1454 - val_accuracy: 0.9555\n",
      "Epoch 51/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.0807 - accuracy: 0.9697 - val_loss: 0.1479 - val_accuracy: 0.9569\n",
      "Epoch 52/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.0817 - accuracy: 0.9688 - val_loss: 0.1639 - val_accuracy: 0.9523\n",
      "Epoch 53/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.0829 - accuracy: 0.9695 - val_loss: 0.1491 - val_accuracy: 0.9572\n",
      "Epoch 54/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.0791 - accuracy: 0.9703 - val_loss: 0.1545 - val_accuracy: 0.9591\n",
      "Epoch 55/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.0798 - accuracy: 0.9702 - val_loss: 0.1488 - val_accuracy: 0.9591\n",
      "Epoch 56/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.0748 - accuracy: 0.9716 - val_loss: 0.1537 - val_accuracy: 0.9575\n",
      "Epoch 57/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.0760 - accuracy: 0.9709 - val_loss: 0.1706 - val_accuracy: 0.9563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.0781 - accuracy: 0.9705 - val_loss: 0.1600 - val_accuracy: 0.9575\n",
      "Epoch 59/60\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.0762 - accuracy: 0.9711 - val_loss: 0.1673 - val_accuracy: 0.9541\n",
      "Epoch 60/60\n",
      "228/228 [==============================] - 5s 24ms/step - loss: 0.0720 - accuracy: 0.9727 - val_loss: 0.1595 - val_accuracy: 0.9594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1272684a988>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, \n",
    "         batch_size=batch_size, epochs=epochs, verbose=1, \n",
    "         validation_data=(X_valid, y_valid), \n",
    "         callbacks=[modelcheckpoint])\n",
    "\n",
    "# model.fit(x_train, y_train, \n",
    "#           batch_size=batch_size, epochs=epochs, verbose=1, \n",
    "#           validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7PWlH5SJ8JmP"
   },
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z8t0nVCw8JmP"
   },
   "outputs": [],
   "source": [
    "model.load_weights(output_dir+\"/weights.49.hdf5\") # 96.19 val accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZwGk5dR8JmS"
   },
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EPc5_h6K8JmW",
    "outputId": "cad99da9-9f89-437f-854b-a315616ed50f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6476"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E_GNq-R_8JmZ",
    "outputId": "0fc16efe-7739-4394-f1a3-46da92e79584"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.2657222e-03, 1.1235670e-08, 2.5627809e-09, 1.4760160e-09,\n",
       "       1.3126909e-05, 9.9672121e-01], dtype=float32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFqRQ5XB8Jmc",
    "outputId": "a957fbed-92f8-4bdd-d24b-5df3361a8bac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FTWf6Cf-8Jme",
    "outputId": "1256b170-33dc-4171-f2ff-7a3d7ddd74e3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_hat)\n",
    "_ = plt.axvline(x=0.5, color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RNljSx3v8Jmh"
   },
   "outputs": [],
   "source": [
    "pct_auc = roc_auc_score(y_valid, y_hat)*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SO_N9lJG8Jmj",
    "outputId": "20968a52-8ca0-44d3-abf6-a1382bba5cde"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'99.79'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{:0.2f}\".format(pct_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vU3hnA298Jmo"
   },
   "outputs": [],
   "source": [
    "float_y_hat = []\n",
    "for y in y_hat:\n",
    "    float_y_hat.append(y[0:6].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zrTf-aQl8Jmr"
   },
   "outputs": [],
   "source": [
    "ydf = pd.DataFrame(list(zip(float_y_hat, y_valid)), columns=['y_hat', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zAyKh_mX8Jmu",
    "outputId": "611224b8-f980-47df-d596-9aa3a5811009"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_hat</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.003, 0.0, 0.0, 0.0, 0.0, 0.997]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.003, 0.0, 0.0, 0.0, 0.0, 0.997]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.002, 0.998, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                y_hat                               y\n",
       "0  [0.003, 0.0, 0.0, 0.0, 0.0, 0.997]  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
       "1      [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n",
       "2      [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
       "3      [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "4      [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
       "5  [0.003, 0.0, 0.0, 0.0, 0.0, 0.997]  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
       "6      [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
       "7  [0.002, 0.998, 0.0, 0.0, 0.0, 0.0]  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
       "8      [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "9      [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ydf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning process\n",
    "\n",
    "Using `for` loops and `checkpoints` to exhaustively search for the best hyperparameters is very time consuming. Therefore, we use the `keras-tuner` module to randomly search for the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2125764, 'models to train!')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hyperparams to tune\n",
    "\n",
    "layers_number = [2, 3]\n",
    "\n",
    "# pooling layer parameters\n",
    "maxpooling_pool_size = [2, 4, 8]\n",
    "avepooling_pool_size = [2, 4, 8]\n",
    "\n",
    "\n",
    "# convolutional layer architecture:\n",
    "n_conv_1 = [128, 256, 512] # filters, a.k.a. kernels\n",
    "k_conv_1 = [3, 5, 7] # kernel length\n",
    "n_conv_2 = [128, 256, 512]\n",
    "k_conv_2 = [3, 5, 7] # kernel length\n",
    "n_conv_3 = [128, 256, 512] # filters, a.k.a. kernels\n",
    "k_conv_3 = [3, 5, 7] # kernel length\n",
    "\n",
    "# dense layer architecture: \n",
    "n_dense_1 = [256, 512, 1024]\n",
    "dropout_1 = [0.2, 0.3, 0.4]\n",
    "n_dense_2 = [256, 512, 1024]\n",
    "dropout_2 = [0.2, 0.3, 0.4]\n",
    "\n",
    "# training:\n",
    "epochs = 60\n",
    "batch_size = [256, 128]\n",
    "2*3*3*3*3*3*3*3*3*3*3*3*3*2, \"models to train!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define kerastuner hyper parameters\n",
    "kt_max_pooling = hp.Int('maxpooling', min_value=2, max_value=10, step=2)\n",
    "kt_ave_pooling = hp.Int('avepooling', min_value=2, max_value=10, step=2)\n",
    "kt_n_conv_1 = hp.Int('n_conv_1', min_value = 128, max_value= 1024, step = 256)\n",
    "kt_k_conv_1 = hp.Int('k_conv_1', min_value = 2, max_value= 10, step = 2)\n",
    "kt_n_conv_2 = hp.Int('n_conv_2', min_value = 128, max_value= 1024, step = 256)\n",
    "kt_k_conv_2 = hp.Int('k_conv_2', min_value = 2, max_value= 10, step = 2)\n",
    "kt_n_conv_3 = hp.Int('n_conv_3', min_value = 128, max_value= 1024, step = 256)\n",
    "kt_k_conv_3 = hp.Int('k_conv_3', min_value = 2, max_value= 10, step = 2)\n",
    "kt_activation = hp.Choice('activation', values = ['relu','elu'])\n",
    "kt_n_dense_1 = hp.Int('n_dense_1', min_value = 128, max_value= 1024, step = 256)\n",
    "kt_dropout_1 = hp.Choice('dropout_1', values= [0.2, 0.3, 0.4])\n",
    "kt_n_dense_2 = hp.Int('n_dense_2', min_value = 128, max_value= 1024, step = 256)\n",
    "kt_dropout_2 = hp.Choice('dropout_2', values= [0.2, 0.3, 0.4, 0.5])\n",
    "kt_batch_size = hp.Choice('batch_size', values= [128, 256])\n",
    "kt_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4]) \n",
    "kt_optimizer = hp.Choice('optimizer', values=['adam', 'nadam'])\n",
    "tunning_dir = 'tunning-dir'\n",
    "if not os.path.exists(tunning_dir):\n",
    "    os.makedirs(tunning_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    #\n",
    "    model.add(Conv1D(kt_n_conv_1, kt_k_conv_1, activation=kt_activation, input_shape=input_shape[1:]))\n",
    "    model.add(MaxPool1D(pool_size = kt_max_pooling))\n",
    "    model.add(Conv1D(kt_n_conv_2, kt_k_conv_2, activation=kt_activation))\n",
    "    model.add(AveragePooling1D(pool_size = kt_ave_pooling))\n",
    "    model.add(Conv1D(kt_n_conv_3, kt_k_conv_3, activation=kt_activation))\n",
    "    # model.add(GlobalMaxPooling1D())\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(kt_n_dense_1, kt_activation))\n",
    "    model.add(Dropout(kt_dropout_1))\n",
    "    model.add(Dense(kt_n_dense_2, kt_activation))\n",
    "    model.add(Dropout(kt_dropout_2))\n",
    "    model.add(Dense(n_class, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=kt_optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project tunning-dir\\CNN_tunning\\oracle.json\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.RandomSearch(build_model,\n",
    "                        objective = 'val_accuracy', \n",
    "                        max_trials = 1,\n",
    "                        directory = tunning_dir,\n",
    "                        project_name = 'CNN_tunning') \n",
    "\n",
    "modelcheckpoint = ModelCheckpoint(filepath=tunning_dir, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Search space summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Default search space size: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tuner.search_space_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "1822/1822 [==============================] - ETA: 0s - loss: 1.7849 - accuracy: 0.28 - ETA: 7s - loss: 1.7705 - accuracy: 0.27 - ETA: 7s - loss: 1.7326 - accuracy: 0.28 - ETA: 7s - loss: 1.6625 - accuracy: 0.30 - ETA: 6s - loss: 1.5765 - accuracy: 0.33 - ETA: 6s - loss: 1.5203 - accuracy: 0.35 - ETA: 6s - loss: 1.4885 - accuracy: 0.36 - ETA: 6s - loss: 1.4560 - accuracy: 0.38 - ETA: 6s - loss: 1.4239 - accuracy: 0.39 - ETA: 6s - loss: 1.4084 - accuracy: 0.39 - ETA: 6s - loss: 1.3898 - accuracy: 0.40 - ETA: 6s - loss: 1.3666 - accuracy: 0.41 - ETA: 6s - loss: 1.3533 - accuracy: 0.42 - ETA: 6s - loss: 1.3326 - accuracy: 0.43 - ETA: 6s - loss: 1.3192 - accuracy: 0.43 - ETA: 6s - loss: 1.3090 - accuracy: 0.44 - ETA: 6s - loss: 1.2938 - accuracy: 0.44 - ETA: 5s - loss: 1.2852 - accuracy: 0.45 - ETA: 5s - loss: 1.2739 - accuracy: 0.45 - ETA: 5s - loss: 1.2669 - accuracy: 0.46 - ETA: 5s - loss: 1.2546 - accuracy: 0.46 - ETA: 5s - loss: 1.2444 - accuracy: 0.46 - ETA: 5s - loss: 1.2340 - accuracy: 0.47 - ETA: 5s - loss: 1.2231 - accuracy: 0.47 - ETA: 5s - loss: 1.2159 - accuracy: 0.48 - ETA: 5s - loss: 1.2070 - accuracy: 0.48 - ETA: 5s - loss: 1.2023 - accuracy: 0.48 - ETA: 5s - loss: 1.1985 - accuracy: 0.48 - ETA: 5s - loss: 1.1911 - accuracy: 0.49 - ETA: 5s - loss: 1.1858 - accuracy: 0.49 - ETA: 5s - loss: 1.1804 - accuracy: 0.49 - ETA: 5s - loss: 1.1754 - accuracy: 0.50 - ETA: 5s - loss: 1.1727 - accuracy: 0.50 - ETA: 4s - loss: 1.1643 - accuracy: 0.50 - ETA: 4s - loss: 1.1605 - accuracy: 0.50 - ETA: 4s - loss: 1.1540 - accuracy: 0.51 - ETA: 4s - loss: 1.1470 - accuracy: 0.51 - ETA: 4s - loss: 1.1428 - accuracy: 0.51 - ETA: 4s - loss: 1.1375 - accuracy: 0.51 - ETA: 4s - loss: 1.1319 - accuracy: 0.51 - ETA: 4s - loss: 1.1254 - accuracy: 0.52 - ETA: 4s - loss: 1.1208 - accuracy: 0.52 - ETA: 4s - loss: 1.1150 - accuracy: 0.52 - ETA: 4s - loss: 1.1098 - accuracy: 0.53 - ETA: 4s - loss: 1.1062 - accuracy: 0.53 - ETA: 4s - loss: 1.1017 - accuracy: 0.53 - ETA: 4s - loss: 1.0963 - accuracy: 0.53 - ETA: 4s - loss: 1.0915 - accuracy: 0.53 - ETA: 4s - loss: 1.0866 - accuracy: 0.54 - ETA: 4s - loss: 1.0817 - accuracy: 0.54 - ETA: 4s - loss: 1.0764 - accuracy: 0.54 - ETA: 3s - loss: 1.0724 - accuracy: 0.54 - ETA: 3s - loss: 1.0704 - accuracy: 0.54 - ETA: 3s - loss: 1.0682 - accuracy: 0.54 - ETA: 3s - loss: 1.0652 - accuracy: 0.55 - ETA: 3s - loss: 1.0617 - accuracy: 0.55 - ETA: 3s - loss: 1.0580 - accuracy: 0.55 - ETA: 3s - loss: 1.0543 - accuracy: 0.55 - ETA: 3s - loss: 1.0505 - accuracy: 0.55 - ETA: 3s - loss: 1.0459 - accuracy: 0.55 - ETA: 3s - loss: 1.0425 - accuracy: 0.56 - ETA: 3s - loss: 1.0388 - accuracy: 0.56 - ETA: 3s - loss: 1.0356 - accuracy: 0.56 - ETA: 3s - loss: 1.0320 - accuracy: 0.56 - ETA: 3s - loss: 1.0301 - accuracy: 0.56 - ETA: 3s - loss: 1.0255 - accuracy: 0.56 - ETA: 3s - loss: 1.0222 - accuracy: 0.56 - ETA: 3s - loss: 1.0191 - accuracy: 0.57 - ETA: 3s - loss: 1.0156 - accuracy: 0.57 - ETA: 3s - loss: 1.0127 - accuracy: 0.57 - ETA: 3s - loss: 1.0093 - accuracy: 0.57 - ETA: 2s - loss: 1.0062 - accuracy: 0.57 - ETA: 2s - loss: 1.0038 - accuracy: 0.57 - ETA: 2s - loss: 1.0004 - accuracy: 0.57 - ETA: 2s - loss: 0.9982 - accuracy: 0.58 - ETA: 2s - loss: 0.9963 - accuracy: 0.58 - ETA: 2s - loss: 0.9939 - accuracy: 0.58 - ETA: 2s - loss: 0.9916 - accuracy: 0.58 - ETA: 2s - loss: 0.9884 - accuracy: 0.58 - ETA: 2s - loss: 0.9850 - accuracy: 0.58 - ETA: 2s - loss: 0.9814 - accuracy: 0.58 - ETA: 2s - loss: 0.9796 - accuracy: 0.58 - ETA: 2s - loss: 0.9767 - accuracy: 0.58 - ETA: 2s - loss: 0.9745 - accuracy: 0.58 - ETA: 2s - loss: 0.9724 - accuracy: 0.59 - ETA: 2s - loss: 0.9705 - accuracy: 0.59 - ETA: 2s - loss: 0.9679 - accuracy: 0.59 - ETA: 2s - loss: 0.9651 - accuracy: 0.59 - ETA: 2s - loss: 0.9623 - accuracy: 0.59 - ETA: 1s - loss: 0.9597 - accuracy: 0.59 - ETA: 1s - loss: 0.9565 - accuracy: 0.59 - ETA: 1s - loss: 0.9551 - accuracy: 0.59 - ETA: 1s - loss: 0.9530 - accuracy: 0.59 - ETA: 1s - loss: 0.9512 - accuracy: 0.59 - ETA: 1s - loss: 0.9489 - accuracy: 0.60 - ETA: 1s - loss: 0.9469 - accuracy: 0.60 - ETA: 1s - loss: 0.9449 - accuracy: 0.60 - ETA: 1s - loss: 0.9424 - accuracy: 0.60 - ETA: 1s - loss: 0.9398 - accuracy: 0.60 - ETA: 1s - loss: 0.9383 - accuracy: 0.60 - ETA: 1s - loss: 0.9359 - accuracy: 0.60 - ETA: 1s - loss: 0.9342 - accuracy: 0.60 - ETA: 1s - loss: 0.9315 - accuracy: 0.60 - ETA: 1s - loss: 0.9291 - accuracy: 0.60 - ETA: 1s - loss: 0.9267 - accuracy: 0.60 - ETA: 1s - loss: 0.9246 - accuracy: 0.60 - ETA: 1s - loss: 0.9219 - accuracy: 0.61 - ETA: 1s - loss: 0.9199 - accuracy: 0.61 - ETA: 0s - loss: 0.9175 - accuracy: 0.61 - ETA: 0s - loss: 0.9154 - accuracy: 0.61 - ETA: 0s - loss: 0.9135 - accuracy: 0.61 - ETA: 0s - loss: 0.9119 - accuracy: 0.61 - ETA: 0s - loss: 0.9094 - accuracy: 0.61 - ETA: 0s - loss: 0.9078 - accuracy: 0.61 - ETA: 0s - loss: 0.9063 - accuracy: 0.61 - ETA: 0s - loss: 0.9047 - accuracy: 0.61 - ETA: 0s - loss: 0.9030 - accuracy: 0.61 - ETA: 0s - loss: 0.9006 - accuracy: 0.61 - ETA: 0s - loss: 0.8991 - accuracy: 0.62 - ETA: 0s - loss: 0.8975 - accuracy: 0.62 - ETA: 0s - loss: 0.8955 - accuracy: 0.62 - ETA: 0s - loss: 0.8936 - accuracy: 0.62 - ETA: 0s - loss: 0.8912 - accuracy: 0.62 - ETA: 0s - loss: 0.8889 - accuracy: 0.62 - ETA: 0s - loss: 0.8872 - accuracy: 0.62 - ETA: 0s - loss: 0.8851 - accuracy: 0.62 - ETA: 0s - loss: 0.8840 - accuracy: 0.62 - ETA: 0s - loss: 0.8826 - accuracy: 0.62 - 7s 4ms/step - loss: 0.8826 - accuracy: 0.6277 - val_loss: 0.6102 - val_accuracy: 0.7539\n",
      "Epoch 2/60\n",
      "1822/1822 [==============================] - ETA: 0s - loss: 0.5656 - accuracy: 0.75 - ETA: 6s - loss: 0.6112 - accuracy: 0.74 - ETA: 6s - loss: 0.6208 - accuracy: 0.73 - ETA: 6s - loss: 0.6173 - accuracy: 0.74 - ETA: 6s - loss: 0.6269 - accuracy: 0.73 - ETA: 6s - loss: 0.6397 - accuracy: 0.73 - ETA: 6s - loss: 0.6373 - accuracy: 0.73 - ETA: 6s - loss: 0.6429 - accuracy: 0.72 - ETA: 6s - loss: 0.6400 - accuracy: 0.72 - ETA: 6s - loss: 0.6343 - accuracy: 0.73 - ETA: 6s - loss: 0.6381 - accuracy: 0.73 - ETA: 6s - loss: 0.6380 - accuracy: 0.73 - ETA: 6s - loss: 0.6397 - accuracy: 0.73 - ETA: 6s - loss: 0.6406 - accuracy: 0.73 - ETA: 6s - loss: 0.6385 - accuracy: 0.73 - ETA: 5s - loss: 0.6401 - accuracy: 0.73 - ETA: 5s - loss: 0.6435 - accuracy: 0.73 - ETA: 5s - loss: 0.6431 - accuracy: 0.73 - ETA: 5s - loss: 0.6374 - accuracy: 0.73 - ETA: 5s - loss: 0.6345 - accuracy: 0.73 - ETA: 5s - loss: 0.6364 - accuracy: 0.73 - ETA: 5s - loss: 0.6334 - accuracy: 0.73 - ETA: 5s - loss: 0.6309 - accuracy: 0.74 - ETA: 5s - loss: 0.6280 - accuracy: 0.74 - ETA: 5s - loss: 0.6281 - accuracy: 0.74 - ETA: 5s - loss: 0.6291 - accuracy: 0.74 - ETA: 5s - loss: 0.6270 - accuracy: 0.74 - ETA: 5s - loss: 0.6250 - accuracy: 0.74 - ETA: 5s - loss: 0.6244 - accuracy: 0.74 - ETA: 5s - loss: 0.6225 - accuracy: 0.74 - ETA: 5s - loss: 0.6196 - accuracy: 0.74 - ETA: 4s - loss: 0.6192 - accuracy: 0.74 - ETA: 4s - loss: 0.6193 - accuracy: 0.74 - ETA: 4s - loss: 0.6170 - accuracy: 0.74 - ETA: 4s - loss: 0.6149 - accuracy: 0.74 - ETA: 4s - loss: 0.6152 - accuracy: 0.74 - ETA: 4s - loss: 0.6144 - accuracy: 0.74 - ETA: 4s - loss: 0.6140 - accuracy: 0.74 - ETA: 4s - loss: 0.6121 - accuracy: 0.74 - ETA: 4s - loss: 0.6119 - accuracy: 0.74 - ETA: 4s - loss: 0.6118 - accuracy: 0.74 - ETA: 4s - loss: 0.6102 - accuracy: 0.75 - ETA: 4s - loss: 0.6082 - accuracy: 0.75 - ETA: 4s - loss: 0.6089 - accuracy: 0.75 - ETA: 4s - loss: 0.6074 - accuracy: 0.75 - ETA: 4s - loss: 0.6066 - accuracy: 0.75 - ETA: 4s - loss: 0.6048 - accuracy: 0.75 - ETA: 4s - loss: 0.6043 - accuracy: 0.75 - ETA: 4s - loss: 0.6035 - accuracy: 0.75 - ETA: 4s - loss: 0.6019 - accuracy: 0.75 - ETA: 4s - loss: 0.6015 - accuracy: 0.75 - ETA: 4s - loss: 0.6004 - accuracy: 0.75 - ETA: 3s - loss: 0.5989 - accuracy: 0.75 - ETA: 3s - loss: 0.5995 - accuracy: 0.75 - ETA: 3s - loss: 0.5999 - accuracy: 0.75 - ETA: 3s - loss: 0.6007 - accuracy: 0.75 - ETA: 3s - loss: 0.5998 - accuracy: 0.75 - ETA: 3s - loss: 0.5983 - accuracy: 0.75 - ETA: 3s - loss: 0.5977 - accuracy: 0.75 - ETA: 3s - loss: 0.5968 - accuracy: 0.75 - ETA: 3s - loss: 0.5968 - accuracy: 0.75 - ETA: 3s - loss: 0.5957 - accuracy: 0.75 - ETA: 3s - loss: 0.5938 - accuracy: 0.75 - ETA: 3s - loss: 0.5935 - accuracy: 0.75 - ETA: 3s - loss: 0.5933 - accuracy: 0.75 - ETA: 3s - loss: 0.5922 - accuracy: 0.75 - ETA: 3s - loss: 0.5925 - accuracy: 0.75 - ETA: 3s - loss: 0.5914 - accuracy: 0.75 - ETA: 3s - loss: 0.5905 - accuracy: 0.76 - ETA: 3s - loss: 0.5887 - accuracy: 0.76 - ETA: 3s - loss: 0.5878 - accuracy: 0.76 - ETA: 2s - loss: 0.5872 - accuracy: 0.76 - ETA: 2s - loss: 0.5868 - accuracy: 0.76 - ETA: 2s - loss: 0.5862 - accuracy: 0.76 - ETA: 2s - loss: 0.5854 - accuracy: 0.76 - ETA: 2s - loss: 0.5851 - accuracy: 0.76 - ETA: 2s - loss: 0.5842 - accuracy: 0.76 - ETA: 2s - loss: 0.5827 - accuracy: 0.76 - ETA: 2s - loss: 0.5820 - accuracy: 0.76 - ETA: 2s - loss: 0.5818 - accuracy: 0.76 - ETA: 2s - loss: 0.5805 - accuracy: 0.76 - ETA: 2s - loss: 0.5797 - accuracy: 0.76 - ETA: 2s - loss: 0.5798 - accuracy: 0.76 - ETA: 2s - loss: 0.5782 - accuracy: 0.76 - ETA: 2s - loss: 0.5768 - accuracy: 0.76 - ETA: 2s - loss: 0.5767 - accuracy: 0.76 - ETA: 2s - loss: 0.5767 - accuracy: 0.76 - ETA: 2s - loss: 0.5756 - accuracy: 0.76 - ETA: 2s - loss: 0.5744 - accuracy: 0.76 - ETA: 1s - loss: 0.5740 - accuracy: 0.76 - ETA: 1s - loss: 0.5731 - accuracy: 0.76 - ETA: 1s - loss: 0.5726 - accuracy: 0.76 - ETA: 1s - loss: 0.5718 - accuracy: 0.76 - ETA: 1s - loss: 0.5713 - accuracy: 0.76 - ETA: 1s - loss: 0.5711 - accuracy: 0.76 - ETA: 1s - loss: 0.5717 - accuracy: 0.76 - ETA: 1s - loss: 0.5707 - accuracy: 0.77 - ETA: 1s - loss: 0.5708 - accuracy: 0.77 - ETA: 1s - loss: 0.5701 - accuracy: 0.77 - ETA: 1s - loss: 0.5697 - accuracy: 0.77 - ETA: 1s - loss: 0.5683 - accuracy: 0.77 - ETA: 1s - loss: 0.5681 - accuracy: 0.77 - ETA: 1s - loss: 0.5671 - accuracy: 0.77 - ETA: 1s - loss: 0.5664 - accuracy: 0.77 - ETA: 1s - loss: 0.5656 - accuracy: 0.77 - ETA: 1s - loss: 0.5646 - accuracy: 0.77 - ETA: 1s - loss: 0.5637 - accuracy: 0.77 - ETA: 0s - loss: 0.5623 - accuracy: 0.77 - ETA: 0s - loss: 0.5612 - accuracy: 0.77 - ETA: 0s - loss: 0.5608 - accuracy: 0.77 - ETA: 0s - loss: 0.5607 - accuracy: 0.77 - ETA: 0s - loss: 0.5596 - accuracy: 0.77 - ETA: 0s - loss: 0.5593 - accuracy: 0.77 - ETA: 0s - loss: 0.5585 - accuracy: 0.77 - ETA: 0s - loss: 0.5575 - accuracy: 0.77 - ETA: 0s - loss: 0.5563 - accuracy: 0.77 - ETA: 0s - loss: 0.5553 - accuracy: 0.77 - ETA: 0s - loss: 0.5545 - accuracy: 0.77 - ETA: 0s - loss: 0.5540 - accuracy: 0.77 - ETA: 0s - loss: 0.5534 - accuracy: 0.77 - ETA: 0s - loss: 0.5528 - accuracy: 0.77 - ETA: 0s - loss: 0.5519 - accuracy: 0.77 - ETA: 0s - loss: 0.5508 - accuracy: 0.77 - ETA: 0s - loss: 0.5504 - accuracy: 0.78 - ETA: 0s - loss: 0.5496 - accuracy: 0.78 - ETA: 0s - loss: 0.5488 - accuracy: 0.78 - ETA: 0s - loss: 0.5476 - accuracy: 0.78 - 7s 4ms/step - loss: 0.5476 - accuracy: 0.7813 - val_loss: 0.5065 - val_accuracy: 0.8036\n",
      "Epoch 3/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1573/1822 [========================>.....] - ETA: 0s - loss: 0.5868 - accuracy: 0.71 - ETA: 6s - loss: 0.5327 - accuracy: 0.77 - ETA: 6s - loss: 0.4993 - accuracy: 0.79 - ETA: 6s - loss: 0.4886 - accuracy: 0.80 - ETA: 6s - loss: 0.4872 - accuracy: 0.80 - ETA: 6s - loss: 0.4839 - accuracy: 0.80 - ETA: 6s - loss: 0.4701 - accuracy: 0.81 - ETA: 6s - loss: 0.4784 - accuracy: 0.81 - ETA: 6s - loss: 0.4744 - accuracy: 0.81 - ETA: 6s - loss: 0.4738 - accuracy: 0.81 - ETA: 6s - loss: 0.4711 - accuracy: 0.81 - ETA: 6s - loss: 0.4658 - accuracy: 0.81 - ETA: 6s - loss: 0.4647 - accuracy: 0.81 - ETA: 6s - loss: 0.4649 - accuracy: 0.81 - ETA: 6s - loss: 0.4625 - accuracy: 0.81 - ETA: 6s - loss: 0.4688 - accuracy: 0.81 - ETA: 5s - loss: 0.4678 - accuracy: 0.81 - ETA: 5s - loss: 0.4675 - accuracy: 0.81 - ETA: 5s - loss: 0.4664 - accuracy: 0.81 - ETA: 5s - loss: 0.4638 - accuracy: 0.81 - ETA: 5s - loss: 0.4629 - accuracy: 0.81 - ETA: 5s - loss: 0.4636 - accuracy: 0.81 - ETA: 5s - loss: 0.4656 - accuracy: 0.81 - ETA: 5s - loss: 0.4640 - accuracy: 0.81 - ETA: 5s - loss: 0.4619 - accuracy: 0.82 - ETA: 5s - loss: 0.4607 - accuracy: 0.82 - ETA: 5s - loss: 0.4604 - accuracy: 0.82 - ETA: 5s - loss: 0.4603 - accuracy: 0.82 - ETA: 5s - loss: 0.4590 - accuracy: 0.82 - ETA: 5s - loss: 0.4583 - accuracy: 0.82 - ETA: 5s - loss: 0.4577 - accuracy: 0.82 - ETA: 5s - loss: 0.4578 - accuracy: 0.82 - ETA: 5s - loss: 0.4580 - accuracy: 0.82 - ETA: 5s - loss: 0.4568 - accuracy: 0.82 - ETA: 4s - loss: 0.4561 - accuracy: 0.82 - ETA: 4s - loss: 0.4564 - accuracy: 0.82 - ETA: 4s - loss: 0.4537 - accuracy: 0.82 - ETA: 4s - loss: 0.4533 - accuracy: 0.82 - ETA: 4s - loss: 0.4529 - accuracy: 0.82 - ETA: 4s - loss: 0.4505 - accuracy: 0.82 - ETA: 4s - loss: 0.4494 - accuracy: 0.82 - ETA: 4s - loss: 0.4490 - accuracy: 0.82 - ETA: 4s - loss: 0.4471 - accuracy: 0.82 - ETA: 4s - loss: 0.4470 - accuracy: 0.82 - ETA: 4s - loss: 0.4458 - accuracy: 0.82 - ETA: 4s - loss: 0.4455 - accuracy: 0.82 - ETA: 4s - loss: 0.4457 - accuracy: 0.82 - ETA: 4s - loss: 0.4455 - accuracy: 0.82 - ETA: 4s - loss: 0.4455 - accuracy: 0.82 - ETA: 4s - loss: 0.4463 - accuracy: 0.82 - ETA: 4s - loss: 0.4472 - accuracy: 0.82 - ETA: 4s - loss: 0.4475 - accuracy: 0.82 - ETA: 3s - loss: 0.4480 - accuracy: 0.82 - ETA: 3s - loss: 0.4483 - accuracy: 0.82 - ETA: 3s - loss: 0.4494 - accuracy: 0.82 - ETA: 3s - loss: 0.4491 - accuracy: 0.82 - ETA: 3s - loss: 0.4489 - accuracy: 0.82 - ETA: 3s - loss: 0.4497 - accuracy: 0.82 - ETA: 3s - loss: 0.4492 - accuracy: 0.82 - ETA: 3s - loss: 0.4474 - accuracy: 0.82 - ETA: 3s - loss: 0.4448 - accuracy: 0.82 - ETA: 3s - loss: 0.4435 - accuracy: 0.82 - ETA: 3s - loss: 0.4419 - accuracy: 0.82 - ETA: 3s - loss: 0.4422 - accuracy: 0.82 - ETA: 3s - loss: 0.4420 - accuracy: 0.82 - ETA: 3s - loss: 0.4416 - accuracy: 0.82 - ETA: 3s - loss: 0.4405 - accuracy: 0.82 - ETA: 3s - loss: 0.4410 - accuracy: 0.82 - ETA: 3s - loss: 0.4407 - accuracy: 0.82 - ETA: 3s - loss: 0.4403 - accuracy: 0.82 - ETA: 3s - loss: 0.4403 - accuracy: 0.82 - ETA: 3s - loss: 0.4397 - accuracy: 0.82 - ETA: 2s - loss: 0.4394 - accuracy: 0.82 - ETA: 2s - loss: 0.4380 - accuracy: 0.82 - ETA: 2s - loss: 0.4375 - accuracy: 0.82 - ETA: 2s - loss: 0.4366 - accuracy: 0.83 - ETA: 2s - loss: 0.4362 - accuracy: 0.83 - ETA: 2s - loss: 0.4357 - accuracy: 0.83 - ETA: 2s - loss: 0.4351 - accuracy: 0.83 - ETA: 2s - loss: 0.4363 - accuracy: 0.83 - ETA: 2s - loss: 0.4366 - accuracy: 0.83 - ETA: 2s - loss: 0.4361 - accuracy: 0.83 - ETA: 2s - loss: 0.4355 - accuracy: 0.83 - ETA: 2s - loss: 0.4354 - accuracy: 0.83 - ETA: 2s - loss: 0.4356 - accuracy: 0.83 - ETA: 2s - loss: 0.4361 - accuracy: 0.83 - ETA: 2s - loss: 0.4362 - accuracy: 0.83 - ETA: 2s - loss: 0.4364 - accuracy: 0.83 - ETA: 2s - loss: 0.4370 - accuracy: 0.83 - ETA: 2s - loss: 0.4372 - accuracy: 0.83 - ETA: 2s - loss: 0.4365 - accuracy: 0.83 - ETA: 2s - loss: 0.4359 - accuracy: 0.83 - ETA: 1s - loss: 0.4355 - accuracy: 0.83 - ETA: 1s - loss: 0.4360 - accuracy: 0.83 - ETA: 1s - loss: 0.4357 - accuracy: 0.83 - ETA: 1s - loss: 0.4346 - accuracy: 0.83 - ETA: 1s - loss: 0.4339 - accuracy: 0.83 - ETA: 1s - loss: 0.4337 - accuracy: 0.83 - ETA: 1s - loss: 0.4340 - accuracy: 0.83 - ETA: 1s - loss: 0.4327 - accuracy: 0.83 - ETA: 1s - loss: 0.4326 - accuracy: 0.83 - ETA: 1s - loss: 0.4329 - accuracy: 0.83 - ETA: 1s - loss: 0.4329 - accuracy: 0.83 - ETA: 1s - loss: 0.4325 - accuracy: 0.83 - ETA: 1s - loss: 0.4320 - accuracy: 0.83 - ETA: 1s - loss: 0.4325 - accuracy: 0.83 - ETA: 1s - loss: 0.4333 - accuracy: 0.83 - ETA: 1s - loss: 0.4333 - accuracy: 0.83 - ETA: 1s - loss: 0.4327 - accuracy: 0.83 - ETA: 1s - loss: 0.4325 - accuracy: 0.83 - ETA: 1s - loss: 0.4321 - accuracy: 0.83 - ETA: 1s - loss: 0.4314 - accuracy: 0.83 - ETA: 1s - loss: 0.4313 - accuracy: 0.83 - ETA: 0s - loss: 0.4319 - accuracy: 0.83 - ETA: 0s - loss: 0.4318 - accuracy: 0.8342"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-7f844bf2a53b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtuner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Get the optimal hyperparameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbest_hps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_best_hyperparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_trials\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\kerastuner\\engine\\base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\kerastuner\\engine\\multi_execution_tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcopied_fit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_values\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirection\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'min'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train, epochs = epochs, validation_data = (X_valid, y_valid),\n",
    "            callbacks=[modelcheckpoint])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 2)[0]\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "dense_sentiment_classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
