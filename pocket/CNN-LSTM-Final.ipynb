{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ahUZuAfz8Jje"
   },
   "source": [
    "# Best CNN-LSTM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Re0ecmVI8Jjk"
   },
   "source": [
    "This model created from the hyper parameters obtained from hyper parameter tuning with SKlearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ixsGb9tY8Jjm"
   },
   "source": [
    "#### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5At1PKQp8Jjp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.3.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv1D, GlobalMaxPooling1D, LSTM\n",
    "from tensorflow.keras.layers import AveragePooling1D, LeakyReLU , MaxPool1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint \n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import  accuracy_score as score\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "\n",
    "# Visiualization imports, Netron and tensorboard:\n",
    "import netron\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "from tensorflow import keras\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\"\n",
    "\n",
    "import tensorboard\n",
    "tensorboard.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to get reproducable results\n",
    "from numpy.random import seed\n",
    "seed(85)\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hg5wX2Dd8Jjz"
   },
   "source": [
    "#### Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lYLOM99-8Jj2"
   },
   "outputs": [],
   "source": [
    "# output directory name:\n",
    "output_dir = 'output/CNN-LSTM-Final/'\n",
    "input_dir =  'Z:/Research/dfuller/Walkabilly/studies/smarphone_accel/data/Ethica_Jaeger_Merged/pocket/'\n",
    "input_file_name = 'pocket-NN-data.npz'\n",
    "\n",
    "# from the data preparation section we have:\n",
    "window_size_second = 3\n",
    "frequency = 30\n",
    "lenght_of_each_seq = window_size_second * frequency\n",
    "\n",
    "n_lstm_2 = 128\n",
    "n_lstm_1 = 128\n",
    "n_dense_2= 384\n",
    "n_dense_1= 384\n",
    "n_conv_3= 512\n",
    "n_conv_2= 512\n",
    "n_conv_1= 768\n",
    "maxpooling_pool_size= 2\n",
    "k_conv_3= 2\n",
    "k_conv_2= 2\n",
    "k_conv_1= 2\n",
    "dropout_2= 0.2\n",
    "dropout_1= 0.2\n",
    "drop_lstm_2 = 0.02\n",
    "drop_lstm_1 = 0.1\n",
    "avepooling_pool_size= 2\n",
    "activation_dense = LeakyReLU()\n",
    "activation_conv = 'relu'\n",
    "\n",
    "\n",
    "\n",
    "# training:\n",
    "epochs = 120\n",
    "batch_size = 256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DaLx4yZ48Jj9"
   },
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CD9X0yJg8Jj_"
   },
   "source": [
    "For this notebook we use only the **acceleration** data prepared in the DataPrep-Deep notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "raw_data = np.load(file=input_dir+input_file_name,allow_pickle=True)\n",
    "accel_array = raw_data['acceleration_data']\n",
    "meta_array = raw_data['metadata']\n",
    "labels_array = raw_data['labels']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WWq3ipbu8JlG"
   },
   "source": [
    "#### Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the  labels to integer.\n",
    "In the raw data format of the labels is String and there are 6 classes. 'Lying', 'Sitting', 'Self Pace walk', 'Running 3 METs',\n",
    "       'Running 5 METs', 'Running 7 METs' <br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LyReiequ8Jln"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The are 6 classes: ['Lying' 'Running 3 METs' 'Running 5 METs' 'Running 7 METs'\n",
      " 'Self Pace walk' 'Sitting']\n"
     ]
    }
   ],
   "source": [
    "# change from string to integer so keras.to_categorical can consume it\n",
    "\n",
    "# could do with factorize method as well\n",
    "n_class = len(np.unique(labels_array))\n",
    "class_list, labels_array_int = np.unique(labels_array,return_inverse=True)\n",
    "print(f'The are {len(class_list)} classes: {class_list}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels to categorical\n",
    "\n",
    "y = to_categorical(labels_array_int, num_classes=n_class)\n",
    "input_shape = list(accel_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "GscSnCpk8Jlm"
   },
   "source": [
    "\n",
    "#### Design neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best conv-lstm model!\n",
    "model = Sequential()\n",
    "model.add(Conv1D(n_conv_1, k_conv_1, activation=activation_conv, input_shape=input_shape[1:]))\n",
    "model.add(MaxPool1D(pool_size = maxpooling_pool_size))\n",
    "model.add(Conv1D(n_conv_2, k_conv_2, activation=activation_conv))\n",
    "model.add(AveragePooling1D(pool_size = avepooling_pool_size))\n",
    "model.add(Conv1D(n_conv_3, k_conv_3, activation=activation_conv))\n",
    "\n",
    "# LSTM layers\n",
    "model.add(LSTM(n_lstm_1, dropout=drop_lstm_1, return_sequences=True)) \n",
    "model.add(LSTM(n_lstm_2, dropout=drop_lstm_2, return_sequences=True))\n",
    "\n",
    "# Dense layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(n_dense_1, activation=activation_dense))\n",
    "model.add(Dropout(dropout_1))\n",
    "model.add(Dense(n_dense_2, activation=activation_dense))\n",
    "model.add(Dropout(dropout_2))\n",
    "model.add(Dense(n_class, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3nx7eGkg8Jlr",
    "outputId": "76af5145-5c7d-45c1-f292-e9f68a0907a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 89, 768)           5376      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 44, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 43, 512)           786944    \n",
      "_________________________________________________________________\n",
      "average_pooling1d (AveragePo (None, 21, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 20, 512)           524800    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 20, 128)           328192    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 20, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 384)               983424    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 384)               147840    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 2310      \n",
      "=================================================================\n",
      "Total params: 2,910,470\n",
      "Trainable params: 2,910,470\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "40x3s0KY8Jlv",
    "outputId": "d3ac3e31-3c47-4b01-c2f9-ada0684b31b7"
   },
   "source": [
    "### Splitting and shuffeling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f5grc2H78Jlz",
    "outputId": "3f7eeb55-f851-4796-de4e-0c68eb9df88d"
   },
   "outputs": [],
   "source": [
    "# split and shuffle\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "     accel_array, y, test_size=0.1, random_state=65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JkbhJMox8Jl9"
   },
   "source": [
    "#### Prepare to save each epoch\n",
    "The weights and a log for tensorboard are saved. Later we load back the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zHjvYe288JmE"
   },
   "outputs": [],
   "source": [
    "modelcheckpoint = ModelCheckpoint(filepath=output_dir+\n",
    "                                  \"/weights.{epoch:02d}.hdf5\")\n",
    "\n",
    "# Define the Keras TensorBoard callback.\n",
    "logdir=  \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a folder to save each epoch of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "esUwodZA8JmI"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((58278, 90, 3), (58278, 6), (6476, 90, 3), (6476, 6))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEYpX7968JmL"
   },
   "source": [
    "#### Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QaD1W7Ka8JmM",
    "outputId": "f0c30141-0962-48f6-a000-d136af50af79",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "  1/228 [..............................] - ETA: 0s - loss: 1.7924 - accuracy: 0.1914WARNING:tensorflow:From C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "  2/228 [..............................] - ETA: 14s - loss: 1.7750 - accuracy: 0.2539WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0319s vs `on_train_batch_end` time: 0.0934s). Check your callbacks.\n",
      "228/228 [==============================] - 18s 79ms/step - loss: 0.9398 - accuracy: 0.6033 - val_loss: 0.6706 - val_accuracy: 0.7353\n",
      "Epoch 2/120\n",
      "228/228 [==============================] - 17s 74ms/step - loss: 0.5654 - accuracy: 0.7771 - val_loss: 0.4588 - val_accuracy: 0.8173\n",
      "Epoch 3/120\n",
      "228/228 [==============================] - 16s 72ms/step - loss: 0.4089 - accuracy: 0.8427 - val_loss: 0.3703 - val_accuracy: 0.8544\n",
      "Epoch 4/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.3639 - accuracy: 0.8586 - val_loss: 0.3382 - val_accuracy: 0.8698\n",
      "Epoch 5/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.3203 - accuracy: 0.8753 - val_loss: 0.2788 - val_accuracy: 0.8930\n",
      "Epoch 6/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.2905 - accuracy: 0.8873 - val_loss: 0.2626 - val_accuracy: 0.9021\n",
      "Epoch 7/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.2711 - accuracy: 0.8968 - val_loss: 0.2676 - val_accuracy: 0.8972\n",
      "Epoch 8/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.2574 - accuracy: 0.9024 - val_loss: 0.2855 - val_accuracy: 0.8911\n",
      "Epoch 9/120\n",
      "228/228 [==============================] - 17s 74ms/step - loss: 0.2452 - accuracy: 0.9069 - val_loss: 0.2694 - val_accuracy: 0.9012\n",
      "Epoch 10/120\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 0.2314 - accuracy: 0.9123 - val_loss: 0.2294 - val_accuracy: 0.9117\n",
      "Epoch 11/120\n",
      "161/228 [====================>.........] - ETA: 4s - loss: 0.2224 - accuracy: 0.9153"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, \n",
    "         batch_size=batch_size, epochs=epochs, verbose=1, \n",
    "         validation_data=(x_valid, y_valid), \n",
    "         callbacks=[modelcheckpoint, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the best epoch's weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z8t0nVCw8JmP"
   },
   "outputs": [],
   "source": [
    "model.load_weights(output_dir+\"/weights.110.hdf5\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7PWlH5SJ8JmP"
   },
   "source": [
    "## Evaluate\n",
    "\n",
    "Make predictions and check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZwGk5dR8JmS"
   },
   "outputs": [],
   "source": [
    "y_hat = model.predict(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FTWf6Cf-8Jme",
    "outputId": "1256b170-33dc-4171-f2ff-7a3d7ddd74e3"
   },
   "outputs": [],
   "source": [
    "plt.hist(y_hat)\n",
    "_ = plt.axvline(x=0.5, color='orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram of probabilities for each class\n",
    "If most probabilities are around either zero or one, it means the model was confidant in classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_hat, columns=class_list)\n",
    "_ = df.hist(figsize=(15,10), color='brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RNljSx3v8Jmh"
   },
   "outputs": [],
   "source": [
    "# Calculate ROC and Accuracy\n",
    "pct_auc = roc_auc_score(y_valid, y_hat)*100.0\n",
    "y_pred_classes = y_hat.round()\n",
    "acc = score(y_valid, y_pred_classes) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SO_N9lJG8Jmj",
    "outputId": "20968a52-8ca0-44d3-abf6-a1382bba5cde"
   },
   "outputs": [],
   "source": [
    "print(\"The model accuracy is: {:0.2f}\".format(acc))\n",
    "print(\"The area under ROC curve is: {:0.2f}\".format(pct_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vU3hnA298Jmo"
   },
   "outputs": [],
   "source": [
    "# Convert the predicted outcomes to categorical variable to compare with validation data, just for checking\n",
    "\n",
    "float_y_hat = []\n",
    "for y in y_hat:\n",
    "    float_y_hat.append(y[0:7].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zrTf-aQl8Jmr"
   },
   "outputs": [],
   "source": [
    "ydf = pd.DataFrame(list(zip(float_y_hat, y_valid)), columns=['y_hat', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zAyKh_mX8Jmu",
    "outputId": "611224b8-f980-47df-d596-9aa3a5811009"
   },
   "outputs": [],
   "source": [
    "ydf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert probability to class\n",
    "y_pred_class = np.argmax(y_hat, axis=1)\n",
    "y_class = np.argmax(y_valid, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cf_matrix, annot=True, fmt = '1', xticklabels=class_list, yticklabels=class_list, cmap='ocean')\n",
    "# cf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot confusion matrix\n",
    "First use seaborn, then a custom stript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confucion matrix:\n",
    "cf_matrix = confusion_matrix(y_class, y_pred_class, labels=range(len(class_list)))\n",
    "\n",
    "\n",
    "# the classes are not in order, for example 'sitting' is not following 'lying'. Sort them:\n",
    "\n",
    "confusion_matrix_df = pd.DataFrame(cf_matrix, index=class_list, columns=class_list)\n",
    "confusion_matrix_df = confusion_matrix_df[['Lying','Sitting', 'Self Pace walk',\\\n",
    "                                           'Running 3 METs','Running 5 METs', 'Running 7 METs']]\n",
    "confusion_matrix_df = confusion_matrix_df.reindex(['Lying', 'Sitting', 'Self Pace walk',\\\n",
    "                                                   'Running 3 METs','Running 5 METs', 'Running 7 METs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (13,10))\n",
    "\n",
    "_ = sns.heatmap(confusion_matrix_df, annot=True, fmt = '1', cmap='rocket_r') # try cmap='rocket', 'PuBuGn', 'magma'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prettyconfusion as pf\n",
    "pf.pretty_plot_confusion_matrix(confusion_matrix_df,cmap='PuBuGn', cbar='oranges'\n",
    "                                ,fz = 13, lw=1, figsize=[12,12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize performance metrics and the model's structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs if you don't want to see it in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All stored weights have the same structure. Load one of them to netron and save the image \n",
    "netron.start(file=output_dir+\"/weights.110.hdf5\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "dense_sentiment_classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
