{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook prepares the data for NN models\n",
    "In this Notebook we \n",
    "- Remove some of the reading at the beginning and the end of each activity. This eliminates the noise presented when the participant is changing activity.\n",
    "\n",
    "- Select the acceleration columns, and reshape them.\n",
    "\n",
    "- Select the participants' demographic data such as age, weight, etc. Then reshape them to be aligned with acceleration data.\n",
    "\n",
    "- Extract the labels with the same method.\n",
    "\n",
    "**Note** that we used the NumPy module, and the labels are not one-hot encoded, and the data is not shuffled.\n",
    "\n",
    "You can change the parameters to manipulate this process.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data frame dictionary:\n",
    "- *df* = the raw csv dataframe\n",
    "- *df_clean* = final cleaned and processed dataframe\n",
    "- *df_level* = df for each activity level\n",
    "- *df_level_clean* = clean version of df_level\n",
    "- *df_temp* = a helper dtaframe to store temporary data for each participant and each level\n",
    "\n",
    "#### numPy array dictionary\n",
    "- accel_array, contains x,y, and z acceleration and a shape of (number_of_sequences, lenght_of_each_seq, number_of_axis i.e. 3)\n",
    "- meta_array, has the demographic data and its size is (number_of_sequences, len(mata_column_list) \n",
    "- label_array, contains the labels for each sequence with a size of number_of_sequences, 1)\n",
    "\n",
    "#### variable dictionary:\n",
    "- n_ignore: number of reading to ignore from the beginning and the end of each activity. If set to 600, ignores 20 seconds as the frequency is 30Hz\n",
    "- window_size_second: length of the window used for sequencing in seconds. Each window_size_second is a sequence \n",
    "- frequency: Of the accelerometer\n",
    "- lenght_of_each_seq: window_size_second * frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "n_ignore = 600 # ignores 20 sec with a frequency of 30 Hz\n",
    "window_size_second = 3\n",
    "frequency = 30\n",
    "lenght_of_each_seq = window_size_second * frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = 'pocket'\n",
    "input_dir =  'Z:/Research/dfuller/Walkabilly/studies/smarphone_accel/data/Ethica_Jaeger_Merged/'\n",
    "input_file_name = 'pocket_with_couns_and_vec_meg_30Hz.csv'\n",
    "file_path = os.path.join(input_dir, location, input_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = 'backpack'\n",
    "input_dir =  'Z:/Research/dfuller/Walkabilly/studies/smarphone_accel/data/Ethica_Jaeger_Merged/'\n",
    "input_file_name = 'backpack_with_counts_30Hz.csv'\n",
    "file_path = os.path.join(input_dir, location, input_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = 'hand'\n",
    "input_dir =  'Z:/Research/dfuller/Walkabilly/studies/smarphone_accel/data/Ethica_Jaeger_Merged/'\n",
    "input_file_name = 'hand_with_counts_30Hz.csv'\n",
    "file_path = os.path.join(input_dir, location, input_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the noise \n",
    "\n",
    "Igonre n_ignore item from the beginning and the end of each activity for each person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_list = list(df.participant_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[108,\n",
       " 111,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 123,\n",
       " 127,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 128,\n",
       " 133,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 159,\n",
       " 142,\n",
       " 143,\n",
       " 109,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 149,\n",
       " 152,\n",
       " 150,\n",
       " 151,\n",
       " 153,\n",
       " 139,\n",
       " 134,\n",
       " 141,\n",
       " 155,\n",
       " 140,\n",
       " 154,\n",
       " 156,\n",
       " 157]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 112 , 121 .122 , and 132 as they are not properly classified.\n",
    "misclass_participants = [112,121,122,132]\n",
    "# participant_list.remove()\n",
    "participant_list = [elem for elem in participant_list if elem not in misclass_participants]\n",
    "participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_time</th>\n",
       "      <th>x_axis</th>\n",
       "      <th>y_axis</th>\n",
       "      <th>z_axis</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>wear_location</th>\n",
       "      <th>activity</th>\n",
       "      <th>trimmed_activity</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>counts_vec_mag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-07T10:30:01Z</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.470003</td>\n",
       "      <td>-0.939727</td>\n",
       "      <td>108</td>\n",
       "      <td>hand</td>\n",
       "      <td>1-Lying</td>\n",
       "      <td>Lying</td>\n",
       "      <td>164.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>30</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>77.87811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-07T10:30:01.033300Z</td>\n",
       "      <td>-0.000189</td>\n",
       "      <td>0.467589</td>\n",
       "      <td>-0.939440</td>\n",
       "      <td>108</td>\n",
       "      <td>hand</td>\n",
       "      <td>1-Lying</td>\n",
       "      <td>Lying</td>\n",
       "      <td>164.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>30</td>\n",
       "      <td>Female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77.87811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   record_time    x_axis    y_axis    z_axis  participant_id  \\\n",
       "0         2019-01-07T10:30:01Z  0.000011  0.470003 -0.939727             108   \n",
       "1  2019-01-07T10:30:01.033300Z -0.000189  0.467589 -0.939440             108   \n",
       "\n",
       "  wear_location activity trimmed_activity  height  weight  age  gender    x  \\\n",
       "0          hand  1-Lying            Lying   164.0    68.0   30  Female  0.0   \n",
       "1          hand  1-Lying            Lying   164.0    68.0   30  Female  NaN   \n",
       "\n",
       "      y     z  counts_vec_mag  \n",
       "0  32.0  71.0        77.87811  \n",
       "1   NaN   NaN        77.87811  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select important columns, x, y, z, height, weight, age, gender, also participant_id for cleaning. remove it later\n",
    "important_columns = ['x_axis','y_axis','z_axis','participant_id','trimmed_activity','height','weight','age','gender']\n",
    "df = df[important_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# change gender to dummy\n",
    "df.gender[df['gender']=='Female'] = 0\n",
    "df.gender[df['gender']=='Male'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_axis</th>\n",
       "      <th>y_axis</th>\n",
       "      <th>z_axis</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>trimmed_activity</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.470003</td>\n",
       "      <td>-0.939727</td>\n",
       "      <td>108</td>\n",
       "      <td>Lying</td>\n",
       "      <td>164.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000189</td>\n",
       "      <td>0.467589</td>\n",
       "      <td>-0.939440</td>\n",
       "      <td>108</td>\n",
       "      <td>Lying</td>\n",
       "      <td>164.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.456086</td>\n",
       "      <td>-0.924464</td>\n",
       "      <td>108</td>\n",
       "      <td>Lying</td>\n",
       "      <td>164.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x_axis    y_axis    z_axis  participant_id trimmed_activity  height  \\\n",
       "0  0.000011  0.470003 -0.939727             108            Lying   164.0   \n",
       "1 -0.000189  0.467589 -0.939440             108            Lying   164.0   \n",
       "2  0.000174  0.456086 -0.924464             108            Lying   164.0   \n",
       "\n",
       "   weight  age gender  \n",
       "0    68.0   30      0  \n",
       "1    68.0   30      0  \n",
       "2    68.0   30      0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on Lying level\n",
      "working on 108 participant\n",
      "working on 111 participant\n",
      "working on 113 participant\n",
      "working on 114 participant\n",
      "working on 115 participant\n",
      "working on 116 participant\n",
      "working on 117 participant\n",
      "working on 118 participant\n",
      "working on 119 participant\n",
      "working on 120 participant\n",
      "working on 124 participant\n",
      "working on 125 participant\n",
      "working on 126 participant\n",
      "working on 123 participant\n",
      "working on 127 participant\n",
      "working on 129 participant\n",
      "working on 130 participant\n",
      "working on 131 participant\n",
      "working on 128 participant\n",
      "working on 133 participant\n",
      "working on 136 participant\n",
      "working on 137 participant\n",
      "working on 138 participant\n",
      "working on 159 participant\n",
      "working on 142 participant\n",
      "working on 143 participant\n",
      "working on 109 participant\n",
      "working on 144 participant\n",
      "working on 145 participant\n",
      "working on 146 participant\n",
      "working on 147 participant\n",
      "working on 149 participant\n",
      "working on 152 participant\n",
      "working on 150 participant\n",
      "working on 151 participant\n",
      "working on 153 participant\n",
      "working on 139 participant\n",
      "working on 134 participant\n",
      "working on 141 participant\n",
      "working on 155 participant\n",
      "working on 140 participant\n",
      "working on 154 participant\n",
      "working on 156 participant\n",
      "working on 157 participant\n",
      "working on Sitting level\n",
      "working on 108 participant\n",
      "working on 111 participant\n",
      "working on 113 participant\n",
      "working on 114 participant\n",
      "working on 115 participant\n",
      "working on 116 participant\n",
      "working on 117 participant\n",
      "working on 118 participant\n",
      "working on 119 participant\n",
      "working on 120 participant\n",
      "working on 124 participant\n",
      "working on 125 participant\n",
      "working on 126 participant\n",
      "working on 123 participant\n",
      "working on 127 participant\n",
      "working on 129 participant\n",
      "working on 130 participant\n",
      "working on 131 participant\n",
      "working on 128 participant\n",
      "working on 133 participant\n",
      "working on 136 participant\n",
      "working on 137 participant\n",
      "working on 138 participant\n",
      "working on 159 participant\n",
      "working on 142 participant\n",
      "working on 143 participant\n",
      "working on 109 participant\n",
      "working on 144 participant\n",
      "working on 145 participant\n",
      "working on 146 participant\n",
      "working on 147 participant\n",
      "working on 149 participant\n",
      "working on 152 participant\n",
      "working on 150 participant\n",
      "working on 151 participant\n",
      "working on 153 participant\n",
      "working on 139 participant\n",
      "working on 134 participant\n",
      "working on 141 participant\n",
      "working on 155 participant\n",
      "working on 140 participant\n",
      "working on 154 participant\n",
      "working on 156 participant\n",
      "working on 157 participant\n",
      "working on Self Pace walk level\n",
      "working on 108 participant\n",
      "working on 111 participant\n",
      "working on 113 participant\n",
      "working on 114 participant\n",
      "working on 115 participant\n",
      "working on 116 participant\n",
      "working on 117 participant\n",
      "working on 118 participant\n",
      "working on 119 participant\n",
      "working on 120 participant\n",
      "working on 124 participant\n",
      "working on 125 participant\n",
      "working on 126 participant\n",
      "working on 123 participant\n",
      "working on 127 participant\n",
      "working on 129 participant\n",
      "working on 130 participant\n",
      "working on 131 participant\n",
      "working on 128 participant\n",
      "working on 133 participant\n",
      "working on 136 participant\n",
      "working on 137 participant\n",
      "working on 138 participant\n",
      "working on 159 participant\n",
      "working on 142 participant\n",
      "working on 143 participant\n",
      "working on 109 participant\n",
      "working on 144 participant\n",
      "working on 145 participant\n",
      "working on 146 participant\n",
      "working on 147 participant\n",
      "working on 149 participant\n",
      "working on 152 participant\n",
      "working on 150 participant\n",
      "working on 151 participant\n",
      "working on 153 participant\n",
      "working on 139 participant\n",
      "working on 134 participant\n",
      "working on 141 participant\n",
      "working on 155 participant\n",
      "working on 140 participant\n",
      "working on 154 participant\n",
      "working on 156 participant\n",
      "working on 157 participant\n",
      "working on Running 3 METs level\n",
      "working on 108 participant\n",
      "working on 111 participant\n",
      "working on 113 participant\n",
      "working on 114 participant\n",
      "working on 115 participant\n",
      "working on 116 participant\n",
      "working on 117 participant\n",
      "working on 118 participant\n",
      "working on 119 participant\n",
      "working on 120 participant\n",
      "working on 124 participant\n",
      "working on 125 participant\n",
      "working on 126 participant\n",
      "working on 123 participant\n",
      "working on 127 participant\n",
      "working on 129 participant\n",
      "working on 130 participant\n",
      "working on 131 participant\n",
      "working on 128 participant\n",
      "working on 133 participant\n",
      "working on 136 participant\n",
      "working on 137 participant\n",
      "working on 138 participant\n",
      "working on 159 participant\n",
      "working on 142 participant\n",
      "working on 143 participant\n",
      "working on 109 participant\n",
      "working on 144 participant\n",
      "working on 145 participant\n",
      "working on 146 participant\n",
      "working on 147 participant\n",
      "working on 149 participant\n",
      "working on 152 participant\n",
      "working on 150 participant\n",
      "working on 151 participant\n",
      "working on 153 participant\n",
      "working on 139 participant\n",
      "working on 134 participant\n",
      "working on 141 participant\n",
      "working on 155 participant\n",
      "working on 140 participant\n",
      "working on 154 participant\n",
      "working on 156 participant\n",
      "working on 157 participant\n",
      "working on Running 5 METs level\n",
      "working on 108 participant\n",
      "working on 111 participant\n",
      "working on 113 participant\n",
      "working on 114 participant\n",
      "working on 115 participant\n",
      "working on 116 participant\n",
      "working on 117 participant\n",
      "working on 118 participant\n",
      "working on 119 participant\n",
      "working on 120 participant\n",
      "working on 124 participant\n",
      "working on 125 participant\n",
      "working on 126 participant\n",
      "working on 123 participant\n",
      "working on 127 participant\n",
      "working on 129 participant\n",
      "working on 130 participant\n",
      "working on 131 participant\n",
      "working on 128 participant\n",
      "working on 133 participant\n",
      "working on 136 participant\n",
      "working on 137 participant\n",
      "working on 138 participant\n",
      "working on 159 participant\n",
      "working on 142 participant\n",
      "working on 143 participant\n",
      "working on 109 participant\n",
      "working on 144 participant\n",
      "working on 145 participant\n",
      "working on 146 participant\n",
      "working on 147 participant\n",
      "working on 149 participant\n",
      "working on 152 participant\n",
      "working on 150 participant\n",
      "working on 151 participant\n",
      "working on 153 participant\n",
      "working on 139 participant\n",
      "working on 134 participant\n",
      "working on 141 participant\n",
      "working on 155 participant\n",
      "working on 140 participant\n",
      "working on 154 participant\n",
      "working on 156 participant\n",
      "working on 157 participant\n",
      "working on Running 7 METs level\n",
      "working on 108 participant\n",
      "working on 111 participant\n",
      "working on 113 participant\n",
      "working on 114 participant\n",
      "working on 115 participant\n",
      "working on 116 participant\n",
      "working on 117 participant\n",
      "working on 118 participant\n",
      "working on 119 participant\n",
      "working on 120 participant\n",
      "working on 124 participant\n",
      "working on 125 participant\n",
      "working on 126 participant\n",
      "working on 123 participant\n",
      "working on 127 participant\n",
      "working on 129 participant\n",
      "working on 130 participant\n",
      "working on 131 participant\n",
      "working on 128 participant\n",
      "working on 133 participant\n",
      "working on 136 participant\n",
      "working on 137 participant\n",
      "working on 138 participant\n",
      "working on 159 participant\n",
      "working on 142 participant\n",
      "working on 143 participant\n",
      "working on 109 participant\n",
      "working on 144 participant\n",
      "working on 145 participant\n",
      "working on 146 participant\n",
      "working on 147 participant\n",
      "working on 149 participant\n",
      "working on 152 participant\n",
      "working on 150 participant\n",
      "working on 151 participant\n",
      "working on 153 participant\n",
      "working on 139 participant\n",
      "working on 134 participant\n",
      "working on 141 participant\n",
      "working on 155 participant\n",
      "working on 140 participant\n",
      "working on 154 participant\n",
      "working on 156 participant\n",
      "working on 157 participant\n"
     ]
    }
   ],
   "source": [
    "# repeat for all PE levels \n",
    "\n",
    "# get levels to loop thru\n",
    "PE_levels = df.trimmed_activity.unique()\n",
    "\n",
    "# ceate empty df\n",
    "df_clean = pd.DataFrame(columns = important_columns)\n",
    "\n",
    "for level in PE_levels:\n",
    "    print(\"working on {} level\".format(level))\n",
    "    df_level = df[df['trimmed_activity'] == level]\n",
    "    df_level_clean = pd.DataFrame(columns = important_columns)\n",
    "\n",
    "\n",
    "    for partip in participant_list:\n",
    "        df_temp = df_level[df_level['participant_id'] == partip]\n",
    "        df_temp_nrow = df_temp.shape[0]\n",
    "        # ignore the noisy data in the beginning and the end\n",
    "        df_temp = df_temp.iloc[n_ignore:df_temp_nrow-n_ignore,]\n",
    "        \n",
    "        # make it devisable by sequence length\n",
    "        number_of_sequences = df_temp.shape[0] // lenght_of_each_seq\n",
    "        n_row= number_of_sequences * lenght_of_each_seq\n",
    "        df_temp = df_temp.iloc[:n_row,]\n",
    "    \n",
    "        df_level_clean = pd.concat([df_level_clean, df_temp])\n",
    "        print(\"working on {} participant\".format(partip))\n",
    "\n",
    "\n",
    "    df_clean = pd.concat([df_clean, df_level_clean])\n",
    "\n",
    "  \n",
    "# df_clean = df_clean.drop('participant_id', axis = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to create something like an 1-D image. so we can feed it to CNN.\n",
    "For image processing, an image has three channels, and two dimenssion. So for a 264*264 pixel image,the shape is:\n",
    "264, 264, 3\n",
    "\n",
    "In our case, if we use a window of 3 second we have 90 reading(30 Hz), similar to pixel number in images. And we have 3 dimenssion,z,y, andz so the input shape is (90,3)\n",
    "\n",
    "Now if we have n input (n sequesnces or n images), the inout shape is (n,90,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator\n",
    "\n",
    "### Create sequence of acceleration data\n",
    "\n",
    "For each axis we do:\n",
    "\n",
    "\n",
    "Get the axis and put in a numpy array\n",
    "\n",
    "reshape it to (n,1) where n is the total length of acceleration data for a specific activity and person\n",
    "\n",
    "stack all the axis horizontally. e.i. bind columns\n",
    "\n",
    "reshape to (number_of_sequences, lenght_of_each_seq, number_of_axis i.e. 3) \n",
    "\n",
    "Note that n =number_of_sequences * lenght_of_each_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create meta data and labels\n",
    "\n",
    "\n",
    "How to create meta data:\n",
    "- follow the pre cell, but don't need to stack anything. We will process age, gender, labesl, etc separetely.\n",
    "- the dim is **number_of_sequences, lenght_of_each_seq,**\n",
    "- use numpy max and get the max (or min) and reduce the matrix to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_axis</th>\n",
       "      <th>y_axis</th>\n",
       "      <th>z_axis</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>trimmed_activity</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>0.002324</td>\n",
       "      <td>0.461016</td>\n",
       "      <td>-0.952328</td>\n",
       "      <td>108</td>\n",
       "      <td>Lying</td>\n",
       "      <td>164.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>0.003189</td>\n",
       "      <td>0.461295</td>\n",
       "      <td>-0.949071</td>\n",
       "      <td>108</td>\n",
       "      <td>Lying</td>\n",
       "      <td>164.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>0.003026</td>\n",
       "      <td>0.453003</td>\n",
       "      <td>-0.926937</td>\n",
       "      <td>108</td>\n",
       "      <td>Lying</td>\n",
       "      <td>164.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>0.001850</td>\n",
       "      <td>0.450760</td>\n",
       "      <td>-0.921899</td>\n",
       "      <td>108</td>\n",
       "      <td>Lying</td>\n",
       "      <td>164.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.451368</td>\n",
       "      <td>-0.925682</td>\n",
       "      <td>108</td>\n",
       "      <td>Lying</td>\n",
       "      <td>164.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       x_axis    y_axis    z_axis participant_id trimmed_activity  height  \\\n",
       "600  0.002324  0.461016 -0.952328            108            Lying   164.0   \n",
       "601  0.003189  0.461295 -0.949071            108            Lying   164.0   \n",
       "602  0.003026  0.453003 -0.926937            108            Lying   164.0   \n",
       "603  0.001850  0.450760 -0.921899            108            Lying   164.0   \n",
       "604  0.000245  0.451368 -0.925682            108            Lying   164.0   \n",
       "\n",
       "     weight age gender  \n",
       "600    68.0  30      0  \n",
       "601    68.0  30      0  \n",
       "602    68.0  30      0  \n",
       "603    68.0  30      0  \n",
       "604    68.0  30      0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acceleration sequense data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will have  64726  sequences to available.\n",
      "(5825340, 1)\n",
      "(5825340, 2)\n",
      "(5825340, 3)\n",
      "(64726, 90, 3)\n"
     ]
    }
   ],
   "source": [
    "# sequence generator\n",
    "# output size (number_of_sequences, lenght_of_each_seq, number_of_axis i.e. 3)\n",
    "\n",
    "n_row = df_clean.shape[0]\n",
    "number_of_sequences = int(n_row / lenght_of_each_seq)\n",
    "print(\"We will have \", number_of_sequences ,\" sequences to available.\")\n",
    "accel_array = np.empty((n_row,0))\n",
    "\n",
    "\n",
    "# repeat for all axes\n",
    "axes_list = ['x_axis','y_axis','z_axis']\n",
    "for axis in axes_list:\n",
    "\n",
    "    # filter based on axis\n",
    "    working_array = df_clean[axis]\n",
    "\n",
    "    working_array = np.array(working_array).reshape(n_row,1)\n",
    "    accel_array = np.hstack((accel_array, working_array))\n",
    "    \n",
    "    print(accel_array.shape)\n",
    "n_axis = len(axes_list)\n",
    "accel_array = accel_array.reshape((number_of_sequences, lenght_of_each_seq, 3)) \n",
    "print(accel_array.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meta data generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will have  64726  sequences to available.\n",
      "(64726, 1)      (64726, 90, 1)\n",
      "(64726, 1)      (64726, 90, 1)\n",
      "(64726, 1)      (64726, 90, 1)\n",
      "(64726, 1)      (64726, 90, 1)\n",
      "(64726, 4)\n"
     ]
    }
   ],
   "source": [
    "# has the same logic as accelereation sequence generator\n",
    "# for each column output size (number_of_sequences,  1)\n",
    "# for all of them, the out put in meta_array  size ((number_of_sequences, len(mata_column_list)))\n",
    "\n",
    "\n",
    "\n",
    "n_row = df_clean.shape[0]\n",
    "number_of_sequences = int(n_row / lenght_of_each_seq)\n",
    "print(\"We will have \", number_of_sequences ,\" sequences to available.\")\n",
    "# repeat for all meta data columns \n",
    "meta_column_list = ['height','weight','age','gender']\n",
    "\n",
    "\n",
    "compressed_array = np.empty((number_of_sequences,1))\n",
    "meta_array = np.empty((number_of_sequences, 0))\n",
    "\n",
    "for meta in meta_column_list:\n",
    "\n",
    "    # filter based on meta data column\n",
    "    working_array = df_clean[meta]\n",
    "\n",
    "    working_array = np.array(working_array).reshape(number_of_sequences, lenght_of_each_seq, 1)\n",
    "    for i in range(number_of_sequences):\n",
    "        compressed_array[i] = working_array[i,].max()\n",
    "    \n",
    "    meta_array = np.hstack((meta_array, compressed_array))\n",
    "    print(compressed_array.shape, \"    \" , working_array.shape)\n",
    "print(meta_array.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will have  64726  sequences to available.\n",
      "(64726, 90, 1)\n",
      "(64726, 1)\n",
      "[['Lying']\n",
      " ['Lying']\n",
      " ['Lying']\n",
      " ...\n",
      " ['Running 7 METs']\n",
      " ['Running 7 METs']\n",
      " ['Running 7 METs']]\n"
     ]
    }
   ],
   "source": [
    "# repeat for  trimmed activity which is the labels\n",
    "# has the same logic as accelereation sequence generator\n",
    "# for labels output size (number_of_sequences,  1)\n",
    "\n",
    "n_row = df_clean.shape[0]\n",
    "number_of_sequences = int(n_row / lenght_of_each_seq)\n",
    "print(\"We will have \", number_of_sequences ,\" sequences to available.\")\n",
    "label_column_list = ['trimmed_activity']\n",
    "\n",
    "\n",
    "label_array = np.empty((number_of_sequences,1), dtype=list)\n",
    "\n",
    "# as we only have one outcome ( label) we don;t need another array to store all of them\n",
    "# We could do it without the for loop as well\n",
    "for label in label_column_list:\n",
    "    # filter based on the column\n",
    "    working_array = df_clean[label]\n",
    "    working_array = np.array(working_array).reshape(number_of_sequences, lenght_of_each_seq, 1)\n",
    "    print(working_array.shape)\n",
    "    for i in range(number_of_sequences):\n",
    "        label_array[i] = working_array[i,0] \n",
    "print(label_array.shape)\n",
    "print(label_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check to see if the data is intact\n",
    "\n",
    "We ignored n_ignore data from the beginning of each activity. Therefore the first processed data in n_ignore th +1 data in the raw data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we start from  600 th when indexing from the raw dataframe\n",
      "[ 0.00318886  0.46129513 -0.94907142]\n",
      "[164.  68.  30.   0.]\n",
      "['Lying']\n",
      "       x_axis    y_axis    z_axis  participant_id trimmed_activity  height  \\\n",
      "600  0.002324  0.461016 -0.952328             108            Lying   164.0   \n",
      "601  0.003189  0.461295 -0.949071             108            Lying   164.0   \n",
      "602  0.003026  0.453003 -0.926937             108            Lying   164.0   \n",
      "\n",
      "     weight  age gender  \n",
      "600    68.0   30      0  \n",
      "601    68.0   30      0  \n",
      "602    68.0   30      0  \n"
     ]
    }
   ],
   "source": [
    "print(\"we start from \",n_ignore,\"th when indexing from the raw dataframe\")\n",
    "print(accel_array[0,1])\n",
    "print(meta_array[0])\n",
    "print(label_array[0])\n",
    "print(df.iloc[n_ignore:n_ignore+3,])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the results as numpy objects\n",
    "output_file_name = input_dir +location + \"/\" + location + '-NN-data'\n",
    "np.savez_compressed(output_file_name,\n",
    "                    acceleration_data=accel_array,\n",
    "                    metadata=meta_array,\n",
    "                    labels=label_array\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Z:/Research/dfuller/Walkabilly/studies/smarphone_accel/data/Ethica_Jaeger_Merged/hand/hand-NN-data'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
