{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook prepares the data for NN models\n",
    "In this Notebook we \n",
    "- Remove some of the reading at the beginning and the end of each activity. This eliminates the noise presented when the participant is changing activity.\n",
    "\n",
    "- Select the acceleration columns, and reshape them.\n",
    "\n",
    "- Select the participants' demographic data such as age, weight, etc. Then reshape them to be aligned with acceleration data.\n",
    "\n",
    "- Extract the labels with the same method.\n",
    "\n",
    "**Note** that we used the NumPy module, and the labels are not one-hot encoded, and the data is not shuffled.\n",
    "\n",
    "You can change the parameters to manipulate this process.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data frame dictionary:\n",
    "- *df* = the raw csv dataframe\n",
    "- *df_clean* = final cleaned and processed dataframe\n",
    "- *df_level* = df for each activity level\n",
    "- *df_level_clean* = clean version of df_level\n",
    "- *df_temp* = a helper dtaframe to store temporary data for each participant and each level\n",
    "\n",
    "#### numPy array dictionary\n",
    "- accel_array, contains x,y, and z acceleration and a shape of (number_of_sequences, lenght_of_each_seq, number_of_axis i.e. 3)\n",
    "- meta_array, has the demographic data and its size is (number_of_sequences, len(mata_column_list) \n",
    "- label_array, contains the labels for each sequence with a size of number_of_sequences, 1)\n",
    "\n",
    "#### variable dictionary:\n",
    "- n_ignore: number of reading to ignore from the beginning and the end of each activity. If set to 600, ignores 20 seconds as the frequency is 30Hz\n",
    "- window_size_second: length of the window used for sequencing in seconds. Each window_size_second is a sequence \n",
    "- frequency: Of the accelerometer\n",
    "- lenght_of_each_seq: window_size_second * frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "n_ignore = 600 # ignores 20 sec with a frequency of 30 Hz\n",
    "window_size_second = 3\n",
    "frequency = 30\n",
    "lenght_of_each_seq = window_size_second * frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir =  'Z:/Research/dfuller/Walkabilly/studies/smarphone_accel/data/Ethica_Jaeger_Merged/pocket/'\n",
    "input_file_name = 'pocket_with_couns_and_vec_meg_30Hz.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_dir + input_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break it down to several CSV file, Lying, Sitting, Walking, Running3, Running5, Running7\n",
    "\n",
    "Keep gender, weight and height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_list = list(df.participant_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_time</th>\n",
       "      <th>x_axis</th>\n",
       "      <th>y_axis</th>\n",
       "      <th>z_axis</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>wear_location</th>\n",
       "      <th>V.O2</th>\n",
       "      <th>VO2.kg</th>\n",
       "      <th>V.CO2</th>\n",
       "      <th>V.E</th>\n",
       "      <th>trimmed_activity</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>counts_vec_mag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-07T10:30:01Z</td>\n",
       "      <td>0.266111</td>\n",
       "      <td>-0.069464</td>\n",
       "      <td>0.966880</td>\n",
       "      <td>108</td>\n",
       "      <td>pock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lying</td>\n",
       "      <td>164.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>30</td>\n",
       "      <td>Female</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>74.1485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-07T10:30:01.033300Z</td>\n",
       "      <td>0.266401</td>\n",
       "      <td>-0.069351</td>\n",
       "      <td>0.963832</td>\n",
       "      <td>108</td>\n",
       "      <td>pock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lying</td>\n",
       "      <td>164.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>30</td>\n",
       "      <td>Female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74.1485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   record_time    x_axis    y_axis    z_axis  participant_id  \\\n",
       "0         2019-01-07T10:30:01Z  0.266111 -0.069464  0.966880             108   \n",
       "1  2019-01-07T10:30:01.033300Z  0.266401 -0.069351  0.963832             108   \n",
       "\n",
       "  wear_location  V.O2  VO2.kg  V.CO2  V.E trimmed_activity  height  weight  \\\n",
       "0          pock   NaN     NaN    NaN  NaN            Lying   164.0    68.0   \n",
       "1          pock   NaN     NaN    NaN  NaN            Lying   164.0    68.0   \n",
       "\n",
       "   age  gender     x    y     z  counts_vec_mag  \n",
       "0   30  Female  13.0  0.0  73.0         74.1485  \n",
       "1   30  Female   NaN  NaN   NaN         74.1485  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select important columns, x, y, z, height, weight, age, gender, also participant_id for cleaning. remove it later\n",
    "important_columns = ['x_axis','y_axis','z_axis','participant_id','trimmed_activity','height','weight','age','gender']\n",
    "df = df[important_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# change gender to dummy\n",
    "df.gender[df['gender']=='Female'] = 0\n",
    "df.gender[df['gender']=='Male'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_axis</th>\n",
       "      <th>y_axis</th>\n",
       "      <th>z_axis</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>trimmed_activity</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.266111</td>\n",
       "      <td>-0.069464</td>\n",
       "      <td>0.966880</td>\n",
       "      <td>108</td>\n",
       "      <td>Lying</td>\n",
       "      <td>164.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.266401</td>\n",
       "      <td>-0.069351</td>\n",
       "      <td>0.963832</td>\n",
       "      <td>108</td>\n",
       "      <td>Lying</td>\n",
       "      <td>164.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.262441</td>\n",
       "      <td>-0.067394</td>\n",
       "      <td>0.943335</td>\n",
       "      <td>108</td>\n",
       "      <td>Lying</td>\n",
       "      <td>164.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x_axis    y_axis    z_axis  participant_id trimmed_activity  height  \\\n",
       "0  0.266111 -0.069464  0.966880             108            Lying   164.0   \n",
       "1  0.266401 -0.069351  0.963832             108            Lying   164.0   \n",
       "2  0.262441 -0.067394  0.943335             108            Lying   164.0   \n",
       "\n",
       "   weight  age gender  \n",
       "0    68.0   30      0  \n",
       "1    68.0   30      0  \n",
       "2    68.0   30      0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on Lying level\n",
      "working on 108 participant\n",
      "working on 111 participant\n",
      "working on 112 participant\n",
      "working on 113 participant\n",
      "working on 114 participant\n",
      "working on 115 participant\n",
      "working on 116 participant\n",
      "working on 117 participant\n",
      "working on 118 participant\n",
      "working on 119 participant\n",
      "working on 120 participant\n",
      "working on 121 participant\n",
      "working on 122 participant\n",
      "working on 124 participant\n",
      "working on 125 participant\n",
      "working on 126 participant\n",
      "working on 123 participant\n",
      "working on 127 participant\n",
      "working on 129 participant\n",
      "working on 130 participant\n",
      "working on 131 participant\n",
      "working on 132 participant\n",
      "working on 128 participant\n",
      "working on 133 participant\n",
      "working on 136 participant\n",
      "working on 137 participant\n",
      "working on 138 participant\n",
      "working on 159 participant\n",
      "working on 142 participant\n",
      "working on 143 participant\n",
      "working on 109 participant\n",
      "working on 144 participant\n",
      "working on 145 participant\n",
      "working on 146 participant\n",
      "working on 147 participant\n",
      "working on 149 participant\n",
      "working on 152 participant\n",
      "working on 150 participant\n",
      "working on 151 participant\n",
      "working on 153 participant\n",
      "working on 139 participant\n",
      "working on 134 participant\n",
      "working on 141 participant\n",
      "working on 155 participant\n",
      "working on 140 participant\n",
      "working on 154 participant\n",
      "working on 156 participant\n",
      "working on 157 participant\n",
      "working on Sitting level\n",
      "working on 108 participant\n",
      "working on 111 participant\n",
      "working on 112 participant\n",
      "working on 113 participant\n",
      "working on 114 participant\n",
      "working on 115 participant\n",
      "working on 116 participant\n",
      "working on 117 participant\n",
      "working on 118 participant\n",
      "working on 119 participant\n",
      "working on 120 participant\n",
      "working on 121 participant\n",
      "working on 122 participant\n",
      "working on 124 participant\n",
      "working on 125 participant\n",
      "working on 126 participant\n",
      "working on 123 participant\n",
      "working on 127 participant\n",
      "working on 129 participant\n",
      "working on 130 participant\n",
      "working on 131 participant\n",
      "working on 132 participant\n",
      "working on 128 participant\n",
      "working on 133 participant\n",
      "working on 136 participant\n",
      "working on 137 participant\n",
      "working on 138 participant\n",
      "working on 159 participant\n",
      "working on 142 participant\n",
      "working on 143 participant\n",
      "working on 109 participant\n",
      "working on 144 participant\n",
      "working on 145 participant\n",
      "working on 146 participant\n",
      "working on 147 participant\n",
      "working on 149 participant\n",
      "working on 152 participant\n",
      "working on 150 participant\n",
      "working on 151 participant\n",
      "working on 153 participant\n",
      "working on 139 participant\n",
      "working on 134 participant\n",
      "working on 141 participant\n",
      "working on 155 participant\n",
      "working on 140 participant\n",
      "working on 154 participant\n",
      "working on 156 participant\n",
      "working on 157 participant\n",
      "working on Self Pace walk level\n",
      "working on 108 participant\n",
      "working on 111 participant\n",
      "working on 112 participant\n",
      "working on 113 participant\n",
      "working on 114 participant\n",
      "working on 115 participant\n",
      "working on 116 participant\n",
      "working on 117 participant\n",
      "working on 118 participant\n",
      "working on 119 participant\n",
      "working on 120 participant\n",
      "working on 121 participant\n",
      "working on 122 participant\n",
      "working on 124 participant\n",
      "working on 125 participant\n",
      "working on 126 participant\n",
      "working on 123 participant\n",
      "working on 127 participant\n",
      "working on 129 participant\n",
      "working on 130 participant\n",
      "working on 131 participant\n",
      "working on 132 participant\n",
      "working on 128 participant\n",
      "working on 133 participant\n",
      "working on 136 participant\n",
      "working on 137 participant\n",
      "working on 138 participant\n",
      "working on 159 participant\n",
      "working on 142 participant\n",
      "working on 143 participant\n",
      "working on 109 participant\n",
      "working on 144 participant\n",
      "working on 145 participant\n",
      "working on 146 participant\n",
      "working on 147 participant\n",
      "working on 149 participant\n",
      "working on 152 participant\n",
      "working on 150 participant\n",
      "working on 151 participant\n",
      "working on 153 participant\n",
      "working on 139 participant\n",
      "working on 134 participant\n",
      "working on 141 participant\n",
      "working on 155 participant\n",
      "working on 140 participant\n",
      "working on 154 participant\n",
      "working on 156 participant\n",
      "working on 157 participant\n",
      "working on Running 3 METs level\n",
      "working on 108 participant\n",
      "working on 111 participant\n",
      "working on 112 participant\n",
      "working on 113 participant\n",
      "working on 114 participant\n",
      "working on 115 participant\n",
      "working on 116 participant\n",
      "working on 117 participant\n",
      "working on 118 participant\n",
      "working on 119 participant\n",
      "working on 120 participant\n",
      "working on 121 participant\n",
      "working on 122 participant\n",
      "working on 124 participant\n",
      "working on 125 participant\n",
      "working on 126 participant\n",
      "working on 123 participant\n",
      "working on 127 participant\n",
      "working on 129 participant\n",
      "working on 130 participant\n",
      "working on 131 participant\n",
      "working on 132 participant\n",
      "working on 128 participant\n",
      "working on 133 participant\n",
      "working on 136 participant\n",
      "working on 137 participant\n",
      "working on 138 participant\n",
      "working on 159 participant\n",
      "working on 142 participant\n",
      "working on 143 participant\n",
      "working on 109 participant\n",
      "working on 144 participant\n",
      "working on 145 participant\n",
      "working on 146 participant\n",
      "working on 147 participant\n",
      "working on 149 participant\n",
      "working on 152 participant\n",
      "working on 150 participant\n",
      "working on 151 participant\n",
      "working on 153 participant\n",
      "working on 139 participant\n",
      "working on 134 participant\n",
      "working on 141 participant\n",
      "working on 155 participant\n",
      "working on 140 participant\n",
      "working on 154 participant\n",
      "working on 156 participant\n",
      "working on 157 participant\n",
      "working on Running 5 METs level\n",
      "working on 108 participant\n",
      "working on 111 participant\n",
      "working on 112 participant\n",
      "working on 113 participant\n",
      "working on 114 participant\n",
      "working on 115 participant\n",
      "working on 116 participant\n",
      "working on 117 participant\n",
      "working on 118 participant\n",
      "working on 119 participant\n",
      "working on 120 participant\n",
      "working on 121 participant\n",
      "working on 122 participant\n",
      "working on 124 participant\n",
      "working on 125 participant\n",
      "working on 126 participant\n",
      "working on 123 participant\n",
      "working on 127 participant\n",
      "working on 129 participant\n",
      "working on 130 participant\n",
      "working on 131 participant\n",
      "working on 132 participant\n",
      "working on 128 participant\n",
      "working on 133 participant\n",
      "working on 136 participant\n",
      "working on 137 participant\n",
      "working on 138 participant\n",
      "working on 159 participant\n",
      "working on 142 participant\n",
      "working on 143 participant\n",
      "working on 109 participant\n",
      "working on 144 participant\n",
      "working on 145 participant\n",
      "working on 146 participant\n",
      "working on 147 participant\n",
      "working on 149 participant\n",
      "working on 152 participant\n",
      "working on 150 participant\n",
      "working on 151 participant\n",
      "working on 153 participant\n",
      "working on 139 participant\n",
      "working on 134 participant\n",
      "working on 141 participant\n",
      "working on 155 participant\n",
      "working on 140 participant\n",
      "working on 154 participant\n",
      "working on 156 participant\n",
      "working on 157 participant\n",
      "working on Running 7 METs level\n",
      "working on 108 participant\n",
      "working on 111 participant\n",
      "working on 112 participant\n",
      "working on 113 participant\n",
      "working on 114 participant\n",
      "working on 115 participant\n",
      "working on 116 participant\n",
      "working on 117 participant\n",
      "working on 118 participant\n",
      "working on 119 participant\n",
      "working on 120 participant\n",
      "working on 121 participant\n",
      "working on 122 participant\n",
      "working on 124 participant\n",
      "working on 125 participant\n",
      "working on 126 participant\n",
      "working on 123 participant\n",
      "working on 127 participant\n",
      "working on 129 participant\n",
      "working on 130 participant\n",
      "working on 131 participant\n",
      "working on 132 participant\n",
      "working on 128 participant\n",
      "working on 133 participant\n",
      "working on 136 participant\n",
      "working on 137 participant\n",
      "working on 138 participant\n",
      "working on 159 participant\n",
      "working on 142 participant\n",
      "working on 143 participant\n",
      "working on 109 participant\n",
      "working on 144 participant\n",
      "working on 145 participant\n",
      "working on 146 participant\n",
      "working on 147 participant\n",
      "working on 149 participant\n",
      "working on 152 participant\n",
      "working on 150 participant\n",
      "working on 151 participant\n",
      "working on 153 participant\n",
      "working on 139 participant\n",
      "working on 134 participant\n",
      "working on 141 participant\n",
      "working on 155 participant\n",
      "working on 140 participant\n",
      "working on 154 participant\n",
      "working on 156 participant\n",
      "working on 157 participant\n"
     ]
    }
   ],
   "source": [
    "# repeat for all PE levels \n",
    "\n",
    "# get levels to loop thru\n",
    "PE_levels = df.trimmed_activity.unique()\n",
    "\n",
    "# ceate empty df\n",
    "df_clean = pd.DataFrame(columns = important_columns)\n",
    "\n",
    "for level in PE_levels:\n",
    "    print(\"working on {} level\".format(level))\n",
    "    df_level = df[df['trimmed_activity'] == level]\n",
    "    df_level_clean = pd.DataFrame(columns = important_columns)\n",
    "\n",
    "\n",
    "    for partip in participant_list:\n",
    "        df_temp = df_level[df_level['participant_id'] == partip]\n",
    "        df_temp_nrow = df_temp.shape[0]\n",
    "        # ignore the noisy data in the beginning and the end\n",
    "        df_temp = df_temp.iloc[n_ignore:df_temp_nrow-n_ignore,]\n",
    "        \n",
    "        # make it devisable by sequence length\n",
    "        number_of_sequences = df_temp.shape[0] // lenght_of_each_seq\n",
    "        n_row= number_of_sequences * lenght_of_each_seq\n",
    "        df_temp = df_temp.iloc[:n_row,]\n",
    "    \n",
    "        df_level_clean = pd.concat([df_level_clean, df_temp])\n",
    "        print(\"working on {} participant\".format(partip))\n",
    "\n",
    "\n",
    "    df_clean = pd.concat([df_clean, df_level_clean])\n",
    "\n",
    "  \n",
    "# df_clean = df_clean.drop('participant_id', axis = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to create something like an 1-D image. so we can feed it to CNN.\n",
    "For image processing, an image has three channels, and two dimenssion. So for a 264*264 pixel image,the shape is:\n",
    "264, 264, 3\n",
    "\n",
    "In our case, if we use a window of 3 second we have 90 reading(30 Hz), similar to pixel number in images. And we have 3 dimenssion,z,y, andz so the input shape is (90,3)\n",
    "\n",
    "Now if we have n input (n sequesnces or n images), the inout shape is (n,90,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator\n",
    "\n",
    "### Create sequence of acceleration data\n",
    "\n",
    "For each axis we do:\n",
    "\n",
    "\n",
    "Get the axis and put in a numpy array\n",
    "\n",
    "reshape it to (n,1) where n is the total length of acceleration data for a specific activity and person\n",
    "\n",
    "stack all the axis horizontally. e.i. bind columns\n",
    "\n",
    "reshape to (number_of_sequences, lenght_of_each_seq, number_of_axis i.e. 3) \n",
    "\n",
    "Note that n =number_of_sequences * lenght_of_each_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create meta data and labels\n",
    "\n",
    "\n",
    "How to create meta data:\n",
    "- follow the pre cell, but don't need to stack anything. We will process age, gender, labesl, etc separetely.\n",
    "- the dim is **number_of_sequences, lenght_of_each_seq,**\n",
    "- use numpy max and get the max (or min) and reduce the matrix to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_axis</th>\n",
       "      <th>y_axis</th>\n",
       "      <th>z_axis</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>trimmed_activity</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>0.270312</td>\n",
       "      <td>-0.066408</td>\n",
       "      <td>0.961516</td>\n",
       "      <td>108</td>\n",
       "      <td>Lying</td>\n",
       "      <td>164.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>0.269653</td>\n",
       "      <td>-0.066482</td>\n",
       "      <td>0.959821</td>\n",
       "      <td>108</td>\n",
       "      <td>Lying</td>\n",
       "      <td>164.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>0.263954</td>\n",
       "      <td>-0.065834</td>\n",
       "      <td>0.942104</td>\n",
       "      <td>108</td>\n",
       "      <td>Lying</td>\n",
       "      <td>164.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>0.262995</td>\n",
       "      <td>-0.066327</td>\n",
       "      <td>0.941666</td>\n",
       "      <td>108</td>\n",
       "      <td>Lying</td>\n",
       "      <td>164.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>0.264132</td>\n",
       "      <td>-0.066887</td>\n",
       "      <td>0.947164</td>\n",
       "      <td>108</td>\n",
       "      <td>Lying</td>\n",
       "      <td>164.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       x_axis    y_axis    z_axis participant_id trimmed_activity  height  \\\n",
       "600  0.270312 -0.066408  0.961516            108            Lying   164.0   \n",
       "601  0.269653 -0.066482  0.959821            108            Lying   164.0   \n",
       "602  0.263954 -0.065834  0.942104            108            Lying   164.0   \n",
       "603  0.262995 -0.066327  0.941666            108            Lying   164.0   \n",
       "604  0.264132 -0.066887  0.947164            108            Lying   164.0   \n",
       "\n",
       "     weight age gender  \n",
       "600    68.0  30      0  \n",
       "601    68.0  30      0  \n",
       "602    68.0  30      0  \n",
       "603    68.0  30      0  \n",
       "604    68.0  30      0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acceleration sequense data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will have  70392  sequences to available.\n",
      "(6335280, 1)\n",
      "(6335280, 2)\n",
      "(6335280, 3)\n",
      "(70392, 90, 3)\n"
     ]
    }
   ],
   "source": [
    "# sequence generator\n",
    "# output size (number_of_sequences, lenght_of_each_seq, number_of_axis i.e. 3)\n",
    "\n",
    "n_row = df_clean.shape[0]\n",
    "number_of_sequences = int(n_row / lenght_of_each_seq)\n",
    "print(\"We will have \", number_of_sequences ,\" sequences to available.\")\n",
    "accel_array = np.empty((n_row,0))\n",
    "\n",
    "\n",
    "# repeat for all axes\n",
    "axes_list = ['x_axis','y_axis','z_axis']\n",
    "for axis in axes_list:\n",
    "\n",
    "    # filter based on axis\n",
    "    working_array = df_clean[axis]\n",
    "\n",
    "    working_array = np.array(working_array).reshape(n_row,1)\n",
    "    accel_array = np.hstack((accel_array, working_array))\n",
    "    \n",
    "    print(accel_array.shape)\n",
    "n_axis = len(axes_list)\n",
    "accel_array = accel_array.reshape((number_of_sequences, lenght_of_each_seq, 3)) \n",
    "print(accel_array.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meta data generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will have  70392  sequences to available.\n",
      "(70392, 1)      (70392, 90, 1)\n",
      "(70392, 1)      (70392, 90, 1)\n",
      "(70392, 1)      (70392, 90, 1)\n",
      "(70392, 1)      (70392, 90, 1)\n",
      "(70392, 4)\n"
     ]
    }
   ],
   "source": [
    "# has the same logic as accelereation sequence generator\n",
    "# for each column output size (number_of_sequences,  1)\n",
    "# for all of them, the out put in meta_array  size ((number_of_sequences, len(mata_column_list)))\n",
    "\n",
    "\n",
    "\n",
    "n_row = df_clean.shape[0]\n",
    "number_of_sequences = int(n_row / lenght_of_each_seq)\n",
    "print(\"We will have \", number_of_sequences ,\" sequences to available.\")\n",
    "# repeat for all meta data columns \n",
    "meta_column_list = ['height','weight','age','gender']\n",
    "\n",
    "\n",
    "compressed_array = np.empty((number_of_sequences,1))\n",
    "meta_array = np.empty((number_of_sequences, 0))\n",
    "\n",
    "for meta in meta_column_list:\n",
    "\n",
    "    # filter based on meta data column\n",
    "    working_array = df_clean[meta]\n",
    "\n",
    "    working_array = np.array(working_array).reshape(number_of_sequences, lenght_of_each_seq, 1)\n",
    "    for i in range(number_of_sequences):\n",
    "        compressed_array[i] = working_array[i,].max()\n",
    "    \n",
    "    meta_array = np.hstack((meta_array, compressed_array))\n",
    "    print(compressed_array.shape, \"    \" , working_array.shape)\n",
    "print(meta_array.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will have  70392  sequences to available.\n",
      "(70392, 90, 1)\n",
      "(70392, 1)\n",
      "[['Lying']\n",
      " ['Lying']\n",
      " ['Lying']\n",
      " ...\n",
      " ['Running 7 METs']\n",
      " ['Running 7 METs']\n",
      " ['Running 7 METs']]\n"
     ]
    }
   ],
   "source": [
    "# repeat for  trimmed activity which is the labels\n",
    "# has the same logic as accelereation sequence generator\n",
    "# for labels output size (number_of_sequences,  1)\n",
    "\n",
    "n_row = df_clean.shape[0]\n",
    "number_of_sequences = int(n_row / lenght_of_each_seq)\n",
    "print(\"We will have \", number_of_sequences ,\" sequences to available.\")\n",
    "label_column_list = ['trimmed_activity']\n",
    "\n",
    "\n",
    "label_array = np.empty((number_of_sequences,1), dtype=list)\n",
    "\n",
    "# as we only have one outcome ( label) we don;t need another array to store all of them\n",
    "# We could do it without the for loop as well\n",
    "for label in label_column_list:\n",
    "    # filter based on the column\n",
    "    working_array = df_clean[label]\n",
    "    working_array = np.array(working_array).reshape(number_of_sequences, lenght_of_each_seq, 1)\n",
    "    print(working_array.shape)\n",
    "    for i in range(number_of_sequences):\n",
    "        label_array[i] = working_array[i,0] \n",
    "print(label_array.shape)\n",
    "print(label_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check to see if the data is intact\n",
    "\n",
    "We ignored n_ignore data from the beginning of each activity. Therefore the first processed data in n_ignore th +1 data in the raw data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we start for  600  when indexing from the raw dataframe\n",
      "[ 0.26965277 -0.06648238  0.95982104]\n",
      "[164.  68.  30.   0.]\n",
      "['Lying']\n",
      "       x_axis    y_axis    z_axis  participant_id trimmed_activity  height  \\\n",
      "600  0.270312 -0.066408  0.961516             108            Lying   164.0   \n",
      "601  0.269653 -0.066482  0.959821             108            Lying   164.0   \n",
      "602  0.263954 -0.065834  0.942104             108            Lying   164.0   \n",
      "\n",
      "     weight  age gender  \n",
      "600    68.0   30      0  \n",
      "601    68.0   30      0  \n",
      "602    68.0   30      0  \n"
     ]
    }
   ],
   "source": [
    "print(\"we start for \",n_ignore,\" when indexing from the raw dataframe\")\n",
    "print(accel_array[0,1])\n",
    "print(meta_array[0])\n",
    "print(label_array[0])\n",
    "print(df.iloc[n_ignore:n_ignore+3,])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the results as numpy objects\n",
    "location = 'pocket'\n",
    "output_file_name = input_dir +location + '-deep-data'\n",
    "np.savez_compressed(output_file_name,\n",
    "                    acceleration_data=accel_array,\n",
    "                    metadata=meta_array,\n",
    "                    labels=label_array\n",
    "                   )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
