{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook prepares the data for NN models\n",
    "In this Notebook we \n",
    "- Remove some of the reading at the beginning and the end of each activity. This eliminates the noise presented when the participant is changing activity.\n",
    "\n",
    "- Select the acceleration columns, and reshape them.\n",
    "\n",
    "- Select the participants' demographic data such as age, weight, etc. Then reshape them to be aligned with acceleration data.\n",
    "\n",
    "- Extract the labels with the same method.\n",
    "\n",
    "**Note** that we used the NumPy module, and the labels are not one-hot encoded, and the data is not shuffled.\n",
    "\n",
    "You can change the parameters to manipulate this process.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data frame dictionary:\n",
    "- *df* = the raw csv dataframe\n",
    "- *df_clean* = final cleaned and processed dataframe\n",
    "- *df_level* = df for each activity level\n",
    "- *df_level_clean* = clean version of df_level\n",
    "- *df_temp* = a helper dtaframe to store temporary data for each participant and each level\n",
    "\n",
    "#### numPy array dictionary\n",
    "- accel_array, contains x,y, and z acceleration and a shape of (number_of_sequences, lenght_of_each_seq, number_of_axis i.e. 3)\n",
    "- meta_array, has the demographic data and its size is (number_of_sequences, len(mata_column_list) \n",
    "- label_array, contains the labels for each sequence with a size of number_of_sequences, 1)\n",
    "\n",
    "#### variable dictionary:\n",
    "- n_ignore: number of reading to ignore from the beginning and the end of each activity. If set to 600, ignores 20 seconds as the frequency is 30Hz\n",
    "- window_size_second: length of the window used for sequencing in seconds. Each window_size_second is a sequence \n",
    "- frequency: Of the accelerometer\n",
    "- lenght_of_each_seq: window_size_second * frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "n_ignore = 600 # ignores 20 sec with a frequency of 30 Hz\n",
    "window_size_second = 3\n",
    "frequency = 30\n",
    "lenght_of_each_seq = window_size_second * frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = 'pocket'\n",
    "input_dir =  'Z:/Research/dfuller/Walkabilly/studies/smarphone_accel/data/Ethica_Jaeger_Merged/'\n",
    "input_file_name = 'pocket_with_couns_and_vec_meg_30Hz.csv'\n",
    "file_path = os.path.join(input_dir, location, input_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the noise \n",
    "\n",
    "Igonre n_ignore item from the beginning and the end of each activity for each person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_list = list(df.participant_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 112 , 121 .122 , and 132 as they are not properly classified.\n",
    "misclass_participants = [112,121,122,132]\n",
    "# participant_list.remove()\n",
    "participant_list = [elem for elem in participant_list if elem not in misclass_participants]\n",
    "participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select important columns, x, y, z, height, weight, age, gender, also participant_id for cleaning. remove it later\n",
    "important_columns = ['x_axis','y_axis','z_axis','participant_id','trimmed_activity','height','weight','age','gender']\n",
    "df = df[important_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change gender to dummy\n",
    "df.gender[df['gender']=='Female'] = 0\n",
    "df.gender[df['gender']=='Male'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat for all PE levels \n",
    "\n",
    "# get levels to loop thru\n",
    "PE_levels = df.trimmed_activity.unique()\n",
    "\n",
    "# ceate empty df\n",
    "df_clean = pd.DataFrame(columns = important_columns)\n",
    "\n",
    "for level in PE_levels:\n",
    "    print(\"working on {} level\".format(level))\n",
    "    df_level = df[df['trimmed_activity'] == level]\n",
    "    df_level_clean = pd.DataFrame(columns = important_columns)\n",
    "\n",
    "\n",
    "    for partip in participant_list:\n",
    "        df_temp = df_level[df_level['participant_id'] == partip]\n",
    "        df_temp_nrow = df_temp.shape[0]\n",
    "        # ignore the noisy data in the beginning and the end\n",
    "        df_temp = df_temp.iloc[n_ignore:df_temp_nrow-n_ignore,]\n",
    "        \n",
    "        # make it devisable by sequence length\n",
    "        number_of_sequences = df_temp.shape[0] // lenght_of_each_seq\n",
    "        n_row= number_of_sequences * lenght_of_each_seq\n",
    "        df_temp = df_temp.iloc[:n_row,]\n",
    "    \n",
    "        df_level_clean = pd.concat([df_level_clean, df_temp])\n",
    "        print(\"working on {} participant\".format(partip))\n",
    "\n",
    "\n",
    "    df_clean = pd.concat([df_clean, df_level_clean])\n",
    "\n",
    "  \n",
    "# df_clean = df_clean.drop('participant_id', axis = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to create something like an 1-D image. so we can feed it to CNN.\n",
    "For image processing, an image has three channels, and two dimenssion. So for a 264*264 pixel image,the shape is:\n",
    "264, 264, 3\n",
    "\n",
    "In our case, if we use a window of 3 second we have 90 reading(30 Hz), similar to pixel number in images. And we have 3 dimenssion,z,y, andz so the input shape is (90,3)\n",
    "\n",
    "Now if we have n input (n sequesnces or n images), the inout shape is (n,90,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator\n",
    "\n",
    "### Create sequence of acceleration data\n",
    "\n",
    "For each axis we do:\n",
    "\n",
    "\n",
    "Get the axis and put in a numpy array\n",
    "\n",
    "reshape it to (n,1) where n is the total length of acceleration data for a specific activity and person\n",
    "\n",
    "stack all the axis horizontally. e.i. bind columns\n",
    "\n",
    "reshape to (number_of_sequences, lenght_of_each_seq, number_of_axis i.e. 3) \n",
    "\n",
    "Note that n =number_of_sequences * lenght_of_each_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create meta data and labels\n",
    "\n",
    "\n",
    "How to create meta data:\n",
    "- follow the pre cell, but don't need to stack anything. We will process age, gender, labesl, etc separetely.\n",
    "- the dim is **number_of_sequences, lenght_of_each_seq,**\n",
    "- use numpy max and get the max (or min) and reduce the matrix to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acceleration sequense data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence generator\n",
    "# output size (number_of_sequences, lenght_of_each_seq, number_of_axis i.e. 3)\n",
    "\n",
    "n_row = df_clean.shape[0]\n",
    "number_of_sequences = int(n_row / lenght_of_each_seq)\n",
    "print(\"We will have \", number_of_sequences ,\" sequences to available.\")\n",
    "accel_array = np.empty((n_row,0))\n",
    "\n",
    "\n",
    "# repeat for all axes\n",
    "axes_list = ['x_axis','y_axis','z_axis']\n",
    "for axis in axes_list:\n",
    "\n",
    "    # filter based on axis\n",
    "    working_array = df_clean[axis]\n",
    "\n",
    "    working_array = np.array(working_array).reshape(n_row,1)\n",
    "    accel_array = np.hstack((accel_array, working_array))\n",
    "    \n",
    "    print(accel_array.shape)\n",
    "n_axis = len(axes_list)\n",
    "accel_array = accel_array.reshape((number_of_sequences, lenght_of_each_seq, 3)) \n",
    "print(accel_array.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meta data generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# has the same logic as accelereation sequence generator\n",
    "# for each column output size (number_of_sequences,  1)\n",
    "# for all of them, the out put in meta_array  size ((number_of_sequences, len(mata_column_list)))\n",
    "\n",
    "\n",
    "\n",
    "n_row = df_clean.shape[0]\n",
    "number_of_sequences = int(n_row / lenght_of_each_seq)\n",
    "print(\"We will have \", number_of_sequences ,\" sequences to available.\")\n",
    "# repeat for all meta data columns \n",
    "meta_column_list = ['height','weight','age','gender']\n",
    "\n",
    "\n",
    "compressed_array = np.empty((number_of_sequences,1))\n",
    "meta_array = np.empty((number_of_sequences, 0))\n",
    "\n",
    "for meta in meta_column_list:\n",
    "\n",
    "    # filter based on meta data column\n",
    "    working_array = df_clean[meta]\n",
    "\n",
    "    working_array = np.array(working_array).reshape(number_of_sequences, lenght_of_each_seq, 1)\n",
    "    for i in range(number_of_sequences):\n",
    "        compressed_array[i] = working_array[i,].max()\n",
    "    \n",
    "    meta_array = np.hstack((meta_array, compressed_array))\n",
    "    print(compressed_array.shape, \"    \" , working_array.shape)\n",
    "print(meta_array.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat for  trimmed activity which is the labels\n",
    "# has the same logic as accelereation sequence generator\n",
    "# for labels output size (number_of_sequences,  1)\n",
    "\n",
    "n_row = df_clean.shape[0]\n",
    "number_of_sequences = int(n_row / lenght_of_each_seq)\n",
    "print(\"We will have \", number_of_sequences ,\" sequences to available.\")\n",
    "label_column_list = ['trimmed_activity']\n",
    "\n",
    "\n",
    "label_array = np.empty((number_of_sequences,1), dtype=list)\n",
    "\n",
    "# as we only have one outcome ( label) we don;t need another array to store all of them\n",
    "# We could do it without the for loop as well\n",
    "for label in label_column_list:\n",
    "    # filter based on the column\n",
    "    working_array = df_clean[label]\n",
    "    working_array = np.array(working_array).reshape(number_of_sequences, lenght_of_each_seq, 1)\n",
    "    print(working_array.shape)\n",
    "    for i in range(number_of_sequences):\n",
    "        label_array[i] = working_array[i,0] \n",
    "print(label_array.shape)\n",
    "print(label_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check to see if the data is intact\n",
    "\n",
    "We ignored n_ignore data from the beginning of each activity. Therefore the first processed data in n_ignore th +1 data in the raw data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"we start for \",n_ignore,\" when indexing from the raw dataframe\")\n",
    "print(accel_array[0,1])\n",
    "print(meta_array[0])\n",
    "print(label_array[0])\n",
    "print(df.iloc[n_ignore:n_ignore+3,])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the results as numpy objects\n",
    "output_file_name = input_dir +location + \"/\" + location + '-NN-data'\n",
    "np.savez_compressed(output_file_name,\n",
    "                    acceleration_data=accel_array,\n",
    "                    metadata=meta_array,\n",
    "                    labels=label_array\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
