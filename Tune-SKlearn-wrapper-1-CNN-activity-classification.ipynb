{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/khataei/PE-classification-DeepLearning/blob/master/Tunned-Talos-1-CNN-activity-classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ahUZuAfz8Jje"
   },
   "source": [
    "# Talos Tuner for CNN Activity Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Re0ecmVI8Jjk"
   },
   "source": [
    "In this notebook, we use SKlearn  to tune a CNN neural net to classify PE activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ixsGb9tY8Jjm"
   },
   "source": [
    "#### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5At1PKQp8Jjp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4946466680677553101\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 10798057925511371666\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4022219571\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 5227768459458449771\n",
      "physical_device_desc: \"device: 0, name: Quadro P2000, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 9111570899242302647\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n",
      "# GPUs Available:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import AveragePooling1D, LeakyReLU , MaxPool1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint \n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "import tensorflow as tf\n",
    "print(\"# GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hg5wX2Dd8Jjz"
   },
   "source": [
    "#### Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lYLOM99-8Jj2"
   },
   "outputs": [],
   "source": [
    "# output directory name:\n",
    "# output_dir = 'model_output/tunecnn1'\n",
    "input_dir =  'Z:/Research/dfuller/Walkabilly/studies/smarphone_accel/data/Ethica_Jaeger_Merged/pocket/'\n",
    "input_file_name = 'pocket-NN-data.npz'\n",
    "\n",
    "# from the data preparation section we have:\n",
    "window_size_second = 3\n",
    "frequency = 30\n",
    "lenght_of_each_seq = window_size_second * frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn hyperparams\n",
    "params = {\n",
    "    'n_conv_1':[512, 768], # filters, a.k.a. kernels\n",
    "    'k_conv_1':[3, 7], # kernel length\n",
    "    'n_conv_2':[512, 768], # filters, a.k.a. kernels\n",
    "    'k_conv_2':[3, 7], # kernel length\n",
    "    'n_conv_3':[512, 768], # filters, a.k.a. kernels\n",
    "    'k_conv_3':[3, 7], # kernel length\n",
    "    'maxpooling_pool_size':[2, 4],\n",
    "    'avepooling_pool_size':[2, 4],\n",
    "    'n_dense_1':[512, 768],\n",
    "    'dropout_1':[0.22, 0.3],\n",
    "    'n_dense_2':[512],\n",
    "    'dropout_2':[0.25],\n",
    "    'activation':['elu', 'relu', LeakyReLU()]\n",
    "}\n",
    "\n",
    "# training:\n",
    "n_tune_iter = 20\n",
    "cv = 3\n",
    "epochs = 60\n",
    "batch_size = 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DaLx4yZ48Jj9"
   },
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CD9X0yJg8Jj_"
   },
   "source": [
    "##### For this notebook we use the acceleration data gathered from the pocket location. It was prepared in the DataPrep-Deep notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6r44JKj-8JkA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acceleration_data\n",
      "metadata\n",
      "labels\n"
     ]
    }
   ],
   "source": [
    "# read the raw file and get the keys:\n",
    "raw_data = np.load(file=input_dir+input_file_name,allow_pickle=True)\n",
    "for k in raw_data.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "\n",
    "accel_array = raw_data['acceleration_data']\n",
    "meta_array = raw_data['metadata']\n",
    "labels_array = raw_data['labels']\n",
    "input_shape = list(accel_array.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WWq3ipbu8JlG"
   },
   "source": [
    "#### Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the  labels to integer.\n",
    "In the raw data format of the labels is String and there are 6 classes. 'Lying', 'Sitting', 'Self Pace walk', 'Running 3 METs',\n",
    "       'Running 5 METs', 'Running 7 METs' <br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LyReiequ8Jln"
   },
   "outputs": [],
   "source": [
    "n_class = len(np.unique(labels_array))\n",
    "class_list, labels_array_int = np.unique(labels_array,return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels_array_int, num_classes=n_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "40x3s0KY8Jlv",
    "outputId": "d3ac3e31-3c47-4b01-c2f9-ada0684b31b7"
   },
   "source": [
    "### Splitting and shuffeling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f5grc2H78Jlz",
    "outputId": "3f7eeb55-f851-4796-de4e-0c68eb9df88d"
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "     accel_array, y, test_size=0.1, random_state=65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "GscSnCpk8Jlm"
   },
   "source": [
    "\n",
    "#### Design neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_conv_1': [512, 768],\n",
       " 'k_conv_1': [3, 7],\n",
       " 'n_conv_2': [512, 768],\n",
       " 'k_conv_2': [3, 7],\n",
       " 'n_conv_3': [512, 768],\n",
       " 'k_conv_3': [3, 7],\n",
       " 'maxpooling_pool_size': [2, 4],\n",
       " 'avepooling_pool_size': [2, 4],\n",
       " 'n_dense_1': [512, 768],\n",
       " 'dropout_1': [0.22, 0.3],\n",
       " 'n_dense_2': [512],\n",
       " 'dropout_2': [0.25],\n",
       " 'activation': ['elu',\n",
       "  'relu',\n",
       "  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU at 0x16b3bc11888>]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_conv_1=256, k_conv_1=3, n_conv_2=256, k_conv_2=3, n_conv_3=256, k_conv_3=3,\n",
    "                 maxpooling_pool_size = 2, avepooling_pool_size = 2, n_dense_1=256, dropout_1=0.2,\n",
    "                 n_dense_2=256, dropout_2=0.2, activation= 'elu'\n",
    "                ):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(n_conv_1, k_conv_1, activation=activation, input_shape=input_shape[1:]))\n",
    "    model.add(MaxPool1D(pool_size = maxpooling_pool_size))\n",
    "    model.add(Conv1D(n_conv_2, k_conv_2, activation=activation))\n",
    "    model.add(AveragePooling1D(pool_size = avepooling_pool_size))\n",
    "    model.add(Conv1D(n_conv_3, k_conv_3, activation=activation))\n",
    "    # model.add(GlobalMaxPooling1D())\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(n_dense_1, activation=activation))\n",
    "    model.add(Dropout(dropout_1))\n",
    "    model.add(Dense(n_dense_2, activation=activation))\n",
    "    model.add(Dropout(dropout_2))\n",
    "    model.add(Dense(n_class, activation='softmax'))\n",
    "#     model.summary()\n",
    "    print(f\"\"\"n_conv_1 = {n_conv_1}, k_conv_1 = {k_conv_1},  n_conv_2 = {n_conv_2}, k_conv_2 = {k_conv_2}, \n",
    "          n_conv_3 = {n_conv_3},  k_conv_3 = {k_conv_3},  maxpooling_pool_size = {maxpooling_pool_size},\n",
    "          avepooling_pool_size = {avepooling_pool_size},  n_dense_1 = {n_dense_1}, dropout_1 = {dropout_1},\n",
    "          n_dense_2 = {n_dense_2}, dropout_2 = {dropout_2}, activation=  {activation}\"\"\")\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_conv_1 = 256, k_conv_1 = 3,  n_conv_2 = 256, k_conv_2 = 3, \n",
      "          n_conv_3 = 256,  k_conv_3 = 3,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 256, dropout_1 = 0.2,\n",
      "          n_dense_2 = 256, dropout_2 = 0.2, activation=  elu\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 88, 256)           2560      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 44, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 42, 256)           196864    \n",
      "_________________________________________________________________\n",
      "average_pooling1d (AveragePo (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 19, 256)           196864    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 529,414\n",
      "Trainable params: 529,414\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_default = create_model()\n",
    "model_default.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_conv_1 = 512, k_conv_1 = 7,  n_conv_2 = 512, k_conv_2 = 3, \n",
      "          n_conv_3 = 768,  k_conv_3 = 7,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 768, dropout_1 = 0.3,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  elu\n",
      "Epoch 1/60\n",
      "  2/304 [..............................] - ETA: 7s - loss: 2.1398 - accuracy: 0.2969WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0160s vs `on_train_batch_end` time: 0.0339s). Check your callbacks.\n",
      "304/304 [==============================] - 14s 46ms/step - loss: 1.3752 - accuracy: 0.4230\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 1.9304 - accuracy: 0.4914\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 1.0700 - accuracy: 0.5395\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.9075 - accuracy: 0.6143\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.7626 - accuracy: 0.6847\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.6613 - accuracy: 0.7359\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.5799 - accuracy: 0.7689\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.5471 - accuracy: 0.7814\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.5152 - accuracy: 0.7946\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.4847 - accuracy: 0.8099\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.5002 - accuracy: 0.8043\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.4495 - accuracy: 0.8193\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.4346 - accuracy: 0.8280\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.4253 - accuracy: 0.8321\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.4129 - accuracy: 0.8380\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.4120 - accuracy: 0.8385\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.3997 - accuracy: 0.8426\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.3852 - accuracy: 0.8491\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.3778 - accuracy: 0.8536\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.3677 - accuracy: 0.8560\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.3663 - accuracy: 0.8543\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.3545 - accuracy: 0.8629\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3475 - accuracy: 0.8656\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3449 - accuracy: 0.8648\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3424 - accuracy: 0.8664\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3356 - accuracy: 0.8681\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3264 - accuracy: 0.8723\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3299 - accuracy: 0.8706\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3256 - accuracy: 0.8728\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3060 - accuracy: 0.8788\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3137 - accuracy: 0.8763\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3010 - accuracy: 0.8791\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3054 - accuracy: 0.8791\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3006 - accuracy: 0.8804\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2902 - accuracy: 0.8841\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2851 - accuracy: 0.8871\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2874 - accuracy: 0.8839\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2947 - accuracy: 0.8850\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2838 - accuracy: 0.8864\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2758 - accuracy: 0.8902\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2702 - accuracy: 0.8924\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2658 - accuracy: 0.8934\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2663 - accuracy: 0.8936\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2690 - accuracy: 0.8947\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2628 - accuracy: 0.8943\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2556 - accuracy: 0.8977\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2581 - accuracy: 0.8959\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2541 - accuracy: 0.8990\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2505 - accuracy: 0.9007\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2513 - accuracy: 0.9008\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2498 - accuracy: 0.9001\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2426 - accuracy: 0.9028\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2432 - accuracy: 0.9028\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2475 - accuracy: 0.9022\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2425 - accuracy: 0.9024\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2425 - accuracy: 0.9058\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2353 - accuracy: 0.9048\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2364 - accuracy: 0.9048\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2384 - accuracy: 0.9083\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2529 - accuracy: 0.9022\n",
      "152/152 [==============================] - 2s 14ms/step - loss: 0.3461 - accuracy: 0.8912\n",
      "n_conv_1 = 512, k_conv_1 = 7,  n_conv_2 = 512, k_conv_2 = 3, \n",
      "          n_conv_3 = 768,  k_conv_3 = 7,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 768, dropout_1 = 0.3,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  elu\n",
      "Epoch 1/60\n",
      "  1/304 [..............................] - ETA: 8s - loss: 1.7954 - accuracy: 0.1719WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0150s vs `on_train_batch_end` time: 0.0279s). Check your callbacks.\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 1.8120 - accuracy: 0.4232\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 1.3191 - accuracy: 0.4928\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.9073 - accuracy: 0.6224\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.7627 - accuracy: 0.6904\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.6626 - accuracy: 0.7384\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.6053 - accuracy: 0.7612\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.5662 - accuracy: 0.7802\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.5309 - accuracy: 0.7931\n",
      "Epoch 9/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 13s 43ms/step - loss: 0.5084 - accuracy: 0.8006\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.4835 - accuracy: 0.8125\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.4701 - accuracy: 0.8178\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.4605 - accuracy: 0.8206\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.4505 - accuracy: 0.8237\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.4309 - accuracy: 0.8308\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.4158 - accuracy: 0.8384\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.4125 - accuracy: 0.8396\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3924 - accuracy: 0.8489\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3876 - accuracy: 0.8489\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3755 - accuracy: 0.8525\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3688 - accuracy: 0.8575\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3630 - accuracy: 0.8612\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3507 - accuracy: 0.8622\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.3535 - accuracy: 0.8636\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3328 - accuracy: 0.8710\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3355 - accuracy: 0.8675\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3276 - accuracy: 0.8730\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3280 - accuracy: 0.8718\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3176 - accuracy: 0.8763\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3167 - accuracy: 0.8764\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3050 - accuracy: 0.8801\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3023 - accuracy: 0.8796\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3009 - accuracy: 0.8808\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2902 - accuracy: 0.8843\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2942 - accuracy: 0.8827\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2908 - accuracy: 0.8847\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2838 - accuracy: 0.8876\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2870 - accuracy: 0.8863\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2742 - accuracy: 0.8923\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2745 - accuracy: 0.8892\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2704 - accuracy: 0.8915\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2633 - accuracy: 0.8940\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2759 - accuracy: 0.8901\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2626 - accuracy: 0.8946\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2669 - accuracy: 0.8942\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2553 - accuracy: 0.8971\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2653 - accuracy: 0.8950\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2516 - accuracy: 0.8983\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2543 - accuracy: 0.8980\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2558 - accuracy: 0.8967\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2390 - accuracy: 0.9025\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2410 - accuracy: 0.9028\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2439 - accuracy: 0.9033\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2460 - accuracy: 0.9026\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2321 - accuracy: 0.9065\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2286 - accuracy: 0.9069\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2428 - accuracy: 0.9045\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2265 - accuracy: 0.9107\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2213 - accuracy: 0.9120\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2198 - accuracy: 0.9118\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2228 - accuracy: 0.9130\n",
      "152/152 [==============================] - 2s 13ms/step - loss: 0.3610 - accuracy: 0.8991\n",
      "n_conv_1 = 512, k_conv_1 = 7,  n_conv_2 = 512, k_conv_2 = 3, \n",
      "          n_conv_3 = 768,  k_conv_3 = 7,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 768, dropout_1 = 0.3,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  elu\n",
      "Epoch 1/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 1.3485 - accuracy: 0.4420\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.9829 - accuracy: 0.6025\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 1.0401 - accuracy: 0.6278\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.8891 - accuracy: 0.6438\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.6574 - accuracy: 0.7405\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.5931 - accuracy: 0.7636\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.5566 - accuracy: 0.7824\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.5099 - accuracy: 0.7961\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.4844 - accuracy: 0.8078\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.4638 - accuracy: 0.8153\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.4430 - accuracy: 0.8219\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.4238 - accuracy: 0.8318\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.4121 - accuracy: 0.8358\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.3983 - accuracy: 0.8410\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.3900 - accuracy: 0.8452\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.3771 - accuracy: 0.8508\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.3694 - accuracy: 0.8514\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.3541 - accuracy: 0.8555\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.3525 - accuracy: 0.8577\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.3350 - accuracy: 0.8636\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.3304 - accuracy: 0.8655\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.3259 - accuracy: 0.8696\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.3241 - accuracy: 0.8698\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.3149 - accuracy: 0.8736\n",
      "Epoch 25/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 13s 44ms/step - loss: 0.3043 - accuracy: 0.8766\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.2950 - accuracy: 0.8794\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.2967 - accuracy: 0.8793\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.2912 - accuracy: 0.8818\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.2925 - accuracy: 0.8802\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.2830 - accuracy: 0.8850\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.2734 - accuracy: 0.8877\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.2707 - accuracy: 0.8874\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.2806 - accuracy: 0.8859\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.2632 - accuracy: 0.8924\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2702 - accuracy: 0.8908\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.2612 - accuracy: 0.8946\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.2525 - accuracy: 0.8979\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.2472 - accuracy: 0.8982\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.2398 - accuracy: 0.9020\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.2366 - accuracy: 0.9055\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.2343 - accuracy: 0.9060\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.2257 - accuracy: 0.9078\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2324 - accuracy: 0.9061\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.2261 - accuracy: 0.9101\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2241 - accuracy: 0.9106\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2389 - accuracy: 0.9085\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2153 - accuracy: 0.9130\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2171 - accuracy: 0.9121\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2100 - accuracy: 0.9150\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.2085 - accuracy: 0.9156\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2128 - accuracy: 0.9152\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.2107 - accuracy: 0.9165\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.2068 - accuracy: 0.9193\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.1991 - accuracy: 0.9195\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2104 - accuracy: 0.9173\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2093 - accuracy: 0.9190\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2148 - accuracy: 0.9178\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.1977 - accuracy: 0.9226\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2014 - accuracy: 0.9210\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.1894 - accuracy: 0.9252\n",
      "152/152 [==============================] - 2s 13ms/step - loss: 0.4161 - accuracy: 0.9085\n",
      "n_conv_1 = 768, k_conv_1 = 3,  n_conv_2 = 768, k_conv_2 = 7, \n",
      "          n_conv_3 = 768,  k_conv_3 = 7,  maxpooling_pool_size = 4,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 512, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  elu\n",
      "Epoch 1/60\n",
      "  2/304 [..............................] - ETA: 9s - loss: 2.2503 - accuracy: 0.2695WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0170s vs `on_train_batch_end` time: 0.0444s). Check your callbacks.\n",
      "304/304 [==============================] - 19s 61ms/step - loss: 1.3395 - accuracy: 0.4609\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.9915 - accuracy: 0.6007\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.8324 - accuracy: 0.6718\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.7109 - accuracy: 0.7188\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.6385 - accuracy: 0.7477\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.6059 - accuracy: 0.7595\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.5426 - accuracy: 0.7813\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.4883 - accuracy: 0.7998\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.4685 - accuracy: 0.8094\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.4508 - accuracy: 0.8151\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.4117 - accuracy: 0.8304\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.3917 - accuracy: 0.8382\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.3695 - accuracy: 0.8490\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.3534 - accuracy: 0.8571\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.3415 - accuracy: 0.8626\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.3240 - accuracy: 0.8702\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.3099 - accuracy: 0.8768\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2976 - accuracy: 0.8844\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.3003 - accuracy: 0.8839\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2789 - accuracy: 0.8907\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2668 - accuracy: 0.8961\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2857 - accuracy: 0.8920\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2609 - accuracy: 0.8998\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2502 - accuracy: 0.9027\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2407 - accuracy: 0.9086\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2372 - accuracy: 0.9080\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2413 - accuracy: 0.9092\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 18s 61ms/step - loss: 0.2273 - accuracy: 0.9135\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2232 - accuracy: 0.9147\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2268 - accuracy: 0.9145\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2270 - accuracy: 0.91400s - loss: 0.2260 - \n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2176 - accuracy: 0.9184\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2154 - accuracy: 0.9172\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2076 - accuracy: 0.9203\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2097 - accuracy: 0.9217\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2015 - accuracy: 0.9249\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2047 - accuracy: 0.9226\n",
      "Epoch 38/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2087 - accuracy: 0.9203\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2175 - accuracy: 0.9199\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1927 - accuracy: 0.9266\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1853 - accuracy: 0.9295\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1934 - accuracy: 0.9284\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2032 - accuracy: 0.9258\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1879 - accuracy: 0.9288\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1816 - accuracy: 0.9319\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1784 - accuracy: 0.9333\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1979 - accuracy: 0.9281\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1923 - accuracy: 0.9296\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1818 - accuracy: 0.9326\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1795 - accuracy: 0.9316\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2294 - accuracy: 0.9199\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1995 - accuracy: 0.9263\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1768 - accuracy: 0.9342\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1606 - accuracy: 0.9398\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1742 - accuracy: 0.9367\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1742 - accuracy: 0.9356\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1730 - accuracy: 0.93580s - loss: 0.1727 - accuracy\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1658 - accuracy: 0.9403\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1716 - accuracy: 0.9361\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1858 - accuracy: 0.9338\n",
      "152/152 [==============================] - 2s 15ms/step - loss: 0.5582 - accuracy: 0.9041\n",
      "n_conv_1 = 768, k_conv_1 = 3,  n_conv_2 = 768, k_conv_2 = 7, \n",
      "          n_conv_3 = 768,  k_conv_3 = 7,  maxpooling_pool_size = 4,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 512, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  elu\n",
      "Epoch 1/60\n",
      "  2/304 [..............................] - ETA: 15s - loss: 2.1488 - accuracy: 0.2812WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0175s vs `on_train_batch_end` time: 0.0429s). Check your callbacks.\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 1.3425 - accuracy: 0.4612\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 1.0001 - accuracy: 0.5951\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.8158 - accuracy: 0.67830s - loss: 0.818\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.8645 - accuracy: 0.6755\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.6739 - accuracy: 0.7357\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.5785 - accuracy: 0.7691\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.5227 - accuracy: 0.7924\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.4821 - accuracy: 0.8062\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.4460 - accuracy: 0.8214\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.4115 - accuracy: 0.8351\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.4014 - accuracy: 0.8414\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.3810 - accuracy: 0.8482\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.3533 - accuracy: 0.8597\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.3374 - accuracy: 0.8682\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.3215 - accuracy: 0.8738\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.3135 - accuracy: 0.87880s - loss: 0.3133 - accuracy: \n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2994 - accuracy: 0.8845\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2922 - accuracy: 0.8877\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2865 - accuracy: 0.8919\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2768 - accuracy: 0.8951\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2683 - accuracy: 0.8997\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2552 - accuracy: 0.9030\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2579 - accuracy: 0.9014\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2529 - accuracy: 0.9038\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2393 - accuracy: 0.9096\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2459 - accuracy: 0.9091\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2266 - accuracy: 0.9142\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2331 - accuracy: 0.9119\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2336 - accuracy: 0.9121\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2249 - accuracy: 0.9143\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2270 - accuracy: 0.9145\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2256 - accuracy: 0.9166\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2137 - accuracy: 0.9204\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2129 - accuracy: 0.9201\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2033 - accuracy: 0.9239\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2111 - accuracy: 0.9218\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2057 - accuracy: 0.9230\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2014 - accuracy: 0.9263\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2052 - accuracy: 0.9240\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2017 - accuracy: 0.9252\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2014 - accuracy: 0.9266\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2094 - accuracy: 0.9216\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1986 - accuracy: 0.9270\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1940 - accuracy: 0.9282\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1858 - accuracy: 0.9302\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1894 - accuracy: 0.9296\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1895 - accuracy: 0.9289\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1894 - accuracy: 0.9292\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1879 - accuracy: 0.9308\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1697 - accuracy: 0.9368\n",
      "Epoch 51/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1902 - accuracy: 0.9311\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2002 - accuracy: 0.9277\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1811 - accuracy: 0.9328\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1842 - accuracy: 0.9325\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1721 - accuracy: 0.9352\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1695 - accuracy: 0.9380\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1809 - accuracy: 0.9346\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1674 - accuracy: 0.9396\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1835 - accuracy: 0.9337\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1646 - accuracy: 0.9388\n",
      "152/152 [==============================] - 2s 14ms/step - loss: 0.5964 - accuracy: 0.8959\n",
      "n_conv_1 = 768, k_conv_1 = 3,  n_conv_2 = 768, k_conv_2 = 7, \n",
      "          n_conv_3 = 768,  k_conv_3 = 7,  maxpooling_pool_size = 4,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 512, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  elu\n",
      "Epoch 1/60\n",
      "  2/304 [..............................] - ETA: 15s - loss: 2.1798 - accuracy: 0.2695WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0170s vs `on_train_batch_end` time: 0.0439s). Check your callbacks.\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 1.3209 - accuracy: 0.4651\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 1.0597 - accuracy: 0.5826\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.8217 - accuracy: 0.6763\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.7225 - accuracy: 0.7137\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.6402 - accuracy: 0.7466\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.5717 - accuracy: 0.7745\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.5186 - accuracy: 0.7919\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.4711 - accuracy: 0.8097\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.4446 - accuracy: 0.8222\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.4185 - accuracy: 0.8318\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.3924 - accuracy: 0.8418\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.3649 - accuracy: 0.8548\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.3435 - accuracy: 0.8628\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.3427 - accuracy: 0.8669\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.3152 - accuracy: 0.8757\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 18s 61ms/step - loss: 0.2945 - accuracy: 0.8839\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2923 - accuracy: 0.8878\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2792 - accuracy: 0.8934\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2725 - accuracy: 0.8957\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2640 - accuracy: 0.8996\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2706 - accuracy: 0.8980\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2499 - accuracy: 0.9058\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2371 - accuracy: 0.9095\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2371 - accuracy: 0.9101\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2324 - accuracy: 0.9118\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2229 - accuracy: 0.9149\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2316 - accuracy: 0.9126\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2228 - accuracy: 0.9162\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2172 - accuracy: 0.9176\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2280 - accuracy: 0.9153\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2098 - accuracy: 0.9201\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2062 - accuracy: 0.9213\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2170 - accuracy: 0.9191\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1992 - accuracy: 0.9254\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2049 - accuracy: 0.9238\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1918 - accuracy: 0.9282\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.2025 - accuracy: 0.9248\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 18s 61ms/step - loss: 0.1957 - accuracy: 0.9266\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1907 - accuracy: 0.9280\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1850 - accuracy: 0.9304\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1834 - accuracy: 0.9323\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1909 - accuracy: 0.9300\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1921 - accuracy: 0.9295\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1807 - accuracy: 0.9316\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1781 - accuracy: 0.9336\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1914 - accuracy: 0.9280\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1875 - accuracy: 0.9338\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1771 - accuracy: 0.9340\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1705 - accuracy: 0.9350\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1688 - accuracy: 0.9372\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1926 - accuracy: 0.9308\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1728 - accuracy: 0.9361\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1748 - accuracy: 0.9360\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1732 - accuracy: 0.9375\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1675 - accuracy: 0.9389\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1686 - accuracy: 0.9377\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1736 - accuracy: 0.9377\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1867 - accuracy: 0.9347\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1541 - accuracy: 0.9417\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 18s 60ms/step - loss: 0.1599 - accuracy: 0.9414\n",
      "152/152 [==============================] - 2s 14ms/step - loss: 0.6361 - accuracy: 0.8963\n",
      "n_conv_1 = 768, k_conv_1 = 7,  n_conv_2 = 512, k_conv_2 = 7, \n",
      "          n_conv_3 = 768,  k_conv_3 = 3,  maxpooling_pool_size = 4,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 768, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  relu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/304 [..............................] - ETA: 0s - loss: 1.7944 - accuracy: 0.1172WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0140s vs `on_train_batch_end` time: 0.0275s). Check your callbacks.\n",
      "304/304 [==============================] - 12s 39ms/step - loss: 1.0649 - accuracy: 0.5605\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.6671 - accuracy: 0.7377\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.4737 - accuracy: 0.8192\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.3866 - accuracy: 0.8532\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.3480 - accuracy: 0.8694\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.3201 - accuracy: 0.8779\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.3072 - accuracy: 0.8860\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.2726 - accuracy: 0.8972\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.2628 - accuracy: 0.8982\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.2702 - accuracy: 0.8985\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.2385 - accuracy: 0.9099\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.2231 - accuracy: 0.9161\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.2129 - accuracy: 0.9194\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.2014 - accuracy: 0.9236\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1951 - accuracy: 0.9269\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1829 - accuracy: 0.9295\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1807 - accuracy: 0.9311\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1698 - accuracy: 0.9355\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1698 - accuracy: 0.9354\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1630 - accuracy: 0.9381\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1568 - accuracy: 0.9403\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1515 - accuracy: 0.9417\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1592 - accuracy: 0.9376\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1439 - accuracy: 0.9438\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1427 - accuracy: 0.9437\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1347 - accuracy: 0.9480\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1306 - accuracy: 0.9488\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1358 - accuracy: 0.9474\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1269 - accuracy: 0.9509\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1252 - accuracy: 0.9505\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1156 - accuracy: 0.9545\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1220 - accuracy: 0.9527\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1236 - accuracy: 0.9518\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1233 - accuracy: 0.9522\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1115 - accuracy: 0.9569\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1192 - accuracy: 0.9528\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1091 - accuracy: 0.9568\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1085 - accuracy: 0.9589\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1080 - accuracy: 0.9578\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1153 - accuracy: 0.9566\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1052 - accuracy: 0.9589\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1160 - accuracy: 0.9572\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1015 - accuracy: 0.9598\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1015 - accuracy: 0.9604\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0992 - accuracy: 0.9613\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0974 - accuracy: 0.9637\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1015 - accuracy: 0.9619\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0914 - accuracy: 0.9646\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0972 - accuracy: 0.9623\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0986 - accuracy: 0.9615\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0934 - accuracy: 0.9631\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0974 - accuracy: 0.9626\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0898 - accuracy: 0.9663\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0907 - accuracy: 0.9639\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0966 - accuracy: 0.9635\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0916 - accuracy: 0.9660\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0893 - accuracy: 0.9663\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0881 - accuracy: 0.9673\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0842 - accuracy: 0.9682\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0905 - accuracy: 0.9658\n",
      "152/152 [==============================] - 2s 12ms/step - loss: 0.3845 - accuracy: 0.9311\n",
      "n_conv_1 = 768, k_conv_1 = 7,  n_conv_2 = 512, k_conv_2 = 7, \n",
      "          n_conv_3 = 768,  k_conv_3 = 3,  maxpooling_pool_size = 4,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 768, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  relu\n",
      "Epoch 1/60\n",
      "  1/304 [..............................] - ETA: 7s - loss: 1.7923 - accuracy: 0.2109WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0130s vs `on_train_batch_end` time: 0.0260s). Check your callbacks.\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 1.0514 - accuracy: 0.5596\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.6886 - accuracy: 0.7315\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.4818 - accuracy: 0.8112\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.4097 - accuracy: 0.8409\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.3629 - accuracy: 0.8590\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.3356 - accuracy: 0.8736\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.3026 - accuracy: 0.8819\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.2857 - accuracy: 0.8897\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.2638 - accuracy: 0.8975\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.2701 - accuracy: 0.8973\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.2381 - accuracy: 0.9082\n",
      "Epoch 12/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 12s 38ms/step - loss: 0.2221 - accuracy: 0.9139\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.2129 - accuracy: 0.9172\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.2001 - accuracy: 0.9221\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1935 - accuracy: 0.9251\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1852 - accuracy: 0.9260\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1870 - accuracy: 0.9273\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1726 - accuracy: 0.9341\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1657 - accuracy: 0.9351\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1612 - accuracy: 0.9379\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1494 - accuracy: 0.9418\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1503 - accuracy: 0.9426\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1503 - accuracy: 0.9425\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1390 - accuracy: 0.9485\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1353 - accuracy: 0.9483\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1387 - accuracy: 0.9473\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1277 - accuracy: 0.9507\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1269 - accuracy: 0.9513\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1258 - accuracy: 0.9518\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1243 - accuracy: 0.9522\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1241 - accuracy: 0.9532\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1173 - accuracy: 0.9541\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1180 - accuracy: 0.9565\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1100 - accuracy: 0.9573\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1169 - accuracy: 0.9567\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1122 - accuracy: 0.9575\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1105 - accuracy: 0.9571\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1073 - accuracy: 0.9594\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1024 - accuracy: 0.9588\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1061 - accuracy: 0.9597\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1080 - accuracy: 0.9583\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1058 - accuracy: 0.9596\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0988 - accuracy: 0.9616\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0998 - accuracy: 0.9628\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1006 - accuracy: 0.9618\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0926 - accuracy: 0.9636\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0960 - accuracy: 0.9636\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0912 - accuracy: 0.9647\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0963 - accuracy: 0.9633\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0935 - accuracy: 0.9652\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0935 - accuracy: 0.9644\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0927 - accuracy: 0.9652\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0987 - accuracy: 0.9639\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0926 - accuracy: 0.9660\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0851 - accuracy: 0.9676\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0880 - accuracy: 0.9657\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0885 - accuracy: 0.9675\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0939 - accuracy: 0.9652\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0883 - accuracy: 0.9663\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0863 - accuracy: 0.9678\n",
      "152/152 [==============================] - 2s 11ms/step - loss: 0.3896 - accuracy: 0.9290\n",
      "n_conv_1 = 768, k_conv_1 = 7,  n_conv_2 = 512, k_conv_2 = 7, \n",
      "          n_conv_3 = 768,  k_conv_3 = 3,  maxpooling_pool_size = 4,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 768, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  relu\n",
      "Epoch 1/60\n",
      "  1/304 [..............................] - ETA: 7s - loss: 1.7941 - accuracy: 0.2031WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0130s vs `on_train_batch_end` time: 0.0254s). Check your callbacks.\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 1.0578 - accuracy: 0.5608\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.6821 - accuracy: 0.7329\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.4801 - accuracy: 0.8100\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.3995 - accuracy: 0.8441\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.3527 - accuracy: 0.8657\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.3248 - accuracy: 0.8785\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.3069 - accuracy: 0.8842\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.2853 - accuracy: 0.8928\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.2679 - accuracy: 0.9004\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.2521 - accuracy: 0.9056\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.2356 - accuracy: 0.9119\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.2222 - accuracy: 0.9149\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.2094 - accuracy: 0.9206\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1992 - accuracy: 0.9239\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1867 - accuracy: 0.9295\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1812 - accuracy: 0.9309\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1767 - accuracy: 0.9324\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1659 - accuracy: 0.9358\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1595 - accuracy: 0.9387\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1562 - accuracy: 0.9411\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1527 - accuracy: 0.9416\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1433 - accuracy: 0.9451\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1498 - accuracy: 0.9437\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1378 - accuracy: 0.9465\n",
      "Epoch 25/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1379 - accuracy: 0.9469\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1314 - accuracy: 0.9496\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1285 - accuracy: 0.9512\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1259 - accuracy: 0.9526\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1246 - accuracy: 0.9534\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1206 - accuracy: 0.9538\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1193 - accuracy: 0.9539\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1225 - accuracy: 0.9541\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1185 - accuracy: 0.9557\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1114 - accuracy: 0.9574\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1087 - accuracy: 0.9577\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1106 - accuracy: 0.9582\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1088 - accuracy: 0.9594\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1086 - accuracy: 0.9585\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1037 - accuracy: 0.9607\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0994 - accuracy: 0.9630\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1044 - accuracy: 0.9598\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0982 - accuracy: 0.9624\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1060 - accuracy: 0.9604\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1030 - accuracy: 0.9609\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0956 - accuracy: 0.9643\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0935 - accuracy: 0.9646\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0962 - accuracy: 0.9641\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0962 - accuracy: 0.9636\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0962 - accuracy: 0.9640\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.1028 - accuracy: 0.9623\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0925 - accuracy: 0.9655\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0874 - accuracy: 0.9675\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0866 - accuracy: 0.9665\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0835 - accuracy: 0.9673\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0989 - accuracy: 0.9648\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0957 - accuracy: 0.9655\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0905 - accuracy: 0.9675\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0939 - accuracy: 0.9657\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0858 - accuracy: 0.9684\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 0.0867 - accuracy: 0.9666\n",
      "152/152 [==============================] - 2s 11ms/step - loss: 0.3381 - accuracy: 0.9374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1812, in _create_c_op\n",
      "    c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Negative dimension size caused by subtracting 7 from 4 for '{{node conv1d_32/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv1d_32/conv1d/ExpandDims, conv1d_32/conv1d/ExpandDims_1)' with input shapes: [?,1,4,768], [1,7,768,768].\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\", line 223, in fit\n",
      "    return super(KerasClassifier, self).fit(x, y, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\", line 157, in fit\n",
      "    self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n",
      "  File \"<ipython-input-10-eac9e5d5b478>\", line 10, in create_model\n",
      "    model.add(Conv1D(n_conv_3, k_conv_3, activation=activation))\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\n",
      "    result = method(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\", line 221, in add\n",
      "    output_tensor = layer(self.outputs[0])\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 926, in __call__\n",
      "    input_list)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 1117, in _functional_construction_call\n",
      "    outputs = call_fn(cast_inputs, *args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\", line 247, in call\n",
      "    outputs = self._convolution_op(inputs, self.kernel)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1017, in convolution_v2\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1147, in convolution_internal\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 574, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 574, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1888, in conv1d\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 979, in conv2d\n",
      "    data_format=data_format, dilations=dilations, name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 744, in _apply_op_helper\n",
      "    attrs=attr_protos, op_def=op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 593, in _create_op_internal\n",
      "    compute_device)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3485, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1975, in __init__\n",
      "    control_input_ops, op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1815, in _create_c_op\n",
      "    raise ValueError(str(e))\n",
      "ValueError: Negative dimension size caused by subtracting 7 from 4 for '{{node conv1d_32/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv1d_32/conv1d/ExpandDims, conv1d_32/conv1d/ExpandDims_1)' with input shapes: [?,1,4,768], [1,7,768,768].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1812, in _create_c_op\n",
      "    c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Negative dimension size caused by subtracting 7 from 4 for '{{node conv1d_35/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv1d_35/conv1d/ExpandDims, conv1d_35/conv1d/ExpandDims_1)' with input shapes: [?,1,4,768], [1,7,768,768].\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\", line 223, in fit\n",
      "    return super(KerasClassifier, self).fit(x, y, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\", line 157, in fit\n",
      "    self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n",
      "  File \"<ipython-input-10-eac9e5d5b478>\", line 10, in create_model\n",
      "    model.add(Conv1D(n_conv_3, k_conv_3, activation=activation))\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\n",
      "    result = method(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\", line 221, in add\n",
      "    output_tensor = layer(self.outputs[0])\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 926, in __call__\n",
      "    input_list)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 1117, in _functional_construction_call\n",
      "    outputs = call_fn(cast_inputs, *args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\", line 247, in call\n",
      "    outputs = self._convolution_op(inputs, self.kernel)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1017, in convolution_v2\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1147, in convolution_internal\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 574, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 574, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1888, in conv1d\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 979, in conv2d\n",
      "    data_format=data_format, dilations=dilations, name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 744, in _apply_op_helper\n",
      "    attrs=attr_protos, op_def=op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 593, in _create_op_internal\n",
      "    compute_device)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3485, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1975, in __init__\n",
      "    control_input_ops, op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1815, in _create_c_op\n",
      "    raise ValueError(str(e))\n",
      "ValueError: Negative dimension size caused by subtracting 7 from 4 for '{{node conv1d_35/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv1d_35/conv1d/ExpandDims, conv1d_35/conv1d/ExpandDims_1)' with input shapes: [?,1,4,768], [1,7,768,768].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1812, in _create_c_op\n",
      "    c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Negative dimension size caused by subtracting 7 from 4 for '{{node conv1d_38/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv1d_38/conv1d/ExpandDims, conv1d_38/conv1d/ExpandDims_1)' with input shapes: [?,1,4,768], [1,7,768,768].\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\", line 223, in fit\n",
      "    return super(KerasClassifier, self).fit(x, y, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\", line 157, in fit\n",
      "    self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n",
      "  File \"<ipython-input-10-eac9e5d5b478>\", line 10, in create_model\n",
      "    model.add(Conv1D(n_conv_3, k_conv_3, activation=activation))\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\n",
      "    result = method(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\", line 221, in add\n",
      "    output_tensor = layer(self.outputs[0])\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 926, in __call__\n",
      "    input_list)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 1117, in _functional_construction_call\n",
      "    outputs = call_fn(cast_inputs, *args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\", line 247, in call\n",
      "    outputs = self._convolution_op(inputs, self.kernel)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1017, in convolution_v2\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1147, in convolution_internal\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 574, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 574, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1888, in conv1d\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 979, in conv2d\n",
      "    data_format=data_format, dilations=dilations, name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 744, in _apply_op_helper\n",
      "    attrs=attr_protos, op_def=op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 593, in _create_op_internal\n",
      "    compute_device)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3485, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1975, in __init__\n",
      "    control_input_ops, op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1815, in _create_c_op\n",
      "    raise ValueError(str(e))\n",
      "ValueError: Negative dimension size caused by subtracting 7 from 4 for '{{node conv1d_38/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv1d_38/conv1d/ExpandDims, conv1d_38/conv1d/ExpandDims_1)' with input shapes: [?,1,4,768], [1,7,768,768].\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_conv_1 = 768, k_conv_1 = 7,  n_conv_2 = 512, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 7,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 4,  n_dense_1 = 512, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  elu\n",
      "Epoch 1/60\n",
      "  1/304 [..............................] - ETA: 4s - loss: 1.7935 - accuracy: 0.1406WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0150s vs `on_train_batch_end` time: 0.0270s). Check your callbacks.\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 1.3582 - accuracy: 0.4248\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.9802 - accuracy: 0.6037\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.7848 - accuracy: 0.6912\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.9657 - accuracy: 0.6589\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.7080 - accuracy: 0.7186\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.5845 - accuracy: 0.7696\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.5355 - accuracy: 0.7892\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.4880 - accuracy: 0.8043\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.4523 - accuracy: 0.8206\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.4327 - accuracy: 0.8260\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.4250 - accuracy: 0.8305\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.3960 - accuracy: 0.8400\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.3729 - accuracy: 0.8505\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.3684 - accuracy: 0.8506\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.3528 - accuracy: 0.8581\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.3354 - accuracy: 0.8648\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 13s 41ms/step - loss: 0.3468 - accuracy: 0.8621\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.3220 - accuracy: 0.8720\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.3134 - accuracy: 0.8743\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.3052 - accuracy: 0.8786\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2992 - accuracy: 0.8790\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2965 - accuracy: 0.8813\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2904 - accuracy: 0.8825\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 13s 41ms/step - loss: 0.2815 - accuracy: 0.8862\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2776 - accuracy: 0.8876\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2705 - accuracy: 0.8901\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2648 - accuracy: 0.8922\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2675 - accuracy: 0.8926\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2574 - accuracy: 0.8956\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2545 - accuracy: 0.8977\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 13s 41ms/step - loss: 0.2543 - accuracy: 0.8976\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2608 - accuracy: 0.8965\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2466 - accuracy: 0.9014\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2674 - accuracy: 0.8936\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2359 - accuracy: 0.9057\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2403 - accuracy: 0.9056\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2320 - accuracy: 0.9067\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2406 - accuracy: 0.9053\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2246 - accuracy: 0.9108\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2209 - accuracy: 0.9114\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2254 - accuracy: 0.9111\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2369 - accuracy: 0.9076\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2230 - accuracy: 0.9107\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2238 - accuracy: 0.9106\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2163 - accuracy: 0.9131\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2186 - accuracy: 0.9123\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2251 - accuracy: 0.9141\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2148 - accuracy: 0.9148\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2108 - accuracy: 0.9165\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2053 - accuracy: 0.9175\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2004 - accuracy: 0.9213\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2093 - accuracy: 0.9164\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2009 - accuracy: 0.9199\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.1988 - accuracy: 0.9225\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.1986 - accuracy: 0.9211\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2088 - accuracy: 0.9212\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.1885 - accuracy: 0.9260\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2025 - accuracy: 0.9210\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2086 - accuracy: 0.9209\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.1941 - accuracy: 0.9255\n",
      "152/152 [==============================] - 2s 13ms/step - loss: 0.4152 - accuracy: 0.9070\n",
      "n_conv_1 = 768, k_conv_1 = 7,  n_conv_2 = 512, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 7,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 4,  n_dense_1 = 512, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  elu\n",
      "Epoch 1/60\n",
      "  1/304 [..............................] - ETA: 8s - loss: 1.8037 - accuracy: 0.1328WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0140s vs `on_train_batch_end` time: 0.0286s). Check your callbacks.\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 1.3426 - accuracy: 0.4334\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 1.3557 - accuracy: 0.5467\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.8229 - accuracy: 0.6686\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.6730 - accuracy: 0.7327\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.5926 - accuracy: 0.7630\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.5555 - accuracy: 0.7781\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.5038 - accuracy: 0.8016\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.4652 - accuracy: 0.8134\n",
      "Epoch 9/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 12s 41ms/step - loss: 0.4434 - accuracy: 0.8234\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.4262 - accuracy: 0.8313\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.4078 - accuracy: 0.8393\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.3949 - accuracy: 0.8439\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.3745 - accuracy: 0.8523\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.3530 - accuracy: 0.8605\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.3540 - accuracy: 0.8597\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.3395 - accuracy: 0.8662\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.3281 - accuracy: 0.8705\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.3245 - accuracy: 0.8692\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.3096 - accuracy: 0.8763\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.3071 - accuracy: 0.8773\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2909 - accuracy: 0.8834\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2870 - accuracy: 0.8852\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2952 - accuracy: 0.8827\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2904 - accuracy: 0.8841\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2770 - accuracy: 0.8884\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2600 - accuracy: 0.8935\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2710 - accuracy: 0.8902\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2620 - accuracy: 0.8941\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2638 - accuracy: 0.8936\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2574 - accuracy: 0.8956\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2452 - accuracy: 0.8993\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 13s 41ms/step - loss: 0.2564 - accuracy: 0.8978\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2451 - accuracy: 0.9001\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2450 - accuracy: 0.9014\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2471 - accuracy: 0.9013\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2418 - accuracy: 0.9038\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2450 - accuracy: 0.9040\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2709 - accuracy: 0.8941\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2330 - accuracy: 0.9066\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2217 - accuracy: 0.9099\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2313 - accuracy: 0.9077\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2222 - accuracy: 0.9102\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2218 - accuracy: 0.9107\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2305 - accuracy: 0.9099\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2334 - accuracy: 0.9091\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2243 - accuracy: 0.9140\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2296 - accuracy: 0.9104\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2036 - accuracy: 0.9185\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2083 - accuracy: 0.9173\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2056 - accuracy: 0.9186\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2091 - accuracy: 0.9178\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2009 - accuracy: 0.9213\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2019 - accuracy: 0.9204\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2065 - accuracy: 0.9212\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2059 - accuracy: 0.9198\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.1926 - accuracy: 0.9270\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.1947 - accuracy: 0.9243\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.1937 - accuracy: 0.9259\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.1951 - accuracy: 0.9249\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2029 - accuracy: 0.9242\n",
      "152/152 [==============================] - 2s 12ms/step - loss: 0.4574 - accuracy: 0.9031\n",
      "n_conv_1 = 768, k_conv_1 = 7,  n_conv_2 = 512, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 7,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 4,  n_dense_1 = 512, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  elu\n",
      "Epoch 1/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 1.3331 - accuracy: 0.4437\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 1.0033 - accuracy: 0.5916\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.7936 - accuracy: 0.6853\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 1.0728 - accuracy: 0.6148\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.7693 - accuracy: 0.6946\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.5872 - accuracy: 0.7637\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.5272 - accuracy: 0.7882\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.4950 - accuracy: 0.8023\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.4558 - accuracy: 0.8151\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.4356 - accuracy: 0.8245\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.4345 - accuracy: 0.8280\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.3959 - accuracy: 0.8409\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.3918 - accuracy: 0.8413\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.3688 - accuracy: 0.8522\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.3769 - accuracy: 0.8498\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.3510 - accuracy: 0.8608\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.3388 - accuracy: 0.8655\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.3277 - accuracy: 0.8688\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.3211 - accuracy: 0.8709\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.3112 - accuracy: 0.8728\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.3065 - accuracy: 0.8757\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2926 - accuracy: 0.8817\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2974 - accuracy: 0.8814\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2833 - accuracy: 0.8855\n",
      "Epoch 25/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2780 - accuracy: 0.8875\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2774 - accuracy: 0.8886\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2720 - accuracy: 0.8904\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2639 - accuracy: 0.8941\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 12s 41ms/step - loss: 0.2542 - accuracy: 0.8985\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.2604 - accuracy: 0.8960\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 0.2604 - accuracy: 0.8949\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2475 - accuracy: 0.9010\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2469 - accuracy: 0.9017\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2488 - accuracy: 0.9018\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2356 - accuracy: 0.9059\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2398 - accuracy: 0.9050\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2331 - accuracy: 0.9078\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2334 - accuracy: 0.9088\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2340 - accuracy: 0.9089\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2368 - accuracy: 0.9079\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.2203 - accuracy: 0.9129\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.2189 - accuracy: 0.9147\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 13s 41ms/step - loss: 0.2307 - accuracy: 0.9112\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.2230 - accuracy: 0.9130\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 13s 41ms/step - loss: 0.2123 - accuracy: 0.9175\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.2138 - accuracy: 0.9169\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 13s 41ms/step - loss: 0.2142 - accuracy: 0.9176\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.2147 - accuracy: 0.9173\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 13s 41ms/step - loss: 0.2047 - accuracy: 0.9199\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 13s 41ms/step - loss: 0.2114 - accuracy: 0.9166\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 13s 41ms/step - loss: 0.2206 - accuracy: 0.9174\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.2151 - accuracy: 0.9164\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 13s 41ms/step - loss: 0.2049 - accuracy: 0.9203\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 13s 41ms/step - loss: 0.2070 - accuracy: 0.9215\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 13s 41ms/step - loss: 0.1968 - accuracy: 0.9247\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 13s 41ms/step - loss: 0.1978 - accuracy: 0.9214\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 13s 41ms/step - loss: 0.2000 - accuracy: 0.9230\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 13s 41ms/step - loss: 0.2038 - accuracy: 0.9227\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 13s 41ms/step - loss: 0.1903 - accuracy: 0.9258\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 13s 41ms/step - loss: 0.1971 - accuracy: 0.9253\n",
      "152/152 [==============================] - 2s 13ms/step - loss: 0.3965 - accuracy: 0.9004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1812, in _create_c_op\n",
      "    c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Negative dimension size caused by subtracting 7 from 5 for '{{node conv1d_50/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv1d_50/conv1d/ExpandDims, conv1d_50/conv1d/ExpandDims_1)' with input shapes: [?,1,5,768], [1,7,768,512].\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\", line 223, in fit\n",
      "    return super(KerasClassifier, self).fit(x, y, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\", line 157, in fit\n",
      "    self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n",
      "  File \"<ipython-input-10-eac9e5d5b478>\", line 10, in create_model\n",
      "    model.add(Conv1D(n_conv_3, k_conv_3, activation=activation))\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\n",
      "    result = method(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\", line 221, in add\n",
      "    output_tensor = layer(self.outputs[0])\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 926, in __call__\n",
      "    input_list)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 1117, in _functional_construction_call\n",
      "    outputs = call_fn(cast_inputs, *args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\", line 247, in call\n",
      "    outputs = self._convolution_op(inputs, self.kernel)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1017, in convolution_v2\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1147, in convolution_internal\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 574, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 574, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1888, in conv1d\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 979, in conv2d\n",
      "    data_format=data_format, dilations=dilations, name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 744, in _apply_op_helper\n",
      "    attrs=attr_protos, op_def=op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 593, in _create_op_internal\n",
      "    compute_device)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3485, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1975, in __init__\n",
      "    control_input_ops, op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1815, in _create_c_op\n",
      "    raise ValueError(str(e))\n",
      "ValueError: Negative dimension size caused by subtracting 7 from 5 for '{{node conv1d_50/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv1d_50/conv1d/ExpandDims, conv1d_50/conv1d/ExpandDims_1)' with input shapes: [?,1,5,768], [1,7,768,512].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1812, in _create_c_op\n",
      "    c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Negative dimension size caused by subtracting 7 from 5 for '{{node conv1d_53/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv1d_53/conv1d/ExpandDims, conv1d_53/conv1d/ExpandDims_1)' with input shapes: [?,1,5,768], [1,7,768,512].\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\", line 223, in fit\n",
      "    return super(KerasClassifier, self).fit(x, y, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\", line 157, in fit\n",
      "    self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n",
      "  File \"<ipython-input-10-eac9e5d5b478>\", line 10, in create_model\n",
      "    model.add(Conv1D(n_conv_3, k_conv_3, activation=activation))\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\n",
      "    result = method(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\", line 221, in add\n",
      "    output_tensor = layer(self.outputs[0])\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 926, in __call__\n",
      "    input_list)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 1117, in _functional_construction_call\n",
      "    outputs = call_fn(cast_inputs, *args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\", line 247, in call\n",
      "    outputs = self._convolution_op(inputs, self.kernel)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1017, in convolution_v2\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1147, in convolution_internal\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 574, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 574, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1888, in conv1d\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 979, in conv2d\n",
      "    data_format=data_format, dilations=dilations, name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 744, in _apply_op_helper\n",
      "    attrs=attr_protos, op_def=op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 593, in _create_op_internal\n",
      "    compute_device)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3485, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1975, in __init__\n",
      "    control_input_ops, op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1815, in _create_c_op\n",
      "    raise ValueError(str(e))\n",
      "ValueError: Negative dimension size caused by subtracting 7 from 5 for '{{node conv1d_53/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv1d_53/conv1d/ExpandDims, conv1d_53/conv1d/ExpandDims_1)' with input shapes: [?,1,5,768], [1,7,768,512].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1812, in _create_c_op\n",
      "    c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Negative dimension size caused by subtracting 7 from 5 for '{{node conv1d_56/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv1d_56/conv1d/ExpandDims, conv1d_56/conv1d/ExpandDims_1)' with input shapes: [?,1,5,768], [1,7,768,512].\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\", line 223, in fit\n",
      "    return super(KerasClassifier, self).fit(x, y, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\", line 157, in fit\n",
      "    self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n",
      "  File \"<ipython-input-10-eac9e5d5b478>\", line 10, in create_model\n",
      "    model.add(Conv1D(n_conv_3, k_conv_3, activation=activation))\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\n",
      "    result = method(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\", line 221, in add\n",
      "    output_tensor = layer(self.outputs[0])\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 926, in __call__\n",
      "    input_list)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 1117, in _functional_construction_call\n",
      "    outputs = call_fn(cast_inputs, *args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\", line 247, in call\n",
      "    outputs = self._convolution_op(inputs, self.kernel)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1017, in convolution_v2\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1147, in convolution_internal\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 574, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 574, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1888, in conv1d\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 979, in conv2d\n",
      "    data_format=data_format, dilations=dilations, name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 744, in _apply_op_helper\n",
      "    attrs=attr_protos, op_def=op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 593, in _create_op_internal\n",
      "    compute_device)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3485, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1975, in __init__\n",
      "    control_input_ops, op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1815, in _create_c_op\n",
      "    raise ValueError(str(e))\n",
      "ValueError: Negative dimension size caused by subtracting 7 from 5 for '{{node conv1d_56/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv1d_56/conv1d/ExpandDims, conv1d_56/conv1d/ExpandDims_1)' with input shapes: [?,1,5,768], [1,7,768,512].\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_conv_1 = 512, k_conv_1 = 3,  n_conv_2 = 512, k_conv_2 = 7, \n",
      "          n_conv_3 = 768,  k_conv_3 = 7,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 768, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  relu\n",
      "Epoch 1/60\n",
      "  2/304 [..............................] - ETA: 8s - loss: 1.7684 - accuracy: 0.2070WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0169s vs `on_train_batch_end` time: 0.0359s). Check your callbacks.\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 1.0130 - accuracy: 0.5734\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.5568 - accuracy: 0.7763\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.4145 - accuracy: 0.8405\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.3532 - accuracy: 0.8640\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.3124 - accuracy: 0.8817\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.2885 - accuracy: 0.8923\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.2685 - accuracy: 0.8998\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.2544 - accuracy: 0.9031\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.2345 - accuracy: 0.9104\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.2217 - accuracy: 0.9132\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.2108 - accuracy: 0.9177\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1998 - accuracy: 0.9224\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1922 - accuracy: 0.9252\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1837 - accuracy: 0.9289\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1726 - accuracy: 0.9341\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1667 - accuracy: 0.9350\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1687 - accuracy: 0.9337\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1580 - accuracy: 0.9375\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1516 - accuracy: 0.9414\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1460 - accuracy: 0.9429\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1512 - accuracy: 0.9425\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1409 - accuracy: 0.9453\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1338 - accuracy: 0.9487\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1318 - accuracy: 0.9499\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1257 - accuracy: 0.9512\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1287 - accuracy: 0.9521\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1206 - accuracy: 0.9537\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1215 - accuracy: 0.9539\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1154 - accuracy: 0.9549\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1178 - accuracy: 0.9550\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1116 - accuracy: 0.9556\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1092 - accuracy: 0.9575\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1103 - accuracy: 0.9571\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1082 - accuracy: 0.9588\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1071 - accuracy: 0.9593\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1045 - accuracy: 0.9606\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1017 - accuracy: 0.9615\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1044 - accuracy: 0.9609\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0976 - accuracy: 0.9615\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0962 - accuracy: 0.9637\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0947 - accuracy: 0.9639\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0956 - accuracy: 0.9635\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0983 - accuracy: 0.9635\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0946 - accuracy: 0.9645\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0898 - accuracy: 0.9666\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0935 - accuracy: 0.9649\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0981 - accuracy: 0.9638\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0860 - accuracy: 0.9675\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0840 - accuracy: 0.9680\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0860 - accuracy: 0.9680\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0776 - accuracy: 0.9705\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0839 - accuracy: 0.9695\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0824 - accuracy: 0.9699\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0887 - accuracy: 0.9675\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0875 - accuracy: 0.9671\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0873 - accuracy: 0.9681\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0780 - accuracy: 0.9705\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0778 - accuracy: 0.9715\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0735 - accuracy: 0.9728\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0790 - accuracy: 0.9703\n",
      "152/152 [==============================] - 2s 15ms/step - loss: 0.3598 - accuracy: 0.9337\n",
      "n_conv_1 = 512, k_conv_1 = 3,  n_conv_2 = 512, k_conv_2 = 7, \n",
      "          n_conv_3 = 768,  k_conv_3 = 7,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 768, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  relu\n",
      "Epoch 1/60\n",
      "  2/304 [..............................] - ETA: 12s - loss: 1.8163 - accuracy: 0.1172WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0169s vs `on_train_batch_end` time: 0.0349s). Check your callbacks.\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 1.0170 - accuracy: 0.5701\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.5695 - accuracy: 0.7753\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.4149 - accuracy: 0.8401\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.3684 - accuracy: 0.8604\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.3198 - accuracy: 0.8790\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.2953 - accuracy: 0.8872\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.2664 - accuracy: 0.8985\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.2712 - accuracy: 0.8984\n",
      "Epoch 9/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 16s 52ms/step - loss: 0.2477 - accuracy: 0.9071\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.2244 - accuracy: 0.9126\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.2163 - accuracy: 0.9157\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.2006 - accuracy: 0.9222\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1933 - accuracy: 0.9236\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1884 - accuracy: 0.9256\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1728 - accuracy: 0.9317\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1670 - accuracy: 0.9344\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1631 - accuracy: 0.9370\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1652 - accuracy: 0.9372\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1483 - accuracy: 0.9420\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1551 - accuracy: 0.9404\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1440 - accuracy: 0.9442\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1409 - accuracy: 0.9453\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1345 - accuracy: 0.9474\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1347 - accuracy: 0.9469\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1307 - accuracy: 0.9476\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1282 - accuracy: 0.9489\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1267 - accuracy: 0.9532\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1174 - accuracy: 0.9552\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1193 - accuracy: 0.9534\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1172 - accuracy: 0.9530\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1186 - accuracy: 0.9530\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1163 - accuracy: 0.9551\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1106 - accuracy: 0.9577\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1065 - accuracy: 0.9583\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1041 - accuracy: 0.9583\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1060 - accuracy: 0.9588\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1031 - accuracy: 0.9597\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1106 - accuracy: 0.9597\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0956 - accuracy: 0.9628\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0916 - accuracy: 0.9648\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0981 - accuracy: 0.9627\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0952 - accuracy: 0.9649\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0906 - accuracy: 0.9666\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0960 - accuracy: 0.9640\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0890 - accuracy: 0.9664\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0866 - accuracy: 0.9681\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0864 - accuracy: 0.9685\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0920 - accuracy: 0.9664\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0824 - accuracy: 0.9688\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0931 - accuracy: 0.9652\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0820 - accuracy: 0.9692\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0801 - accuracy: 0.9700\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0823 - accuracy: 0.9696\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0851 - accuracy: 0.9690\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0884 - accuracy: 0.9671\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0812 - accuracy: 0.9697\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0854 - accuracy: 0.9688\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0771 - accuracy: 0.9713\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0867 - accuracy: 0.9682\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0759 - accuracy: 0.9726\n",
      "152/152 [==============================] - 2s 15ms/step - loss: 0.3446 - accuracy: 0.9326\n",
      "n_conv_1 = 512, k_conv_1 = 3,  n_conv_2 = 512, k_conv_2 = 7, \n",
      "          n_conv_3 = 768,  k_conv_3 = 7,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 768, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  relu\n",
      "Epoch 1/60\n",
      "  2/304 [..............................] - ETA: 13s - loss: 1.7748 - accuracy: 0.1953WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0170s vs `on_train_batch_end` time: 0.0359s). Check your callbacks.\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.9940 - accuracy: 0.5818\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.5770 - accuracy: 0.7736\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.4289 - accuracy: 0.8353\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.3553 - accuracy: 0.8621\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.3361 - accuracy: 0.8726\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.2982 - accuracy: 0.8859\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.2708 - accuracy: 0.8952\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.2579 - accuracy: 0.9026\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.2507 - accuracy: 0.9048\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.2275 - accuracy: 0.9134\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.2267 - accuracy: 0.9143\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.2133 - accuracy: 0.9192\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1941 - accuracy: 0.9254\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1867 - accuracy: 0.9296\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1829 - accuracy: 0.9314\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1696 - accuracy: 0.9351\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1770 - accuracy: 0.9320\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1590 - accuracy: 0.9382\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1557 - accuracy: 0.9409\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1469 - accuracy: 0.9446\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1430 - accuracy: 0.9457\n",
      "Epoch 22/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1404 - accuracy: 0.9461\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1336 - accuracy: 0.9481\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1355 - accuracy: 0.9470\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1319 - accuracy: 0.9512\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1270 - accuracy: 0.9519\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1243 - accuracy: 0.9521\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1204 - accuracy: 0.9536\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1183 - accuracy: 0.9547\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1188 - accuracy: 0.9548\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1137 - accuracy: 0.9574\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1101 - accuracy: 0.9579\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1106 - accuracy: 0.9580\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1143 - accuracy: 0.9570\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1072 - accuracy: 0.9606\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1021 - accuracy: 0.9629\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1033 - accuracy: 0.9615\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.1027 - accuracy: 0.9625\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0968 - accuracy: 0.9640\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0981 - accuracy: 0.9629\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0934 - accuracy: 0.9643\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0944 - accuracy: 0.9644\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0909 - accuracy: 0.9662\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0908 - accuracy: 0.9658\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0929 - accuracy: 0.9663\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0939 - accuracy: 0.9659\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0914 - accuracy: 0.9659\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0881 - accuracy: 0.9672\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0928 - accuracy: 0.9671\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0882 - accuracy: 0.9675\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0867 - accuracy: 0.9675\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0942 - accuracy: 0.9649\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0839 - accuracy: 0.9684\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0835 - accuracy: 0.9691\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0771 - accuracy: 0.9707\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0822 - accuracy: 0.9708\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0822 - accuracy: 0.9700\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0805 - accuracy: 0.9712\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0802 - accuracy: 0.9701\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 0.0800 - accuracy: 0.9704\n",
      "152/152 [==============================] - 2s 15ms/step - loss: 0.3378 - accuracy: 0.9411\n",
      "n_conv_1 = 512, k_conv_1 = 7,  n_conv_2 = 512, k_conv_2 = 7, \n",
      "          n_conv_3 = 768,  k_conv_3 = 7,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 768, dropout_1 = 0.3,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  elu\n",
      "Epoch 1/60\n",
      "  2/304 [..............................] - ETA: 11s - loss: 2.3855 - accuracy: 0.2266WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0190s vs `on_train_batch_end` time: 0.0389s). Check your callbacks.\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 1.9653 - accuracy: 0.4034\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 1.1035 - accuracy: 0.5345\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.8895 - accuracy: 0.6327\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.7374 - accuracy: 0.7011\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.6478 - accuracy: 0.7416\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.5951 - accuracy: 0.7639\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.5486 - accuracy: 0.7814\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.5344 - accuracy: 0.7886\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.4969 - accuracy: 0.8045\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.4762 - accuracy: 0.8099\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.4594 - accuracy: 0.8176\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.4459 - accuracy: 0.8249\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.4276 - accuracy: 0.8311\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.4185 - accuracy: 0.8358\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.4099 - accuracy: 0.8392\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3985 - accuracy: 0.8452\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3953 - accuracy: 0.8456\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3742 - accuracy: 0.8530\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3575 - accuracy: 0.8574\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3618 - accuracy: 0.8594\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3541 - accuracy: 0.8608\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3551 - accuracy: 0.8619\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3340 - accuracy: 0.8695\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3190 - accuracy: 0.8750\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3233 - accuracy: 0.8739\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3064 - accuracy: 0.8772\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3034 - accuracy: 0.8804\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2990 - accuracy: 0.8783\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2895 - accuracy: 0.8831\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2909 - accuracy: 0.8847\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2767 - accuracy: 0.8902\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2700 - accuracy: 0.8933\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2703 - accuracy: 0.8913\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2634 - accuracy: 0.8947\n",
      "Epoch 35/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2565 - accuracy: 0.8972\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2464 - accuracy: 0.9021\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2543 - accuracy: 0.9016\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2425 - accuracy: 0.9030\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2385 - accuracy: 0.9054\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2748 - accuracy: 0.8914\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2379 - accuracy: 0.9057\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2265 - accuracy: 0.9121\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2214 - accuracy: 0.9123\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2201 - accuracy: 0.9134\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2191 - accuracy: 0.9127\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2226 - accuracy: 0.9111\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2097 - accuracy: 0.9184\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2181 - accuracy: 0.9145\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2253 - accuracy: 0.9123\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2070 - accuracy: 0.9172\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2028 - accuracy: 0.9219\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2009 - accuracy: 0.9218\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2155 - accuracy: 0.9178\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2027 - accuracy: 0.9211\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2102 - accuracy: 0.9181\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2202 - accuracy: 0.9164\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.1931 - accuracy: 0.9236\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.1905 - accuracy: 0.9272\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.1976 - accuracy: 0.9245\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.1991 - accuracy: 0.9246\n",
      "152/152 [==============================] - 2s 15ms/step - loss: 0.3307 - accuracy: 0.9159 0s - loss: 0.3404 \n",
      "n_conv_1 = 512, k_conv_1 = 7,  n_conv_2 = 512, k_conv_2 = 7, \n",
      "          n_conv_3 = 768,  k_conv_3 = 7,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 768, dropout_1 = 0.3,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  elu\n",
      "Epoch 1/60\n",
      "  2/304 [..............................] - ETA: 12s - loss: 2.2829 - accuracy: 0.2031WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0169s vs `on_train_batch_end` time: 0.0342s). Check your callbacks.\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 1.9674 - accuracy: 0.3992\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 1.1235 - accuracy: 0.5309\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.8987 - accuracy: 0.6301\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.8005 - accuracy: 0.6722\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.6996 - accuracy: 0.7213\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.6236 - accuracy: 0.7557\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.5780 - accuracy: 0.7777\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.5417 - accuracy: 0.7891\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.5129 - accuracy: 0.8004\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.4886 - accuracy: 0.8082\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.4879 - accuracy: 0.8100\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.4540 - accuracy: 0.8231\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.4411 - accuracy: 0.8259\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.4176 - accuracy: 0.8346\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.4063 - accuracy: 0.8418\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3938 - accuracy: 0.8485\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3833 - accuracy: 0.8512\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3683 - accuracy: 0.8550\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3658 - accuracy: 0.8585\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3588 - accuracy: 0.8623\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3472 - accuracy: 0.8668\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3341 - accuracy: 0.8674\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3365 - accuracy: 0.8691\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3223 - accuracy: 0.8734\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3110 - accuracy: 0.8760\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3062 - accuracy: 0.8800\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2995 - accuracy: 0.8827\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2996 - accuracy: 0.8815\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2979 - accuracy: 0.8835\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2815 - accuracy: 0.8884\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2692 - accuracy: 0.8937\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2680 - accuracy: 0.8935\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2612 - accuracy: 0.8961\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2603 - accuracy: 0.8961\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2558 - accuracy: 0.8982\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2527 - accuracy: 0.9017\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2437 - accuracy: 0.9054\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2365 - accuracy: 0.9056\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2383 - accuracy: 0.9076\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2419 - accuracy: 0.9074\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2380 - accuracy: 0.9060\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2268 - accuracy: 0.9119\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2173 - accuracy: 0.9125\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2208 - accuracy: 0.9124\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2227 - accuracy: 0.9138\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2303 - accuracy: 0.9103\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2271 - accuracy: 0.9140\n",
      "Epoch 48/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2175 - accuracy: 0.9164\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2124 - accuracy: 0.9166\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2042 - accuracy: 0.9218\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2112 - accuracy: 0.9183\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2073 - accuracy: 0.9212\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2131 - accuracy: 0.9199\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.1979 - accuracy: 0.9227\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.1892 - accuracy: 0.9268\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.1955 - accuracy: 0.9249\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2077 - accuracy: 0.9224\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2063 - accuracy: 0.9227\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.1920 - accuracy: 0.9275\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.1949 - accuracy: 0.9251\n",
      "152/152 [==============================] - 2s 15ms/step - loss: 0.3995 - accuracy: 0.9075\n",
      "n_conv_1 = 512, k_conv_1 = 7,  n_conv_2 = 512, k_conv_2 = 7, \n",
      "          n_conv_3 = 768,  k_conv_3 = 7,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 768, dropout_1 = 0.3,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  elu\n",
      "Epoch 1/60\n",
      "  2/304 [..............................] - ETA: 12s - loss: 2.1250 - accuracy: 0.2734WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0170s vs `on_train_batch_end` time: 0.0359s). Check your callbacks.\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 1.4217 - accuracy: 0.4331\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 1.0289 - accuracy: 0.5815\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.8099 - accuracy: 0.6797\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 1.0944 - accuracy: 0.6435\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.7733 - accuracy: 0.7009\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.6196 - accuracy: 0.7567\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.5700 - accuracy: 0.7788\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.5295 - accuracy: 0.7892\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.4937 - accuracy: 0.8065\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.4756 - accuracy: 0.8113\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.4555 - accuracy: 0.8194\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.4349 - accuracy: 0.8291\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.4348 - accuracy: 0.8294\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.4012 - accuracy: 0.8404\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3889 - accuracy: 0.8464\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3775 - accuracy: 0.8519\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3575 - accuracy: 0.8594\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3419 - accuracy: 0.8667\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3392 - accuracy: 0.8671\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3255 - accuracy: 0.8705\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3170 - accuracy: 0.8743\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3081 - accuracy: 0.8769\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3052 - accuracy: 0.8789\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2943 - accuracy: 0.8827\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2943 - accuracy: 0.8839\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2677 - accuracy: 0.8940\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2635 - accuracy: 0.8951\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2588 - accuracy: 0.8973\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2541 - accuracy: 0.8988\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2498 - accuracy: 0.9018\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2458 - accuracy: 0.9043\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2425 - accuracy: 0.9035\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2418 - accuracy: 0.9031\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2394 - accuracy: 0.9065\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2353 - accuracy: 0.9090\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2263 - accuracy: 0.9112\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2276 - accuracy: 0.9115\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2247 - accuracy: 0.9111\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2198 - accuracy: 0.9143\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2121 - accuracy: 0.9160\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2271 - accuracy: 0.9121\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2103 - accuracy: 0.9179\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2166 - accuracy: 0.9146\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2139 - accuracy: 0.9192\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2051 - accuracy: 0.9198\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2096 - accuracy: 0.9183\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.2014 - accuracy: 0.9223\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 16s 52ms/step - loss: 1.9105 - accuracy: 0.5343\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 1.0069 - accuracy: 0.5880\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.6969 - accuracy: 0.7317\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.5653 - accuracy: 0.7924\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.4973 - accuracy: 0.8153\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.4534 - accuracy: 0.8308\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.4259 - accuracy: 0.8412\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.4026 - accuracy: 0.8494\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3883 - accuracy: 0.85450s - loss: 0.3883 - accuracy: 0.85\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3687 - accuracy: 0.8614\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3638 - accuracy: 0.8650\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3467 - accuracy: 0.8693\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 16s 53ms/step - loss: 0.3378 - accuracy: 0.8718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 2s 15ms/step - loss: 0.3401 - accuracy: 0.8802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1812, in _create_c_op\n",
      "    c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Negative dimension size caused by subtracting 7 from 4 for '{{node conv1d_77/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv1d_77/conv1d/ExpandDims, conv1d_77/conv1d/ExpandDims_1)' with input shapes: [?,1,4,512], [1,7,512,512].\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\", line 223, in fit\n",
      "    return super(KerasClassifier, self).fit(x, y, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\", line 157, in fit\n",
      "    self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n",
      "  File \"<ipython-input-10-eac9e5d5b478>\", line 10, in create_model\n",
      "    model.add(Conv1D(n_conv_3, k_conv_3, activation=activation))\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\n",
      "    result = method(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\", line 221, in add\n",
      "    output_tensor = layer(self.outputs[0])\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 926, in __call__\n",
      "    input_list)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 1117, in _functional_construction_call\n",
      "    outputs = call_fn(cast_inputs, *args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\", line 247, in call\n",
      "    outputs = self._convolution_op(inputs, self.kernel)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1017, in convolution_v2\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1147, in convolution_internal\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 574, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 574, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1888, in conv1d\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 979, in conv2d\n",
      "    data_format=data_format, dilations=dilations, name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 744, in _apply_op_helper\n",
      "    attrs=attr_protos, op_def=op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 593, in _create_op_internal\n",
      "    compute_device)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3485, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1975, in __init__\n",
      "    control_input_ops, op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1815, in _create_c_op\n",
      "    raise ValueError(str(e))\n",
      "ValueError: Negative dimension size caused by subtracting 7 from 4 for '{{node conv1d_77/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv1d_77/conv1d/ExpandDims, conv1d_77/conv1d/ExpandDims_1)' with input shapes: [?,1,4,512], [1,7,512,512].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1812, in _create_c_op\n",
      "    c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Negative dimension size caused by subtracting 7 from 4 for '{{node conv1d_80/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv1d_80/conv1d/ExpandDims, conv1d_80/conv1d/ExpandDims_1)' with input shapes: [?,1,4,512], [1,7,512,512].\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\", line 223, in fit\n",
      "    return super(KerasClassifier, self).fit(x, y, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\", line 157, in fit\n",
      "    self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n",
      "  File \"<ipython-input-10-eac9e5d5b478>\", line 10, in create_model\n",
      "    model.add(Conv1D(n_conv_3, k_conv_3, activation=activation))\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\n",
      "    result = method(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\", line 221, in add\n",
      "    output_tensor = layer(self.outputs[0])\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 926, in __call__\n",
      "    input_list)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 1117, in _functional_construction_call\n",
      "    outputs = call_fn(cast_inputs, *args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\", line 247, in call\n",
      "    outputs = self._convolution_op(inputs, self.kernel)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1017, in convolution_v2\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1147, in convolution_internal\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 574, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 574, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1888, in conv1d\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 979, in conv2d\n",
      "    data_format=data_format, dilations=dilations, name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 744, in _apply_op_helper\n",
      "    attrs=attr_protos, op_def=op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 593, in _create_op_internal\n",
      "    compute_device)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3485, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1975, in __init__\n",
      "    control_input_ops, op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1815, in _create_c_op\n",
      "    raise ValueError(str(e))\n",
      "ValueError: Negative dimension size caused by subtracting 7 from 4 for '{{node conv1d_80/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv1d_80/conv1d/ExpandDims, conv1d_80/conv1d/ExpandDims_1)' with input shapes: [?,1,4,512], [1,7,512,512].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1812, in _create_c_op\n",
      "    c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Negative dimension size caused by subtracting 7 from 4 for '{{node conv1d_83/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv1d_83/conv1d/ExpandDims, conv1d_83/conv1d/ExpandDims_1)' with input shapes: [?,1,4,512], [1,7,512,512].\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\", line 223, in fit\n",
      "    return super(KerasClassifier, self).fit(x, y, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\", line 157, in fit\n",
      "    self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n",
      "  File \"<ipython-input-10-eac9e5d5b478>\", line 10, in create_model\n",
      "    model.add(Conv1D(n_conv_3, k_conv_3, activation=activation))\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\n",
      "    result = method(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\", line 221, in add\n",
      "    output_tensor = layer(self.outputs[0])\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 926, in __call__\n",
      "    input_list)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 1117, in _functional_construction_call\n",
      "    outputs = call_fn(cast_inputs, *args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\", line 247, in call\n",
      "    outputs = self._convolution_op(inputs, self.kernel)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1017, in convolution_v2\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1147, in convolution_internal\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 574, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 574, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1888, in conv1d\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 979, in conv2d\n",
      "    data_format=data_format, dilations=dilations, name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 744, in _apply_op_helper\n",
      "    attrs=attr_protos, op_def=op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 593, in _create_op_internal\n",
      "    compute_device)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3485, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1975, in __init__\n",
      "    control_input_ops, op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1815, in _create_c_op\n",
      "    raise ValueError(str(e))\n",
      "ValueError: Negative dimension size caused by subtracting 7 from 4 for '{{node conv1d_83/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv1d_83/conv1d/ExpandDims, conv1d_83/conv1d/ExpandDims_1)' with input shapes: [?,1,4,512], [1,7,512,512].\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_conv_1 = 768, k_conv_1 = 7,  n_conv_2 = 512, k_conv_2 = 7, \n",
      "          n_conv_3 = 768,  k_conv_3 = 3,  maxpooling_pool_size = 4,\n",
      "          avepooling_pool_size = 4,  n_dense_1 = 512, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  elu\n",
      "Epoch 1/60\n",
      "  1/304 [..............................] - ETA: 6s - loss: 1.8076 - accuracy: 0.1406WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0140s vs `on_train_batch_end` time: 0.0249s). Check your callbacks.\n",
      "304/304 [==============================] - 11s 38ms/step - loss: 1.3059 - accuracy: 0.4569\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 1.0254 - accuracy: 0.5831\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.8125 - accuracy: 0.6781\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 1.2479 - accuracy: 0.5832\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.7376 - accuracy: 0.7061\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.6196 - accuracy: 0.7498\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.5563 - accuracy: 0.7779\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.5123 - accuracy: 0.7955\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4924 - accuracy: 0.8047\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4596 - accuracy: 0.8172\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4397 - accuracy: 0.8236\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4204 - accuracy: 0.8310\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4075 - accuracy: 0.8376\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3963 - accuracy: 0.8442\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3957 - accuracy: 0.8448\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3741 - accuracy: 0.8529\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3823 - accuracy: 0.8519\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3479 - accuracy: 0.8646\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3316 - accuracy: 0.8691\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3242 - accuracy: 0.8729\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3206 - accuracy: 0.8747\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3123 - accuracy: 0.8804\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2991 - accuracy: 0.8830\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2909 - accuracy: 0.8882\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2933 - accuracy: 0.8888\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2817 - accuracy: 0.8929\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2745 - accuracy: 0.8959\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2640 - accuracy: 0.8986\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2588 - accuracy: 0.9015\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2542 - accuracy: 0.9038\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2518 - accuracy: 0.9064\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2501 - accuracy: 0.9063\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2499 - accuracy: 0.9081\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2377 - accuracy: 0.9110\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2295 - accuracy: 0.9143\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2251 - accuracy: 0.9157\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2291 - accuracy: 0.9149\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2223 - accuracy: 0.9169\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2190 - accuracy: 0.9185\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2232 - accuracy: 0.9201\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2227 - accuracy: 0.9187\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2123 - accuracy: 0.9218\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2074 - accuracy: 0.9240\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2165 - accuracy: 0.9203\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.1987 - accuracy: 0.9272\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2100 - accuracy: 0.9243\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2122 - accuracy: 0.9227\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2059 - accuracy: 0.9266\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2048 - accuracy: 0.9259\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2037 - accuracy: 0.9257\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.1977 - accuracy: 0.9289\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.1884 - accuracy: 0.9323\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.1985 - accuracy: 0.9270\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.1919 - accuracy: 0.9303\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.1878 - accuracy: 0.9321\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.1969 - accuracy: 0.9308\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.1971 - accuracy: 0.9295\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.1993 - accuracy: 0.9300\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2583 - accuracy: 0.9040\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.1834 - accuracy: 0.9325\n",
      "152/152 [==============================] - 2s 11ms/step - loss: 0.3866 - accuracy: 0.9133\n",
      "n_conv_1 = 768, k_conv_1 = 7,  n_conv_2 = 512, k_conv_2 = 7, \n",
      "          n_conv_3 = 768,  k_conv_3 = 3,  maxpooling_pool_size = 4,\n",
      "          avepooling_pool_size = 4,  n_dense_1 = 512, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  elu\n",
      "Epoch 1/60\n",
      "  1/304 [..............................] - ETA: 7s - loss: 1.7869 - accuracy: 0.2188WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0130s vs `on_train_batch_end` time: 0.0264s). Check your callbacks.\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 1.6831 - accuracy: 0.4376\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 1.0340 - accuracy: 0.5755\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.8571 - accuracy: 0.6582\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.7452 - accuracy: 0.7054\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.6631 - accuracy: 0.7380\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.6198 - accuracy: 0.7576\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.5684 - accuracy: 0.7769\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.5002 - accuracy: 0.8029\n",
      "Epoch 9/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 11s 37ms/step - loss: 0.5176 - accuracy: 0.7980\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4542 - accuracy: 0.8205\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4477 - accuracy: 0.8265\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4163 - accuracy: 0.8376\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3962 - accuracy: 0.8469\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3900 - accuracy: 0.8469\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3764 - accuracy: 0.8537\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3516 - accuracy: 0.8627\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3493 - accuracy: 0.8670\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3480 - accuracy: 0.8669\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3345 - accuracy: 0.8718\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3188 - accuracy: 0.8788\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3114 - accuracy: 0.8815\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2979 - accuracy: 0.8858\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2878 - accuracy: 0.8893\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2918 - accuracy: 0.8907\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2781 - accuracy: 0.8945\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2713 - accuracy: 0.8961\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2675 - accuracy: 0.8963\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2650 - accuracy: 0.8993\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2687 - accuracy: 0.8998\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2518 - accuracy: 0.9043\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2559 - accuracy: 0.9014\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2539 - accuracy: 0.9033\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2450 - accuracy: 0.9070\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2410 - accuracy: 0.9102\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2287 - accuracy: 0.9134\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2422 - accuracy: 0.9104\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2314 - accuracy: 0.9129\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2290 - accuracy: 0.9149\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2231 - accuracy: 0.9145\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2173 - accuracy: 0.9176\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2228 - accuracy: 0.9156\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2268 - accuracy: 0.9152\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2169 - accuracy: 0.9193\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2146 - accuracy: 0.9198\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2125 - accuracy: 0.9217\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2097 - accuracy: 0.9227\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4130 - accuracy: 0.8531\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3010 - accuracy: 0.8886\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2297 - accuracy: 0.9123\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2169 - accuracy: 0.9174\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2161 - accuracy: 0.9218\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2038 - accuracy: 0.9227\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2034 - accuracy: 0.9246\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.1989 - accuracy: 0.9276\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2012 - accuracy: 0.9243\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.1937 - accuracy: 0.9279\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.1989 - accuracy: 0.9262\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.1891 - accuracy: 0.9309\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.1917 - accuracy: 0.9314\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.1951 - accuracy: 0.9284\n",
      "152/152 [==============================] - 2s 11ms/step - loss: 0.5594 - accuracy: 0.9076\n",
      "n_conv_1 = 768, k_conv_1 = 7,  n_conv_2 = 512, k_conv_2 = 7, \n",
      "          n_conv_3 = 768,  k_conv_3 = 3,  maxpooling_pool_size = 4,\n",
      "          avepooling_pool_size = 4,  n_dense_1 = 512, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  elu\n",
      "Epoch 1/60\n",
      "  1/304 [..............................] - ETA: 6s - loss: 1.7879 - accuracy: 0.1875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0130s vs `on_train_batch_end` time: 0.0239s). Check your callbacks.\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 1.6081 - accuracy: 0.4427\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 1.0360 - accuracy: 0.5758\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.8670 - accuracy: 0.6506\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.7446 - accuracy: 0.7043\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.6602 - accuracy: 0.7375\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.6063 - accuracy: 0.7579\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.5512 - accuracy: 0.7804\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.5202 - accuracy: 0.7919\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4950 - accuracy: 0.8038\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4665 - accuracy: 0.8172\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4612 - accuracy: 0.8215\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4187 - accuracy: 0.8363\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3922 - accuracy: 0.8490\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3912 - accuracy: 0.8504\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3675 - accuracy: 0.8596\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3497 - accuracy: 0.8649\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3310 - accuracy: 0.8731\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3296 - accuracy: 0.8750\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3098 - accuracy: 0.8821\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3033 - accuracy: 0.8844\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2999 - accuracy: 0.8864\n",
      "Epoch 22/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2878 - accuracy: 0.8899\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2908 - accuracy: 0.8897\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2664 - accuracy: 0.8985\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2619 - accuracy: 0.8990\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2576 - accuracy: 0.9016\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2637 - accuracy: 0.9020\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2506 - accuracy: 0.9070\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2428 - accuracy: 0.9085\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2442 - accuracy: 0.9085\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2387 - accuracy: 0.9107\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2255 - accuracy: 0.9134\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2336 - accuracy: 0.9125\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2260 - accuracy: 0.9153\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2256 - accuracy: 0.9170\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2201 - accuracy: 0.9190\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2170 - accuracy: 0.9189\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2049 - accuracy: 0.9230\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2224 - accuracy: 0.9197\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2065 - accuracy: 0.9232\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2073 - accuracy: 0.9239\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2022 - accuracy: 0.9255\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.1983 - accuracy: 0.9264\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.1966 - accuracy: 0.9273\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.1969 - accuracy: 0.9281\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.1922 - accuracy: 0.9291\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.1970 - accuracy: 0.9280\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.1850 - accuracy: 0.9317\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.1889 - accuracy: 0.9293\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.1897 - accuracy: 0.9307\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.1820 - accuracy: 0.9323\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.1983 - accuracy: 0.9282\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.1819 - accuracy: 0.9315\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.1767 - accuracy: 0.9340\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.1992 - accuracy: 0.9286\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2065 - accuracy: 0.9273\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 1.4830 - accuracy: 0.6311\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.5895 - accuracy: 0.7799\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4611 - accuracy: 0.8143\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3995 - accuracy: 0.8384\n",
      "152/152 [==============================] - 2s 11ms/step - loss: 0.4234 - accuracy: 0.8591\n",
      "n_conv_1 = 512, k_conv_1 = 3,  n_conv_2 = 512, k_conv_2 = 7, \n",
      "          n_conv_3 = 768,  k_conv_3 = 7,  maxpooling_pool_size = 4,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 768, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000016C6B9868C8>\n",
      "Epoch 1/60\n",
      "304/304 [==============================] - 12s 38ms/step - loss: 1.6164 - accuracy: 0.4749\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 11s 36ms/step - loss: 0.9233 - accuracy: 0.6085\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.7884 - accuracy: 0.6748\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.7213 - accuracy: 0.7122\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.6258 - accuracy: 0.7482\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.5691 - accuracy: 0.7693\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.5440 - accuracy: 0.7827\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.5120 - accuracy: 0.7949\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4822 - accuracy: 0.8070\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4737 - accuracy: 0.8136\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4448 - accuracy: 0.8238\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4303 - accuracy: 0.8298\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4100 - accuracy: 0.8392\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4034 - accuracy: 0.8428\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4070 - accuracy: 0.8429\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3852 - accuracy: 0.8499\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3877 - accuracy: 0.8512\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3526 - accuracy: 0.8620\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3551 - accuracy: 0.8607\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 11s 36ms/step - loss: 0.3367 - accuracy: 0.8684\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3392 - accuracy: 0.8674\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3303 - accuracy: 0.8713\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3349 - accuracy: 0.8701\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3207 - accuracy: 0.8758\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3092 - accuracy: 0.8769\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3110 - accuracy: 0.8783\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3309 - accuracy: 0.8749\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3020 - accuracy: 0.8831\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2923 - accuracy: 0.8843\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2895 - accuracy: 0.8891\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2925 - accuracy: 0.8869\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2726 - accuracy: 0.8935\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2764 - accuracy: 0.8928\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2828 - accuracy: 0.8925\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2970 - accuracy: 0.8874\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 11s 36ms/step - loss: 0.2620 - accuracy: 0.8982\n",
      "Epoch 37/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2615 - accuracy: 0.8997\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2550 - accuracy: 0.9020\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2697 - accuracy: 0.8974\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2531 - accuracy: 0.9029\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2610 - accuracy: 0.9031\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2554 - accuracy: 0.9041\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2483 - accuracy: 0.9056\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2551 - accuracy: 0.9048\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2664 - accuracy: 0.9002\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2430 - accuracy: 0.9076\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2502 - accuracy: 0.9069\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2435 - accuracy: 0.9076\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2435 - accuracy: 0.9078\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2395 - accuracy: 0.9105\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2339 - accuracy: 0.9106\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2408 - accuracy: 0.9104\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2372 - accuracy: 0.9102\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2274 - accuracy: 0.9146\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2560 - accuracy: 0.9066\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2427 - accuracy: 0.9092\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2267 - accuracy: 0.9150\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2213 - accuracy: 0.9155\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 11s 36ms/step - loss: 0.2222 - accuracy: 0.9184\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2217 - accuracy: 0.9161\n",
      "152/152 [==============================] - 2s 10ms/step - loss: 0.4601 - accuracy: 0.8760\n",
      "n_conv_1 = 512, k_conv_1 = 3,  n_conv_2 = 512, k_conv_2 = 7, \n",
      "          n_conv_3 = 768,  k_conv_3 = 7,  maxpooling_pool_size = 4,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 768, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000016C6BB89C48>\n",
      "Epoch 1/60\n",
      "  1/304 [..............................] - ETA: 8s - loss: 1.7931 - accuracy: 0.1562WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0130s vs `on_train_batch_end` time: 0.0269s). Check your callbacks.\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 1.1784 - accuracy: 0.5026\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.8538 - accuracy: 0.6545\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.6752 - accuracy: 0.7306\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.6101 - accuracy: 0.7586\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.5435 - accuracy: 0.7822\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4982 - accuracy: 0.7984\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4779 - accuracy: 0.8077\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4531 - accuracy: 0.8198\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4329 - accuracy: 0.8265\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4934 - accuracy: 0.8125\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3898 - accuracy: 0.8461\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3765 - accuracy: 0.8517\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3718 - accuracy: 0.8545\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3652 - accuracy: 0.8567\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3480 - accuracy: 0.8627\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3370 - accuracy: 0.8660\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 4.6493 - accuracy: 0.6669\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.5678 - accuracy: 0.7810\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4681 - accuracy: 0.8188\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4150 - accuracy: 0.8399\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3810 - accuracy: 0.8548\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3589 - accuracy: 0.8620\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3407 - accuracy: 0.8661\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3283 - accuracy: 0.8720\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3177 - accuracy: 0.8740\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3091 - accuracy: 0.8791\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3005 - accuracy: 0.8808\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3029 - accuracy: 0.8806\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2972 - accuracy: 0.8817\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2905 - accuracy: 0.8850\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2956 - accuracy: 0.8829\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2864 - accuracy: 0.8868\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2823 - accuracy: 0.8881\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2907 - accuracy: 0.8854\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2855 - accuracy: 0.8872\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2939 - accuracy: 0.8846\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2874 - accuracy: 0.8876\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2798 - accuracy: 0.8905\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2807 - accuracy: 0.8914\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2846 - accuracy: 0.8910\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2695 - accuracy: 0.8940\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2769 - accuracy: 0.8931\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3030 - accuracy: 0.8868\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2595 - accuracy: 0.8988\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2567 - accuracy: 0.9021\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3122 - accuracy: 0.8883\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2588 - accuracy: 0.9005\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2514 - accuracy: 0.9031\n",
      "Epoch 49/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2450 - accuracy: 0.9055\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2506 - accuracy: 0.9035\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2419 - accuracy: 0.9081\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2429 - accuracy: 0.9080\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2671 - accuracy: 0.9020\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2403 - accuracy: 0.9079\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2408 - accuracy: 0.9083\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2357 - accuracy: 0.9116\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2670 - accuracy: 0.9014\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2552 - accuracy: 0.9064\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2318 - accuracy: 0.9124\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2333 - accuracy: 0.9136\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.4549 - accuracy: 0.8895\n",
      "n_conv_1 = 512, k_conv_1 = 3,  n_conv_2 = 512, k_conv_2 = 7, \n",
      "          n_conv_3 = 768,  k_conv_3 = 7,  maxpooling_pool_size = 4,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 768, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000016B678B7A88>\n",
      "Epoch 1/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 1.1591 - accuracy: 0.5076\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.9146 - accuracy: 0.6405\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.6781 - accuracy: 0.7299\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.5927 - accuracy: 0.7625\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.5771 - accuracy: 0.7735\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.5032 - accuracy: 0.7983\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4768 - accuracy: 0.8071\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 13.1741 - accuracy: 0.6554\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.8166 - accuracy: 0.6714\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.6768 - accuracy: 0.7306\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.6096 - accuracy: 0.7582\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.5683 - accuracy: 0.7773\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.5329 - accuracy: 0.7929\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.5032 - accuracy: 0.8044\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4842 - accuracy: 0.8122\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4733 - accuracy: 0.8151\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4563 - accuracy: 0.8244\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4411 - accuracy: 0.8271\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4253 - accuracy: 0.8342\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4158 - accuracy: 0.8383\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4039 - accuracy: 0.8420\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.4023 - accuracy: 0.8434\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3895 - accuracy: 0.8472\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3832 - accuracy: 0.8513\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3866 - accuracy: 0.8497\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3816 - accuracy: 0.8480\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3615 - accuracy: 0.8578\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3615 - accuracy: 0.8584\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3538 - accuracy: 0.8619\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3535 - accuracy: 0.8619\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3507 - accuracy: 0.8633\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3447 - accuracy: 0.8652\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3384 - accuracy: 0.8675\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3480 - accuracy: 0.8649\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3313 - accuracy: 0.8699\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3305 - accuracy: 0.8709\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3353 - accuracy: 0.8715\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3204 - accuracy: 0.8762\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3201 - accuracy: 0.8741\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3248 - accuracy: 0.8734\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3114 - accuracy: 0.8792\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2998 - accuracy: 0.8835\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3023 - accuracy: 0.8823\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.3163 - accuracy: 0.8800\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2891 - accuracy: 0.8880\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2998 - accuracy: 0.8866\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2835 - accuracy: 0.8910\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2890 - accuracy: 0.8894\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2860 - accuracy: 0.8908\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2858 - accuracy: 0.8914\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2752 - accuracy: 0.8945\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2630 - accuracy: 0.8984\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2697 - accuracy: 0.8962\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2693 - accuracy: 0.8978\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2716 - accuracy: 0.8977\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2580 - accuracy: 0.9023\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2558 - accuracy: 0.9045\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2607 - accuracy: 0.9023\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2618 - accuracy: 0.9026\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 11s 37ms/step - loss: 0.2627 - accuracy: 0.9028\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.4082 - accuracy: 0.8757\n",
      "n_conv_1 = 768, k_conv_1 = 7,  n_conv_2 = 768, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 7,  maxpooling_pool_size = 4,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 768, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  elu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/304 [..............................] - ETA: 4s - loss: 1.7921 - accuracy: 0.1797WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0140s vs `on_train_batch_end` time: 0.0290s). Check your callbacks.\n",
      "304/304 [==============================] - 13s 44ms/step - loss: 1.3232 - accuracy: 0.4530\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 1.5983 - accuracy: 0.5208\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.8769 - accuracy: 0.6466\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.7540 - accuracy: 0.7031\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.6509 - accuracy: 0.7460\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.5906 - accuracy: 0.7708\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.5435 - accuracy: 0.7898\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.5052 - accuracy: 0.8017\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.4720 - accuracy: 0.8161\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.4618 - accuracy: 0.8214\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.4374 - accuracy: 0.8286\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.4307 - accuracy: 0.8331\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.4058 - accuracy: 0.8436\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3929 - accuracy: 0.8462\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3884 - accuracy: 0.8497\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3650 - accuracy: 0.8567\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3603 - accuracy: 0.8614\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3554 - accuracy: 0.8624\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3496 - accuracy: 0.8624\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3379 - accuracy: 0.8680\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3198 - accuracy: 0.8748\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3154 - accuracy: 0.8747\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3128 - accuracy: 0.8769\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3049 - accuracy: 0.8802\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2970 - accuracy: 0.8824\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2879 - accuracy: 0.8854\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2816 - accuracy: 0.8888\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2787 - accuracy: 0.8913\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2728 - accuracy: 0.8950\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2711 - accuracy: 0.8951\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2531 - accuracy: 0.9009\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2531 - accuracy: 0.9019\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2542 - accuracy: 0.9029\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2485 - accuracy: 0.9037\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2432 - accuracy: 0.9062\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2407 - accuracy: 0.9085\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2330 - accuracy: 0.9098\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2365 - accuracy: 0.9105\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2328 - accuracy: 0.9113\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2301 - accuracy: 0.9108\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2303 - accuracy: 0.9128\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2285 - accuracy: 0.9132\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2392 - accuracy: 0.9121\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2199 - accuracy: 0.9200\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2088 - accuracy: 0.9202\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2140 - accuracy: 0.9197\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2065 - accuracy: 0.9203\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2137 - accuracy: 0.9203\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 4.2552 - accuracy: 0.8240\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.4130 - accuracy: 0.8636\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3061 - accuracy: 0.8934\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2544 - accuracy: 0.9071\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2370 - accuracy: 0.9130\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2271 - accuracy: 0.9165\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2150 - accuracy: 0.9193\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2065 - accuracy: 0.9233\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2104 - accuracy: 0.9226\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2125 - accuracy: 0.9226\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2048 - accuracy: 0.9226\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2004 - accuracy: 0.9247\n",
      "152/152 [==============================] - 2s 12ms/step - loss: 0.3982 - accuracy: 0.9136\n",
      "n_conv_1 = 768, k_conv_1 = 7,  n_conv_2 = 768, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 7,  maxpooling_pool_size = 4,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 768, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  elu\n",
      "Epoch 1/60\n",
      "  1/304 [..............................] - ETA: 8s - loss: 1.7834 - accuracy: 0.2266WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0140s vs `on_train_batch_end` time: 0.0299s). Check your callbacks.\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 1.9410 - accuracy: 0.4411\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 1.0232 - accuracy: 0.5737\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.8759 - accuracy: 0.6449\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.7423 - accuracy: 0.7092\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.6534 - accuracy: 0.7447\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.5935 - accuracy: 0.7697\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.5419 - accuracy: 0.7888\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.5143 - accuracy: 0.7994\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.4765 - accuracy: 0.8146\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.4469 - accuracy: 0.8233\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.4455 - accuracy: 0.8268\n",
      "Epoch 12/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 13s 43ms/step - loss: 0.4514 - accuracy: 0.82530s - loss: 0.454\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.4067 - accuracy: 0.8399\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3863 - accuracy: 0.8482\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3796 - accuracy: 0.8533\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3683 - accuracy: 0.8553\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3597 - accuracy: 0.8585\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3441 - accuracy: 0.8660\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3418 - accuracy: 0.8664\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3402 - accuracy: 0.8645\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3316 - accuracy: 0.8725\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3280 - accuracy: 0.8728\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3099 - accuracy: 0.8796\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2981 - accuracy: 0.8829\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2890 - accuracy: 0.8863\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2867 - accuracy: 0.8869\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2708 - accuracy: 0.8931\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2681 - accuracy: 0.8963\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2572 - accuracy: 0.9008\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2710 - accuracy: 0.8959\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2533 - accuracy: 0.9007\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2408 - accuracy: 0.9058\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2371 - accuracy: 0.9076\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2516 - accuracy: 0.9030\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2480 - accuracy: 0.9067\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2306 - accuracy: 0.9117\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2338 - accuracy: 0.9106\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2519 - accuracy: 0.9089\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2282 - accuracy: 0.9148\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2195 - accuracy: 0.9164\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2125 - accuracy: 0.9196\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2216 - accuracy: 0.9170\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2183 - accuracy: 0.9184\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2130 - accuracy: 0.9195\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2114 - accuracy: 0.9219\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2003 - accuracy: 0.9255\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2023 - accuracy: 0.9254\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2105 - accuracy: 0.9218\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.1919 - accuracy: 0.9288\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2096 - accuracy: 0.9241\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2101 - accuracy: 0.9232\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2005 - accuracy: 0.9255\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2026 - accuracy: 0.9260\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.1948 - accuracy: 0.9284\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.1842 - accuracy: 0.9326\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.1843 - accuracy: 0.9315\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.1943 - accuracy: 0.9295\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.1835 - accuracy: 0.9328\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.1782 - accuracy: 0.9339\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.1966 - accuracy: 0.9281\n",
      "152/152 [==============================] - 2s 12ms/step - loss: 0.4497 - accuracy: 0.9085\n",
      "n_conv_1 = 768, k_conv_1 = 7,  n_conv_2 = 768, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 7,  maxpooling_pool_size = 4,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 768, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  elu\n",
      "Epoch 1/60\n",
      "  1/304 [..............................] - ETA: 8s - loss: 1.8076 - accuracy: 0.1328WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0130s vs `on_train_batch_end` time: 0.0289s). Check your callbacks.\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 1.8287 - accuracy: 0.4431\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 1.0713 - accuracy: 0.5517\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.9391 - accuracy: 0.6170\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.8024 - accuracy: 0.6777\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.6952 - accuracy: 0.7244\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.6239 - accuracy: 0.7544\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.5675 - accuracy: 0.7787\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.5177 - accuracy: 0.7993\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.4917 - accuracy: 0.8076\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.4534 - accuracy: 0.8222\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.4289 - accuracy: 0.8307\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.4296 - accuracy: 0.8343\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3992 - accuracy: 0.8422\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3898 - accuracy: 0.8498\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3787 - accuracy: 0.8531\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3606 - accuracy: 0.8602\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3520 - accuracy: 0.8627\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3392 - accuracy: 0.8674\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3344 - accuracy: 0.8706\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3234 - accuracy: 0.8723\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3115 - accuracy: 0.8769\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.3145 - accuracy: 0.8765\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2955 - accuracy: 0.8834\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2884 - accuracy: 0.8851\n",
      "Epoch 25/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2874 - accuracy: 0.8874\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2753 - accuracy: 0.8925\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2951 - accuracy: 0.8872\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2636 - accuracy: 0.8977\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2569 - accuracy: 0.9018\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2474 - accuracy: 0.9038\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2515 - accuracy: 0.9034\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2518 - accuracy: 0.9048\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2371 - accuracy: 0.9093\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2335 - accuracy: 0.9115\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2274 - accuracy: 0.9142\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2321 - accuracy: 0.9136\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2229 - accuracy: 0.9150\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2373 - accuracy: 0.9109\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2191 - accuracy: 0.9169\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2172 - accuracy: 0.9193\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2103 - accuracy: 0.9202\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2265 - accuracy: 0.9173\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2122 - accuracy: 0.9226\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.1992 - accuracy: 0.9273\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.1990 - accuracy: 0.9233\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2011 - accuracy: 0.9256\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.1955 - accuracy: 0.9265\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2018 - accuracy: 0.9257\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.1968 - accuracy: 0.9282\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.1989 - accuracy: 0.9253\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2103 - accuracy: 0.9236\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2269 - accuracy: 0.9205\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.1989 - accuracy: 0.9260\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.1739 - accuracy: 0.9354\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.1871 - accuracy: 0.9296\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.1816 - accuracy: 0.9334\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.2041 - accuracy: 0.9267\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.1856 - accuracy: 0.9317\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.1855 - accuracy: 0.9323\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 13s 43ms/step - loss: 0.1735 - accuracy: 0.9356\n",
      "152/152 [==============================] - 2s 12ms/step - loss: 0.4453 - accuracy: 0.9065\n",
      "n_conv_1 = 512, k_conv_1 = 7,  n_conv_2 = 512, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 512, dropout_1 = 0.3,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  elu\n",
      "Epoch 1/60\n",
      "  1/304 [..............................] - ETA: 4s - loss: 1.7824 - accuracy: 0.1719WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0130s vs `on_train_batch_end` time: 0.0200s). Check your callbacks.\n",
      "304/304 [==============================] - 10s 32ms/step - loss: 1.3226 - accuracy: 0.4441\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 1.0180 - accuracy: 0.5937\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.8229 - accuracy: 0.6764\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.7155 - accuracy: 0.7229\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.6546 - accuracy: 0.7513\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.6186 - accuracy: 0.7635\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.5730 - accuracy: 0.7785\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.5506 - accuracy: 0.7862\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.5224 - accuracy: 0.7923\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.4997 - accuracy: 0.7991\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.4820 - accuracy: 0.8027\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.5188 - accuracy: 0.7977\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.4600 - accuracy: 0.8125\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.4436 - accuracy: 0.8184\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.4288 - accuracy: 0.8255\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.4193 - accuracy: 0.8305\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.4089 - accuracy: 0.8330\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 10s 31ms/step - loss: 0.4067 - accuracy: 0.83540s - loss: 0.4061 - accuracy: 0.\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3825 - accuracy: 0.8442\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3850 - accuracy: 0.8431\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3678 - accuracy: 0.8512\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3580 - accuracy: 0.8566\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3479 - accuracy: 0.8596\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3370 - accuracy: 0.8653\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3296 - accuracy: 0.8662\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3274 - accuracy: 0.8702\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3212 - accuracy: 0.8727\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3038 - accuracy: 0.8774\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3014 - accuracy: 0.8803\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2915 - accuracy: 0.8865\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2863 - accuracy: 0.8869\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2805 - accuracy: 0.8893\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2753 - accuracy: 0.8919\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2654 - accuracy: 0.8957\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2649 - accuracy: 0.8953\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2681 - accuracy: 0.8939\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2553 - accuracy: 0.8997\n",
      "Epoch 38/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2465 - accuracy: 0.9011\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2402 - accuracy: 0.9034\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2414 - accuracy: 0.9038\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2398 - accuracy: 0.9053\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2313 - accuracy: 0.9057\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2323 - accuracy: 0.9078\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2256 - accuracy: 0.9089\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2361 - accuracy: 0.9072\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2182 - accuracy: 0.9111\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2183 - accuracy: 0.9131\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2207 - accuracy: 0.9127\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2127 - accuracy: 0.9147\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2174 - accuracy: 0.9133\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2132 - accuracy: 0.9149\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2141 - accuracy: 0.9158\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2057 - accuracy: 0.9172\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.1964 - accuracy: 0.9218\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2027 - accuracy: 0.9198\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2069 - accuracy: 0.9187\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2026 - accuracy: 0.9196\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.1927 - accuracy: 0.9223\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.1990 - accuracy: 0.9220\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.1955 - accuracy: 0.9233\n",
      "152/152 [==============================] - 2s 11ms/step - loss: 0.3210 - accuracy: 0.9039\n",
      "n_conv_1 = 512, k_conv_1 = 7,  n_conv_2 = 512, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 512, dropout_1 = 0.3,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  elu\n",
      "Epoch 1/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 1.3124 - accuracy: 0.4497\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 1.0449 - accuracy: 0.5829\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.8560 - accuracy: 0.6618\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.7396 - accuracy: 0.7118\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.7066 - accuracy: 0.7278\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.6110 - accuracy: 0.7629\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.8186 - accuracy: 0.7027\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.5849 - accuracy: 0.7700\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.5284 - accuracy: 0.7902\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.5002 - accuracy: 0.8030\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.4780 - accuracy: 0.8111\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.4639 - accuracy: 0.8173\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.4418 - accuracy: 0.8263\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.4250 - accuracy: 0.8318\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.4130 - accuracy: 0.8366\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3954 - accuracy: 0.8429\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3857 - accuracy: 0.8498\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3691 - accuracy: 0.8546\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3843 - accuracy: 0.8506\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3552 - accuracy: 0.8613 0s - loss: 0.3555 - accuracy\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 10s 31ms/step - loss: 0.3458 - accuracy: 0.8643\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3456 - accuracy: 0.8667\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3253 - accuracy: 0.8729\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3239 - accuracy: 0.8745\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3150 - accuracy: 0.8753\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3223 - accuracy: 0.8749\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3028 - accuracy: 0.8794\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3009 - accuracy: 0.8806\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2952 - accuracy: 0.8817\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2859 - accuracy: 0.8865\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2861 - accuracy: 0.8864\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2824 - accuracy: 0.8874\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2888 - accuracy: 0.8867\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2810 - accuracy: 0.8895\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2620 - accuracy: 0.8942\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2593 - accuracy: 0.8967\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2522 - accuracy: 0.9003\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2505 - accuracy: 0.9021\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2487 - accuracy: 0.9014\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2410 - accuracy: 0.9054\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2342 - accuracy: 0.9072\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2369 - accuracy: 0.9054\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2349 - accuracy: 0.9098\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2304 - accuracy: 0.9091 0s - loss: 0\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2198 - accuracy: 0.9128\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2257 - accuracy: 0.9113\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2208 - accuracy: 0.9130\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2191 - accuracy: 0.9130\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2153 - accuracy: 0.9168\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2177 - accuracy: 0.9149\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2133 - accuracy: 0.9156\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 10s 31ms/step - loss: 0.2073 - accuracy: 0.9190\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2187 - accuracy: 0.9151\n",
      "Epoch 54/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 9s 31ms/step - loss: 0.1979 - accuracy: 0.9215\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2059 - accuracy: 0.9185\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2068 - accuracy: 0.9173\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2006 - accuracy: 0.9218\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.1970 - accuracy: 0.9229\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.1992 - accuracy: 0.9206\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.1994 - accuracy: 0.9224\n",
      "152/152 [==============================] - 2s 11ms/step - loss: 0.2827 - accuracy: 0.9197\n",
      "n_conv_1 = 512, k_conv_1 = 7,  n_conv_2 = 512, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 512, dropout_1 = 0.3,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  elu\n",
      "Epoch 1/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 1.3099 - accuracy: 0.4460\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 1.0174 - accuracy: 0.5899\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.8395 - accuracy: 0.6686\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.7766 - accuracy: 0.7029\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.6777 - accuracy: 0.7393\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.6308 - accuracy: 0.7568\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.5908 - accuracy: 0.7726\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.5678 - accuracy: 0.7797\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.5362 - accuracy: 0.7899\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.5107 - accuracy: 0.7982\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.4936 - accuracy: 0.8022\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 10s 31ms/step - loss: 0.4716 - accuracy: 0.8105\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.4758 - accuracy: 0.8093\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.4500 - accuracy: 0.8196\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.4440 - accuracy: 0.8228\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.4282 - accuracy: 0.8294\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.4049 - accuracy: 0.8364\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3986 - accuracy: 0.8425\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3837 - accuracy: 0.8472\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3763 - accuracy: 0.8499\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3591 - accuracy: 0.8584\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3590 - accuracy: 0.8579\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 10s 31ms/step - loss: 0.3464 - accuracy: 0.8622\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3290 - accuracy: 0.8711\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3200 - accuracy: 0.8727\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3172 - accuracy: 0.8742\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3061 - accuracy: 0.8761\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.3025 - accuracy: 0.8793\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 10s 31ms/step - loss: 0.2936 - accuracy: 0.8850\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2855 - accuracy: 0.8874\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2735 - accuracy: 0.8930\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2718 - accuracy: 0.8933\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2649 - accuracy: 0.8963\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2548 - accuracy: 0.8977\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2540 - accuracy: 0.8993\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2477 - accuracy: 0.9011\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2451 - accuracy: 0.9028\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2363 - accuracy: 0.9046\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2448 - accuracy: 0.9037\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2382 - accuracy: 0.9043\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2199 - accuracy: 0.9103\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2274 - accuracy: 0.9098\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2326 - accuracy: 0.9067\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 10s 31ms/step - loss: 0.2248 - accuracy: 0.9135\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2161 - accuracy: 0.9138\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2241 - accuracy: 0.9125\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2173 - accuracy: 0.9137\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2082 - accuracy: 0.9149\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2112 - accuracy: 0.9173\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2107 - accuracy: 0.9167\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2123 - accuracy: 0.9172\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.1988 - accuracy: 0.9222\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2032 - accuracy: 0.9183\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2006 - accuracy: 0.9187\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 10s 31ms/step - loss: 0.1966 - accuracy: 0.9205\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.2038 - accuracy: 0.9207\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.1929 - accuracy: 0.9236\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.1978 - accuracy: 0.9216\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.1947 - accuracy: 0.9241\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 9s 31ms/step - loss: 0.1905 - accuracy: 0.9244\n",
      "152/152 [==============================] - 2s 11ms/step - loss: 0.3170 - accuracy: 0.9115\n",
      "n_conv_1 = 512, k_conv_1 = 7,  n_conv_2 = 768, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 7,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 4,  n_dense_1 = 768, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000016C6AA2C0C8>\n",
      "Epoch 1/60\n",
      "  1/304 [..............................] - ETA: 3s - loss: 1.7945 - accuracy: 0.1406WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0150s vs `on_train_batch_end` time: 0.0306s). Check your callbacks.\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 1.1189 - accuracy: 0.5387\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.7954 - accuracy: 0.6929\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.6132 - accuracy: 0.7561\n",
      "Epoch 4/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 14s 45ms/step - loss: 0.5587 - accuracy: 0.7757\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.5026 - accuracy: 0.7919\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4960 - accuracy: 0.7988\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 9.8635 - accuracy: 0.5670\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.6840 - accuracy: 0.7321\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.5770 - accuracy: 0.7752\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.5348 - accuracy: 0.7937\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4990 - accuracy: 0.8093\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4774 - accuracy: 0.8174\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4527 - accuracy: 0.8256\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4385 - accuracy: 0.8313\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4321 - accuracy: 0.8358\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4164 - accuracy: 0.8415\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4237 - accuracy: 0.8392\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3972 - accuracy: 0.8472\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3916 - accuracy: 0.8500\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3803 - accuracy: 0.8519\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3854 - accuracy: 0.8524\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3733 - accuracy: 0.8559\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3628 - accuracy: 0.8591\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3624 - accuracy: 0.8597\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3559 - accuracy: 0.8618\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3511 - accuracy: 0.8649\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3518 - accuracy: 0.8630\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3461 - accuracy: 0.8641\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3370 - accuracy: 0.8681\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3406 - accuracy: 0.8660\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3353 - accuracy: 0.8690\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3307 - accuracy: 0.8672\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3229 - accuracy: 0.8736\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3252 - accuracy: 0.8722\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3215 - accuracy: 0.8725\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3154 - accuracy: 0.8751\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3288 - accuracy: 0.8726\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3072 - accuracy: 0.8786\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3039 - accuracy: 0.8792\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3077 - accuracy: 0.8797\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3036 - accuracy: 0.8779\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2937 - accuracy: 0.8832\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2942 - accuracy: 0.8809\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2948 - accuracy: 0.8842\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2977 - accuracy: 0.8836\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2901 - accuracy: 0.8826\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3010 - accuracy: 0.8826\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2813 - accuracy: 0.8875\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2847 - accuracy: 0.8868\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2709 - accuracy: 0.8907\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2652 - accuracy: 0.8938\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2721 - accuracy: 0.8915\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2735 - accuracy: 0.8917\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2735 - accuracy: 0.8914\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2589 - accuracy: 0.8956\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2644 - accuracy: 0.8956\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2637 - accuracy: 0.8949\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2600 - accuracy: 0.8960\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2755 - accuracy: 0.8912\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2525 - accuracy: 0.8973\n",
      "152/152 [==============================] - 2s 13ms/step - loss: 0.3858 - accuracy: 0.8784\n",
      "n_conv_1 = 512, k_conv_1 = 7,  n_conv_2 = 768, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 7,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 4,  n_dense_1 = 768, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000016B518F74C8>\n",
      "Epoch 1/60\n",
      "  1/304 [..............................] - ETA: 9s - loss: 1.7922 - accuracy: 0.2109WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0149s vs `on_train_batch_end` time: 0.0313s). Check your callbacks.\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 1.1204 - accuracy: 0.5326\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.7573 - accuracy: 0.7010\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.7863 - accuracy: 0.7112\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.5595 - accuracy: 0.7757\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.5161 - accuracy: 0.7921\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4787 - accuracy: 0.8041\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4744 - accuracy: 0.8101\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4350 - accuracy: 0.8231\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4368 - accuracy: 0.8238\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4203 - accuracy: 0.8324\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 11.3888 - accuracy: 0.6825\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.7895 - accuracy: 0.6920\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.6533 - accuracy: 0.7467\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.5755 - accuracy: 0.7795\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.5237 - accuracy: 0.7974\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4958 - accuracy: 0.8075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4716 - accuracy: 0.8173\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4506 - accuracy: 0.8266\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4217 - accuracy: 0.8370\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4204 - accuracy: 0.8389\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4020 - accuracy: 0.8462\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4055 - accuracy: 0.8440\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3897 - accuracy: 0.8492\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3757 - accuracy: 0.8528\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3715 - accuracy: 0.8543\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3692 - accuracy: 0.8573\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3584 - accuracy: 0.86091s - loss:\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3552 - accuracy: 0.8602\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3499 - accuracy: 0.8636\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3491 - accuracy: 0.8643\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3394 - accuracy: 0.8666\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3421 - accuracy: 0.8663\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3416 - accuracy: 0.8654\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3340 - accuracy: 0.8669\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3307 - accuracy: 0.8693\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3250 - accuracy: 0.8726\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3205 - accuracy: 0.8726\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3265 - accuracy: 0.8704\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3202 - accuracy: 0.8737\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3155 - accuracy: 0.8740\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3246 - accuracy: 0.8702\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3135 - accuracy: 0.8746\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3022 - accuracy: 0.8785\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3022 - accuracy: 0.8794\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3056 - accuracy: 0.8786\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2994 - accuracy: 0.8802\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2934 - accuracy: 0.8825\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3049 - accuracy: 0.8811\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2987 - accuracy: 0.8824\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2784 - accuracy: 0.8887\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2949 - accuracy: 0.8842\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2735 - accuracy: 0.8903\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2820 - accuracy: 0.8867\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2793 - accuracy: 0.8878\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2742 - accuracy: 0.8885\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2726 - accuracy: 0.8916\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2785 - accuracy: 0.8896\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2619 - accuracy: 0.8961\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2677 - accuracy: 0.8934\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2598 - accuracy: 0.8970\n",
      "152/152 [==============================] - ETA: 0s - loss: 0.4054 - accuracy: 0.88 - 2s 12ms/step - loss: 0.4044 - accuracy: 0.8810\n",
      "n_conv_1 = 512, k_conv_1 = 7,  n_conv_2 = 768, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 7,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 4,  n_dense_1 = 768, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000016C6AEE0F08>\n",
      "Epoch 1/60\n",
      "  1/304 [..............................] - ETA: 8s - loss: 1.7934 - accuracy: 0.1328WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0150s vs `on_train_batch_end` time: 0.0289s). Check your callbacks.\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 1.1299 - accuracy: 0.52751s\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.8314 - accuracy: 0.6684\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.6416 - accuracy: 0.7431\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.5703 - accuracy: 0.7731\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 6.2296 - accuracy: 0.6603\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.7353 - accuracy: 0.7136\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.6165 - accuracy: 0.7576\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.5619 - accuracy: 0.7801\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.5310 - accuracy: 0.7933\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.5017 - accuracy: 0.8035\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4857 - accuracy: 0.8091\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4631 - accuracy: 0.8196\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4533 - accuracy: 0.8234\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4460 - accuracy: 0.8257\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4294 - accuracy: 0.8295\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4159 - accuracy: 0.8364\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4224 - accuracy: 0.8367\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3979 - accuracy: 0.8432\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3981 - accuracy: 0.8428\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3893 - accuracy: 0.8476\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3799 - accuracy: 0.8507\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3815 - accuracy: 0.8526\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3718 - accuracy: 0.8542\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3613 - accuracy: 0.8586\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3699 - accuracy: 0.8571\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3577 - accuracy: 0.8587\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3517 - accuracy: 0.8613\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3554 - accuracy: 0.8616\n",
      "Epoch 29/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3539 - accuracy: 0.8622\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3378 - accuracy: 0.8657\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3338 - accuracy: 0.8694\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3399 - accuracy: 0.8676\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3298 - accuracy: 0.8697\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3144 - accuracy: 0.8743\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3241 - accuracy: 0.8708\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3075 - accuracy: 0.8758\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3152 - accuracy: 0.8764\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3072 - accuracy: 0.8780\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3359 - accuracy: 0.8726\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2977 - accuracy: 0.8808\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2917 - accuracy: 0.8840\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2934 - accuracy: 0.8825\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2942 - accuracy: 0.8843\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2883 - accuracy: 0.8868\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2876 - accuracy: 0.8854\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2886 - accuracy: 0.8854\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2781 - accuracy: 0.8895\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2792 - accuracy: 0.8888\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2706 - accuracy: 0.8931\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2708 - accuracy: 0.8924\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2744 - accuracy: 0.8898\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2701 - accuracy: 0.8931\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2906 - accuracy: 0.8872\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2572 - accuracy: 0.8972\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2773 - accuracy: 0.8926\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2676 - accuracy: 0.8939\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2546 - accuracy: 0.8981\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2553 - accuracy: 0.8979\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2625 - accuracy: 0.8975\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2505 - accuracy: 0.9011\n",
      "152/152 [==============================] - 2s 13ms/step - loss: 0.4941 - accuracy: 0.8650\n",
      "n_conv_1 = 512, k_conv_1 = 7,  n_conv_2 = 512, k_conv_2 = 7, \n",
      "          n_conv_3 = 768,  k_conv_3 = 3,  maxpooling_pool_size = 4,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 768, dropout_1 = 0.3,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  relu\n",
      "Epoch 1/60\n",
      "  1/304 [..............................] - ETA: 3s - loss: 1.7925 - accuracy: 0.1562WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0120s vs `on_train_batch_end` time: 0.0190s). Check your callbacks.\n",
      "304/304 [==============================] - 9s 30ms/step - loss: 1.0553 - accuracy: 0.5548\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.6457 - accuracy: 0.7467\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.4797 - accuracy: 0.8126\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.4028 - accuracy: 0.8453\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.3540 - accuracy: 0.8648\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.3249 - accuracy: 0.8755\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.2966 - accuracy: 0.8853\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.2833 - accuracy: 0.8904\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.2569 - accuracy: 0.9030\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.2429 - accuracy: 0.9092\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.2251 - accuracy: 0.9153\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.2192 - accuracy: 0.9188\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.2054 - accuracy: 0.9232\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1935 - accuracy: 0.9282\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1834 - accuracy: 0.9305\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1754 - accuracy: 0.9325\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1736 - accuracy: 0.9354\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1636 - accuracy: 0.9379\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1629 - accuracy: 0.9394\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1536 - accuracy: 0.9416\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1416 - accuracy: 0.9445\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1432 - accuracy: 0.9440\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1400 - accuracy: 0.9446\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1379 - accuracy: 0.9474\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1396 - accuracy: 0.9463\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1331 - accuracy: 0.9490\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1230 - accuracy: 0.9520\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1234 - accuracy: 0.9537\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1286 - accuracy: 0.9511\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1230 - accuracy: 0.9518\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1142 - accuracy: 0.9561\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1118 - accuracy: 0.9574\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1173 - accuracy: 0.9550\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1120 - accuracy: 0.9570\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1112 - accuracy: 0.9562\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1106 - accuracy: 0.9568\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1048 - accuracy: 0.9599\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1129 - accuracy: 0.9562\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1064 - accuracy: 0.9590\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1019 - accuracy: 0.9604\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1056 - accuracy: 0.9595\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1015 - accuracy: 0.9610\n",
      "Epoch 43/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1034 - accuracy: 0.9613\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0991 - accuracy: 0.9622\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0973 - accuracy: 0.9626\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1018 - accuracy: 0.9608\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0963 - accuracy: 0.9628\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0919 - accuracy: 0.9647\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0930 - accuracy: 0.9643\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0905 - accuracy: 0.9638\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0943 - accuracy: 0.9639\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0879 - accuracy: 0.9661\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0915 - accuracy: 0.9647\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0864 - accuracy: 0.9662\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0909 - accuracy: 0.9650\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0907 - accuracy: 0.9660\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0940 - accuracy: 0.9651\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0812 - accuracy: 0.9692\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0860 - accuracy: 0.9671\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0893 - accuracy: 0.9661\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.3439 - accuracy: 0.9347\n",
      "n_conv_1 = 512, k_conv_1 = 7,  n_conv_2 = 512, k_conv_2 = 7, \n",
      "          n_conv_3 = 768,  k_conv_3 = 3,  maxpooling_pool_size = 4,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 768, dropout_1 = 0.3,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  relu\n",
      "Epoch 1/60\n",
      "  1/304 [..............................] - ETA: 6s - loss: 1.7933 - accuracy: 0.1250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0120s vs `on_train_batch_end` time: 0.0219s). Check your callbacks.\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 1.0455 - accuracy: 0.5661\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.6424 - accuracy: 0.7485\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.4601 - accuracy: 0.8195\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.3933 - accuracy: 0.8495\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.3511 - accuracy: 0.8662\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.3151 - accuracy: 0.8802\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.2876 - accuracy: 0.8880\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.2794 - accuracy: 0.8910\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.2610 - accuracy: 0.8979\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.2455 - accuracy: 0.9050\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.2369 - accuracy: 0.9098\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.2230 - accuracy: 0.9133\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.2066 - accuracy: 0.9204\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.2044 - accuracy: 0.9220\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1912 - accuracy: 0.9267\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1797 - accuracy: 0.9299\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1800 - accuracy: 0.9325\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1670 - accuracy: 0.9360\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1643 - accuracy: 0.9377\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1557 - accuracy: 0.9407\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1524 - accuracy: 0.9416\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1451 - accuracy: 0.9455\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1417 - accuracy: 0.9458\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1410 - accuracy: 0.9454\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1387 - accuracy: 0.9470\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1364 - accuracy: 0.9488\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1309 - accuracy: 0.9508\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1277 - accuracy: 0.9502\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1223 - accuracy: 0.9541\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1262 - accuracy: 0.9519\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1219 - accuracy: 0.9534\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1195 - accuracy: 0.9533\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1213 - accuracy: 0.9539\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1147 - accuracy: 0.9541\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1275 - accuracy: 0.9536\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1135 - accuracy: 0.9563\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1081 - accuracy: 0.9574\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1090 - accuracy: 0.9566\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1108 - accuracy: 0.9578\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1050 - accuracy: 0.9592\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1035 - accuracy: 0.9600\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1088 - accuracy: 0.9582\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1086 - accuracy: 0.9591\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0987 - accuracy: 0.9612\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1033 - accuracy: 0.9607\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1031 - accuracy: 0.9606\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0997 - accuracy: 0.9615\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0971 - accuracy: 0.9624\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0957 - accuracy: 0.9621\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0949 - accuracy: 0.9642\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0956 - accuracy: 0.9635\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0970 - accuracy: 0.9632\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0903 - accuracy: 0.9650\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0885 - accuracy: 0.9646\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0997 - accuracy: 0.9623\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0914 - accuracy: 0.9654\n",
      "Epoch 57/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0981 - accuracy: 0.9625\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0861 - accuracy: 0.9676\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0961 - accuracy: 0.9627\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0917 - accuracy: 0.9649\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.3745 - accuracy: 0.9295\n",
      "n_conv_1 = 512, k_conv_1 = 7,  n_conv_2 = 512, k_conv_2 = 7, \n",
      "          n_conv_3 = 768,  k_conv_3 = 3,  maxpooling_pool_size = 4,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 768, dropout_1 = 0.3,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  relu\n",
      "Epoch 1/60\n",
      "  1/304 [..............................] - ETA: 5s - loss: 1.7935 - accuracy: 0.1172WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0110s vs `on_train_batch_end` time: 0.0189s). Check your callbacks.\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 1.0606 - accuracy: 0.5574\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.6903 - accuracy: 0.7272\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.4747 - accuracy: 0.8192\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.3932 - accuracy: 0.8507\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.3525 - accuracy: 0.8658\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.3280 - accuracy: 0.8781\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.3019 - accuracy: 0.8874\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.2811 - accuracy: 0.8944\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.2697 - accuracy: 0.9013\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.2474 - accuracy: 0.9067\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.2293 - accuracy: 0.9146\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.2247 - accuracy: 0.9161\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.2100 - accuracy: 0.9208\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1970 - accuracy: 0.9271\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1919 - accuracy: 0.9279\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1855 - accuracy: 0.9302\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1793 - accuracy: 0.9341\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1714 - accuracy: 0.9357\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1668 - accuracy: 0.9386\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1561 - accuracy: 0.9402\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1508 - accuracy: 0.9432\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1503 - accuracy: 0.9443\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1542 - accuracy: 0.9440\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1428 - accuracy: 0.9457\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1353 - accuracy: 0.9500\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1386 - accuracy: 0.9476\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1286 - accuracy: 0.9518\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1304 - accuracy: 0.9521\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1232 - accuracy: 0.9531\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1236 - accuracy: 0.9520\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1190 - accuracy: 0.9547\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1207 - accuracy: 0.9552\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1112 - accuracy: 0.9572\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1138 - accuracy: 0.9572\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1111 - accuracy: 0.9574\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1069 - accuracy: 0.9598\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1110 - accuracy: 0.9582\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1092 - accuracy: 0.9592\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1037 - accuracy: 0.9606\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1053 - accuracy: 0.9611\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1058 - accuracy: 0.9596\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1010 - accuracy: 0.9619\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0927 - accuracy: 0.9642\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0993 - accuracy: 0.9616\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1009 - accuracy: 0.9618\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1038 - accuracy: 0.9612\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.1015 - accuracy: 0.9624\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0964 - accuracy: 0.9647\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0955 - accuracy: 0.9638\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0888 - accuracy: 0.9664\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0886 - accuracy: 0.9659\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0909 - accuracy: 0.9666\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0893 - accuracy: 0.9663\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0944 - accuracy: 0.9658\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0902 - accuracy: 0.9662\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0926 - accuracy: 0.9653\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0835 - accuracy: 0.9689\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0888 - accuracy: 0.9674\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0874 - accuracy: 0.9677\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 9s 29ms/step - loss: 0.0862 - accuracy: 0.9686\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.3452 - accuracy: 0.9349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1812, in _create_c_op\n",
      "    c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Negative dimension size caused by subtracting 7 from 5 for '{{node conv1d_140/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv1d_140/conv1d/ExpandDims, conv1d_140/conv1d/ExpandDims_1)' with input shapes: [?,1,5,768], [1,7,768,512].\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\", line 223, in fit\n",
      "    return super(KerasClassifier, self).fit(x, y, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\", line 157, in fit\n",
      "    self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n",
      "  File \"<ipython-input-10-eac9e5d5b478>\", line 10, in create_model\n",
      "    model.add(Conv1D(n_conv_3, k_conv_3, activation=activation))\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\n",
      "    result = method(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\", line 221, in add\n",
      "    output_tensor = layer(self.outputs[0])\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 926, in __call__\n",
      "    input_list)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 1117, in _functional_construction_call\n",
      "    outputs = call_fn(cast_inputs, *args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\", line 247, in call\n",
      "    outputs = self._convolution_op(inputs, self.kernel)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1017, in convolution_v2\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1147, in convolution_internal\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 574, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 574, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1888, in conv1d\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 979, in conv2d\n",
      "    data_format=data_format, dilations=dilations, name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 744, in _apply_op_helper\n",
      "    attrs=attr_protos, op_def=op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 593, in _create_op_internal\n",
      "    compute_device)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3485, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1975, in __init__\n",
      "    control_input_ops, op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1815, in _create_c_op\n",
      "    raise ValueError(str(e))\n",
      "ValueError: Negative dimension size caused by subtracting 7 from 5 for '{{node conv1d_140/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv1d_140/conv1d/ExpandDims, conv1d_140/conv1d/ExpandDims_1)' with input shapes: [?,1,5,768], [1,7,768,512].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1812, in _create_c_op\n",
      "    c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Negative dimension size caused by subtracting 7 from 5 for '{{node conv1d_143/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv1d_143/conv1d/ExpandDims, conv1d_143/conv1d/ExpandDims_1)' with input shapes: [?,1,5,768], [1,7,768,512].\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\", line 223, in fit\n",
      "    return super(KerasClassifier, self).fit(x, y, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\", line 157, in fit\n",
      "    self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n",
      "  File \"<ipython-input-10-eac9e5d5b478>\", line 10, in create_model\n",
      "    model.add(Conv1D(n_conv_3, k_conv_3, activation=activation))\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\n",
      "    result = method(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\", line 221, in add\n",
      "    output_tensor = layer(self.outputs[0])\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 926, in __call__\n",
      "    input_list)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 1117, in _functional_construction_call\n",
      "    outputs = call_fn(cast_inputs, *args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\", line 247, in call\n",
      "    outputs = self._convolution_op(inputs, self.kernel)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1017, in convolution_v2\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1147, in convolution_internal\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 574, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 574, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1888, in conv1d\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 979, in conv2d\n",
      "    data_format=data_format, dilations=dilations, name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 744, in _apply_op_helper\n",
      "    attrs=attr_protos, op_def=op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 593, in _create_op_internal\n",
      "    compute_device)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3485, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1975, in __init__\n",
      "    control_input_ops, op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1815, in _create_c_op\n",
      "    raise ValueError(str(e))\n",
      "ValueError: Negative dimension size caused by subtracting 7 from 5 for '{{node conv1d_143/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv1d_143/conv1d/ExpandDims, conv1d_143/conv1d/ExpandDims_1)' with input shapes: [?,1,5,768], [1,7,768,512].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1812, in _create_c_op\n",
      "    c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Negative dimension size caused by subtracting 7 from 5 for '{{node conv1d_146/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv1d_146/conv1d/ExpandDims, conv1d_146/conv1d/ExpandDims_1)' with input shapes: [?,1,5,768], [1,7,768,512].\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\", line 223, in fit\n",
      "    return super(KerasClassifier, self).fit(x, y, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\", line 157, in fit\n",
      "    self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n",
      "  File \"<ipython-input-10-eac9e5d5b478>\", line 10, in create_model\n",
      "    model.add(Conv1D(n_conv_3, k_conv_3, activation=activation))\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\n",
      "    result = method(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\", line 221, in add\n",
      "    output_tensor = layer(self.outputs[0])\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 926, in __call__\n",
      "    input_list)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 1117, in _functional_construction_call\n",
      "    outputs = call_fn(cast_inputs, *args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\", line 247, in call\n",
      "    outputs = self._convolution_op(inputs, self.kernel)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1017, in convolution_v2\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1147, in convolution_internal\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 574, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 574, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1888, in conv1d\n",
      "    name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 979, in conv2d\n",
      "    data_format=data_format, dilations=dilations, name=name)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 744, in _apply_op_helper\n",
      "    attrs=attr_protos, op_def=op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 593, in _create_op_internal\n",
      "    compute_device)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3485, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1975, in __init__\n",
      "    control_input_ops, op_def)\n",
      "  File \"C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1815, in _create_c_op\n",
      "    raise ValueError(str(e))\n",
      "ValueError: Negative dimension size caused by subtracting 7 from 5 for '{{node conv1d_146/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv1d_146/conv1d/ExpandDims, conv1d_146/conv1d/ExpandDims_1)' with input shapes: [?,1,5,768], [1,7,768,512].\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_conv_1 = 512, k_conv_1 = 3,  n_conv_2 = 768, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 7,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 4,  n_dense_1 = 512, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000016B4BA21988>\n",
      "Epoch 1/60\n",
      "  1/304 [..............................] - ETA: 3s - loss: 1.7966 - accuracy: 0.1094WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0159s vs `on_train_batch_end` time: 0.0299s). Check your callbacks.\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 1.1401 - accuracy: 0.5192\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.9038 - accuracy: 0.6425\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.6302 - accuracy: 0.7445\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.5547 - accuracy: 0.7727\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.5206 - accuracy: 0.7865\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4856 - accuracy: 0.8007\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4572 - accuracy: 0.8127\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4479 - accuracy: 0.8170\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4153 - accuracy: 0.8325\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4384 - accuracy: 0.8276\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3822 - accuracy: 0.8475\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3774 - accuracy: 0.8508\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3607 - accuracy: 0.8567\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3504 - accuracy: 0.8603\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 1.0228 - accuracy: 0.7487\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3804 - accuracy: 0.8494\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3433 - accuracy: 0.8648\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3280 - accuracy: 0.8699\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3159 - accuracy: 0.8745\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3076 - accuracy: 0.8766\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3041 - accuracy: 0.8770\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2953 - accuracy: 0.8829\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2915 - accuracy: 0.8815\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2921 - accuracy: 0.8823\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2872 - accuracy: 0.8842\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3203 - accuracy: 0.8755\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2761 - accuracy: 0.8897\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2772 - accuracy: 0.8880\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2762 - accuracy: 0.8900\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2702 - accuracy: 0.8895\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2895 - accuracy: 0.8877\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2786 - accuracy: 0.8886\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2558 - accuracy: 0.8950\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2592 - accuracy: 0.8959\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2596 - accuracy: 0.8956\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2554 - accuracy: 0.8989\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2502 - accuracy: 0.8996\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2436 - accuracy: 0.9027\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2447 - accuracy: 0.9031\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2405 - accuracy: 0.9025\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2396 - accuracy: 0.9035\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2410 - accuracy: 0.9047\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2459 - accuracy: 0.9031\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2364 - accuracy: 0.9065\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2390 - accuracy: 0.9072\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2220 - accuracy: 0.9115\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2240 - accuracy: 0.9110\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2302 - accuracy: 0.9104\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2300 - accuracy: 0.9091\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2270 - accuracy: 0.9122\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2261 - accuracy: 0.9124\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2163 - accuracy: 0.9146\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2180 - accuracy: 0.9143\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2184 - accuracy: 0.9138\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2186 - accuracy: 0.9149\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2112 - accuracy: 0.9172\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2316 - accuracy: 0.9127\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2184 - accuracy: 0.9148\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2147 - accuracy: 0.9174\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2070 - accuracy: 0.91920s - loss: 0.2059 \n",
      "152/152 [==============================] - 2s 13ms/step - loss: 0.3687 - accuracy: 0.9009\n",
      "n_conv_1 = 512, k_conv_1 = 3,  n_conv_2 = 768, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 7,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 4,  n_dense_1 = 512, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000016C6BA4B788>\n",
      "Epoch 1/60\n",
      "  1/304 [..............................] - ETA: 12s - loss: 1.7917 - accuracy: 0.1172WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0160s vs `on_train_batch_end` time: 0.0280s). Check your callbacks.\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 1.1723 - accuracy: 0.5090\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.8473 - accuracy: 0.6506\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.6510 - accuracy: 0.7370\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.5504 - accuracy: 0.7755\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.5236 - accuracy: 0.7882\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4811 - accuracy: 0.8039\n",
      "Epoch 7/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4639 - accuracy: 0.8111\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4420 - accuracy: 0.8206\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4040 - accuracy: 0.8335\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 1.1160 - accuracy: 0.7345\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4336 - accuracy: 0.8270\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3985 - accuracy: 0.8400\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3775 - accuracy: 0.8479\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3642 - accuracy: 0.85320s - loss: 0.3632 - ac\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3483 - accuracy: 0.8616\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3376 - accuracy: 0.8651\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3423 - accuracy: 0.8649\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3206 - accuracy: 0.8705\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3121 - accuracy: 0.8748\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3163 - accuracy: 0.8744\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3282 - accuracy: 0.8712\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2965 - accuracy: 0.8812\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2934 - accuracy: 0.8821\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2861 - accuracy: 0.8842\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2924 - accuracy: 0.8816\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2840 - accuracy: 0.8842\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2733 - accuracy: 0.8903\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2757 - accuracy: 0.8894\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2762 - accuracy: 0.8887\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2666 - accuracy: 0.8920\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2691 - accuracy: 0.8904\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2589 - accuracy: 0.8953\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2541 - accuracy: 0.8984\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2671 - accuracy: 0.8949\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2475 - accuracy: 0.8999\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2477 - accuracy: 0.8989\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2403 - accuracy: 0.9028\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2413 - accuracy: 0.9044\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2562 - accuracy: 0.8996\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2368 - accuracy: 0.9058\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2351 - accuracy: 0.9069\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2314 - accuracy: 0.9070\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2387 - accuracy: 0.9054\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2238 - accuracy: 0.9096\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2284 - accuracy: 0.9083\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2995 - accuracy: 0.8988\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2443 - accuracy: 0.9045\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2139 - accuracy: 0.9129\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2043 - accuracy: 0.9162\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2033 - accuracy: 0.9174\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2156 - accuracy: 0.9152\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2052 - accuracy: 0.9174\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2165 - accuracy: 0.9142\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2165 - accuracy: 0.9163\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2068 - accuracy: 0.9171\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2035 - accuracy: 0.9193\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2202 - accuracy: 0.9141\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2246 - accuracy: 0.9144\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2131 - accuracy: 0.9175\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2031 - accuracy: 0.9207\n",
      "152/152 [==============================] - 2s 13ms/step - loss: 0.4369 - accuracy: 0.8954\n",
      "n_conv_1 = 512, k_conv_1 = 3,  n_conv_2 = 768, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 7,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 4,  n_dense_1 = 512, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000016C6AA2EA08>\n",
      "Epoch 1/60\n",
      "  1/304 [..............................] - ETA: 8s - loss: 1.7966 - accuracy: 0.2188WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0139s vs `on_train_batch_end` time: 0.0304s). Check your callbacks.\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 1.1448 - accuracy: 0.5123\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.8527 - accuracy: 0.6585\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.6381 - accuracy: 0.7446\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.5492 - accuracy: 0.7765\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 3.4504 - accuracy: 0.6506\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.7272 - accuracy: 0.7127\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.5922 - accuracy: 0.7623\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.5397 - accuracy: 0.7838\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.5093 - accuracy: 0.7980\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4866 - accuracy: 0.80690s - loss: 0.4868 \n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4641 - accuracy: 0.8150\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4460 - accuracy: 0.8236\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4295 - accuracy: 0.8283\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4208 - accuracy: 0.8315\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4139 - accuracy: 0.8360\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.4033 - accuracy: 0.8403\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3957 - accuracy: 0.8439\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3827 - accuracy: 0.8468\n",
      "Epoch 19/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3778 - accuracy: 0.8510\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3687 - accuracy: 0.8552\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3708 - accuracy: 0.8542\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3550 - accuracy: 0.8614\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3466 - accuracy: 0.8638\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3485 - accuracy: 0.8634\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3363 - accuracy: 0.8665\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3318 - accuracy: 0.8685\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3301 - accuracy: 0.8682\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3246 - accuracy: 0.8702\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3198 - accuracy: 0.8735\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3177 - accuracy: 0.8728\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3085 - accuracy: 0.8765\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3928 - accuracy: 0.8602\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.3058 - accuracy: 0.8801\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2922 - accuracy: 0.8814\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2886 - accuracy: 0.8827\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2878 - accuracy: 0.8843\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2852 - accuracy: 0.8847\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2827 - accuracy: 0.8874\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2839 - accuracy: 0.8870\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2793 - accuracy: 0.8883\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2718 - accuracy: 0.8898\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2734 - accuracy: 0.8907\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2722 - accuracy: 0.8893\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2691 - accuracy: 0.8922\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2613 - accuracy: 0.8927\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2666 - accuracy: 0.8930\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2540 - accuracy: 0.8969\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2535 - accuracy: 0.8980\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2491 - accuracy: 0.9000\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2580 - accuracy: 0.8971\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2482 - accuracy: 0.9021\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2576 - accuracy: 0.8978\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2447 - accuracy: 0.9029\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2449 - accuracy: 0.9023\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2391 - accuracy: 0.9051\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2366 - accuracy: 0.9054\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2377 - accuracy: 0.9051\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2330 - accuracy: 0.9057\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2384 - accuracy: 0.9050\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 14s 45ms/step - loss: 0.2356 - accuracy: 0.9064\n",
      "152/152 [==============================] - 2s 13ms/step - loss: 0.4196 - accuracy: 0.8810\n",
      "n_conv_1 = 512, k_conv_1 = 7,  n_conv_2 = 768, k_conv_2 = 7, \n",
      "          n_conv_3 = 512,  k_conv_3 = 7,  maxpooling_pool_size = 4,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 768, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  relu\n",
      "Epoch 1/60\n",
      "  1/304 [..............................] - ETA: 4s - loss: 1.7934 - accuracy: 0.1797WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0140s vs `on_train_batch_end` time: 0.0299s). Check your callbacks.\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 1.0715 - accuracy: 0.5510\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.6909 - accuracy: 0.7268\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.5208 - accuracy: 0.7989\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.4035 - accuracy: 0.8409\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.3538 - accuracy: 0.8637\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.3178 - accuracy: 0.8775\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.3045 - accuracy: 0.8828\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.2827 - accuracy: 0.8898\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.2565 - accuracy: 0.9009\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.2411 - accuracy: 0.9070\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.2216 - accuracy: 0.9152\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.2146 - accuracy: 0.9160\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1968 - accuracy: 0.9217\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1962 - accuracy: 0.9229\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1808 - accuracy: 0.9297\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1734 - accuracy: 0.9314\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1642 - accuracy: 0.9363\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1538 - accuracy: 0.9396\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1493 - accuracy: 0.9419\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1476 - accuracy: 0.9421\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1377 - accuracy: 0.9465\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1454 - accuracy: 0.9441\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1435 - accuracy: 0.9446\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1276 - accuracy: 0.9506\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1264 - accuracy: 0.9502\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1228 - accuracy: 0.9532\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1204 - accuracy: 0.9543\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1205 - accuracy: 0.9539\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1155 - accuracy: 0.9557\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1088 - accuracy: 0.9584\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1127 - accuracy: 0.9566\n",
      "Epoch 32/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1073 - accuracy: 0.9592\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0976 - accuracy: 0.9619\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1066 - accuracy: 0.9602\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1011 - accuracy: 0.9618\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 13s 41ms/step - loss: 0.1011 - accuracy: 0.9620\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1039 - accuracy: 0.9605\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1051 - accuracy: 0.9628\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0927 - accuracy: 0.9648\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0965 - accuracy: 0.9631\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0949 - accuracy: 0.9653\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0960 - accuracy: 0.9652\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0911 - accuracy: 0.9661\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0898 - accuracy: 0.9669\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0860 - accuracy: 0.9692\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0846 - accuracy: 0.9678\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0805 - accuracy: 0.9693\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0913 - accuracy: 0.9676\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0777 - accuracy: 0.9703\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0947 - accuracy: 0.9665\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0803 - accuracy: 0.9706\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0868 - accuracy: 0.9691\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0791 - accuracy: 0.9708\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0815 - accuracy: 0.9696\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0783 - accuracy: 0.9707\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0718 - accuracy: 0.9736\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0756 - accuracy: 0.9716\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0787 - accuracy: 0.9707\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0890 - accuracy: 0.9682\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0770 - accuracy: 0.9718\n",
      "152/152 [==============================] - 2s 11ms/step - loss: 0.4089 - accuracy: 0.9310\n",
      "n_conv_1 = 512, k_conv_1 = 7,  n_conv_2 = 768, k_conv_2 = 7, \n",
      "          n_conv_3 = 512,  k_conv_3 = 7,  maxpooling_pool_size = 4,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 768, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  relu\n",
      "Epoch 1/60\n",
      "  1/304 [..............................] - ETA: 9s - loss: 1.7955 - accuracy: 0.1719WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0140s vs `on_train_batch_end` time: 0.0299s). Check your callbacks.\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 1.0465 - accuracy: 0.5624\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.6689 - accuracy: 0.7348\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.4685 - accuracy: 0.8176\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.3901 - accuracy: 0.8500\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.3474 - accuracy: 0.8638\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.3174 - accuracy: 0.8782\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.2991 - accuracy: 0.8844\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.2659 - accuracy: 0.8967\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.2517 - accuracy: 0.9020\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.2450 - accuracy: 0.9069\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.2213 - accuracy: 0.9153\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.2035 - accuracy: 0.9210\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1925 - accuracy: 0.9256\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1821 - accuracy: 0.9279\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1775 - accuracy: 0.9322\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1638 - accuracy: 0.9361\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1534 - accuracy: 0.9401\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1594 - accuracy: 0.9385\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1511 - accuracy: 0.9415\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1530 - accuracy: 0.9412\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1350 - accuracy: 0.9472\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1406 - accuracy: 0.9461\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1318 - accuracy: 0.9479\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1301 - accuracy: 0.9497\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1243 - accuracy: 0.9512\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1214 - accuracy: 0.9534\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1199 - accuracy: 0.9534\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1214 - accuracy: 0.9537\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1135 - accuracy: 0.9560\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1159 - accuracy: 0.9554\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1155 - accuracy: 0.9565\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1015 - accuracy: 0.9610\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1096 - accuracy: 0.9580\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1089 - accuracy: 0.9585\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1037 - accuracy: 0.9610\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1078 - accuracy: 0.9579\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1015 - accuracy: 0.9616\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1035 - accuracy: 0.9614\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1017 - accuracy: 0.9616\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0900 - accuracy: 0.9659\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1002 - accuracy: 0.9629\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1007 - accuracy: 0.9647\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0914 - accuracy: 0.9659\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0966 - accuracy: 0.9646\n",
      "Epoch 45/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0937 - accuracy: 0.9657\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0912 - accuracy: 0.9658\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0907 - accuracy: 0.9671\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1021 - accuracy: 0.9639\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0987 - accuracy: 0.9651\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0870 - accuracy: 0.9678\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0806 - accuracy: 0.9702\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0864 - accuracy: 0.9683\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0911 - accuracy: 0.9664\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0857 - accuracy: 0.9689\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0800 - accuracy: 0.9702\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0819 - accuracy: 0.9691\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0800 - accuracy: 0.9706\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0863 - accuracy: 0.9693\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0820 - accuracy: 0.9705\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0822 - accuracy: 0.9709\n",
      "152/152 [==============================] - 2s 10ms/step - loss: 0.4298 - accuracy: 0.9325\n",
      "n_conv_1 = 512, k_conv_1 = 7,  n_conv_2 = 768, k_conv_2 = 7, \n",
      "          n_conv_3 = 512,  k_conv_3 = 7,  maxpooling_pool_size = 4,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 768, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  relu\n",
      "Epoch 1/60\n",
      "  1/304 [..............................] - ETA: 8s - loss: 1.7969 - accuracy: 0.1172WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0130s vs `on_train_batch_end` time: 0.0280s). Check your callbacks.\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 1.0439 - accuracy: 0.5618\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.6735 - accuracy: 0.7313\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.4645 - accuracy: 0.8166\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.3849 - accuracy: 0.8513\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.3707 - accuracy: 0.8635\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 13s 41ms/step - loss: 0.3179 - accuracy: 0.8768\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.2882 - accuracy: 0.8898\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.2744 - accuracy: 0.8924\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.2537 - accuracy: 0.8989\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.2480 - accuracy: 0.9024\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.2233 - accuracy: 0.9101\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.2105 - accuracy: 0.9167\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.2012 - accuracy: 0.9196\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1923 - accuracy: 0.9259\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1801 - accuracy: 0.9290\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1667 - accuracy: 0.9349\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1677 - accuracy: 0.9363\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1609 - accuracy: 0.9371\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1573 - accuracy: 0.9397\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1449 - accuracy: 0.9437\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1443 - accuracy: 0.9448\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1463 - accuracy: 0.9441\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1342 - accuracy: 0.9498\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1380 - accuracy: 0.9473\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1244 - accuracy: 0.9522\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1258 - accuracy: 0.9519\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1245 - accuracy: 0.9536\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1184 - accuracy: 0.9540\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1132 - accuracy: 0.9562\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1117 - accuracy: 0.9572\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1157 - accuracy: 0.9554\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1138 - accuracy: 0.9572\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1114 - accuracy: 0.9577\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1070 - accuracy: 0.9587\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1079 - accuracy: 0.9596\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1027 - accuracy: 0.9606\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1063 - accuracy: 0.9587\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0981 - accuracy: 0.9628\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1107 - accuracy: 0.9604\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0996 - accuracy: 0.9636\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0974 - accuracy: 0.9636\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0979 - accuracy: 0.9632\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.1058 - accuracy: 0.9620\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0958 - accuracy: 0.9636\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0940 - accuracy: 0.9657\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0970 - accuracy: 0.9641\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0923 - accuracy: 0.9650\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0896 - accuracy: 0.9670\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0856 - accuracy: 0.9685\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0807 - accuracy: 0.9705\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0886 - accuracy: 0.9681\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0907 - accuracy: 0.9671\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0869 - accuracy: 0.9676\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0833 - accuracy: 0.9697\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0854 - accuracy: 0.9682\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0802 - accuracy: 0.9709\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0900 - accuracy: 0.9672\n",
      "Epoch 58/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 13s 41ms/step - loss: 0.0823 - accuracy: 0.9700\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 13s 42ms/step - loss: 0.0809 - accuracy: 0.9704\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 13s 41ms/step - loss: 0.0854 - accuracy: 0.9701\n",
      "152/152 [==============================] - 2s 10ms/step - loss: 0.4331 - accuracy: 0.9375\n",
      "n_conv_1 = 768, k_conv_1 = 7,  n_conv_2 = 512, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 512, dropout_1 = 0.3,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000016C72B8E088>\n",
      "Epoch 1/60\n",
      "  1/304 [..............................] - ETA: 6s - loss: 1.7948 - accuracy: 0.1406WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0149s vs `on_train_batch_end` time: 0.0240s). Check your callbacks.\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 1.1015 - accuracy: 0.5435\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.7967 - accuracy: 0.6782\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.7055 - accuracy: 0.7236\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.5674 - accuracy: 0.7713\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.5403 - accuracy: 0.7834\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.4875 - accuracy: 0.8012\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.4896 - accuracy: 0.8049\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.4575 - accuracy: 0.8156\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.4321 - accuracy: 0.8256\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.4252 - accuracy: 0.8327\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3977 - accuracy: 0.8434\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3921 - accuracy: 0.8472\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3869 - accuracy: 0.8479\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3838 - accuracy: 0.8521\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3537 - accuracy: 0.8614\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.9543 - accuracy: 0.7579\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3984 - accuracy: 0.8464\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3656 - accuracy: 0.8598\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3480 - accuracy: 0.8653\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3390 - accuracy: 0.8666\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3254 - accuracy: 0.8727\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3230 - accuracy: 0.8744\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3236 - accuracy: 0.8748\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3184 - accuracy: 0.8753\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3212 - accuracy: 0.8751\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3166 - accuracy: 0.8757\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3986 - accuracy: 0.8558\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3137 - accuracy: 0.8783\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3040 - accuracy: 0.8825\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3033 - accuracy: 0.8820\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3112 - accuracy: 0.8794\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2991 - accuracy: 0.8834\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2979 - accuracy: 0.8851\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2952 - accuracy: 0.8857\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2965 - accuracy: 0.8856\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2890 - accuracy: 0.8876\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2880 - accuracy: 0.8883\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2830 - accuracy: 0.8906\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2866 - accuracy: 0.8881\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2850 - accuracy: 0.8889\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2810 - accuracy: 0.8901\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2831 - accuracy: 0.8889\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2806 - accuracy: 0.8911\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2776 - accuracy: 0.8912\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2763 - accuracy: 0.8936\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2677 - accuracy: 0.8938\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2690 - accuracy: 0.8945\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2658 - accuracy: 0.8956\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2693 - accuracy: 0.8934\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2659 - accuracy: 0.8940\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2710 - accuracy: 0.8951\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2580 - accuracy: 0.8978\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2613 - accuracy: 0.8954\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2587 - accuracy: 0.8969\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2583 - accuracy: 0.8989\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2541 - accuracy: 0.9013\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2566 - accuracy: 0.8980\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2524 - accuracy: 0.9000\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2576 - accuracy: 0.8993\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2511 - accuracy: 0.9015\n",
      "152/152 [==============================] - 2s 14ms/step - loss: 0.2915 - accuracy: 0.8906\n",
      "n_conv_1 = 768, k_conv_1 = 7,  n_conv_2 = 512, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 512, dropout_1 = 0.3,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000016C72B8D548>\n",
      "Epoch 1/60\n",
      "  1/304 [..............................] - ETA: 7s - loss: 1.7941 - accuracy: 0.1406WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0160s vs `on_train_batch_end` time: 0.0249s). Check your callbacks.\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 1.1012 - accuracy: 0.5438\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.8086 - accuracy: 0.6791\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.6884 - accuracy: 0.7331\n",
      "Epoch 4/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 12s 40ms/step - loss: 0.5545 - accuracy: 0.7774\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 1.5558 - accuracy: 0.6870\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.6178 - accuracy: 0.7541\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.5393 - accuracy: 0.7846\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.4917 - accuracy: 0.8024\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.4601 - accuracy: 0.8147\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.4430 - accuracy: 0.8242\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.4254 - accuracy: 0.8319\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.4162 - accuracy: 0.8365\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.4055 - accuracy: 0.8413\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3913 - accuracy: 0.8468\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3809 - accuracy: 0.8522\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.4050 - accuracy: 0.8469\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3687 - accuracy: 0.8587\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3640 - accuracy: 0.8577\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3516 - accuracy: 0.8651\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3609 - accuracy: 0.8630\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3521 - accuracy: 0.8624\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3392 - accuracy: 0.8691\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3502 - accuracy: 0.8674\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3316 - accuracy: 0.8706\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3297 - accuracy: 0.8697\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3241 - accuracy: 0.8733\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.4083 - accuracy: 0.8552\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3283 - accuracy: 0.8731\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3144 - accuracy: 0.8770\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3076 - accuracy: 0.8796\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3081 - accuracy: 0.8790\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3090 - accuracy: 0.8785\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3008 - accuracy: 0.8796\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3002 - accuracy: 0.8808\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2971 - accuracy: 0.8816\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2954 - accuracy: 0.8842\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2931 - accuracy: 0.8836\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3051 - accuracy: 0.8825\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2934 - accuracy: 0.8845\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2822 - accuracy: 0.8858\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2849 - accuracy: 0.8877\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2810 - accuracy: 0.8864\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2972 - accuracy: 0.8862\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2805 - accuracy: 0.8883\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2728 - accuracy: 0.8909\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2732 - accuracy: 0.8909\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2768 - accuracy: 0.8921\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2750 - accuracy: 0.8909\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2659 - accuracy: 0.8944\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2666 - accuracy: 0.8942\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2682 - accuracy: 0.8937\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2707 - accuracy: 0.8939\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2589 - accuracy: 0.8966\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2592 - accuracy: 0.8983\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2608 - accuracy: 0.8986\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2623 - accuracy: 0.8966\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2579 - accuracy: 0.8980\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2551 - accuracy: 0.8991\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2598 - accuracy: 0.8965\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2492 - accuracy: 0.9023\n",
      "152/152 [==============================] - 2s 14ms/step - loss: 0.3239 - accuracy: 0.8767\n",
      "n_conv_1 = 768, k_conv_1 = 7,  n_conv_2 = 512, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 512, dropout_1 = 0.3,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000016C6A9E9E88>\n",
      "Epoch 1/60\n",
      "  1/304 [..............................] - ETA: 6s - loss: 1.7928 - accuracy: 0.1484WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0150s vs `on_train_batch_end` time: 0.0250s). Check your callbacks.\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 1.0862 - accuracy: 0.5464\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.7952 - accuracy: 0.6869\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.6514 - accuracy: 0.7425\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.6020 - accuracy: 0.7624\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.5266 - accuracy: 0.7877\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.4994 - accuracy: 0.7984\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.4623 - accuracy: 0.8128\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.4702 - accuracy: 0.8123\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.4339 - accuracy: 0.8262\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.4334 - accuracy: 0.8324\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3930 - accuracy: 0.8443\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3844 - accuracy: 0.8495\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3717 - accuracy: 0.8559\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3970 - accuracy: 0.8480\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3585 - accuracy: 0.8596\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3456 - accuracy: 0.8660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3372 - accuracy: 0.8675\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3428 - accuracy: 0.8660\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3231 - accuracy: 0.8713\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3513 - accuracy: 0.8678\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3207 - accuracy: 0.8747\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3129 - accuracy: 0.8787\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3101 - accuracy: 0.8803\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3351 - accuracy: 0.8719\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3027 - accuracy: 0.8813\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.3004 - accuracy: 0.8829\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2973 - accuracy: 0.8840\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2961 - accuracy: 0.8847\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2923 - accuracy: 0.8849\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2926 - accuracy: 0.8861\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2864 - accuracy: 0.8884\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2911 - accuracy: 0.8877\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2797 - accuracy: 0.8905\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2826 - accuracy: 0.8908\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2723 - accuracy: 0.8940\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2795 - accuracy: 0.8919\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2728 - accuracy: 0.8954\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2689 - accuracy: 0.8952\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2644 - accuracy: 0.8958\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2660 - accuracy: 0.8955\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2748 - accuracy: 0.8950\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2651 - accuracy: 0.8968\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2608 - accuracy: 0.8959\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2538 - accuracy: 0.8988\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2535 - accuracy: 0.9001\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2573 - accuracy: 0.9006\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2492 - accuracy: 0.9023\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2594 - accuracy: 0.8988\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2575 - accuracy: 0.9001\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2444 - accuracy: 0.9034\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2414 - accuracy: 0.9047\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2586 - accuracy: 0.9001\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2427 - accuracy: 0.9052\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2386 - accuracy: 0.9056\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2369 - accuracy: 0.9065\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2410 - accuracy: 0.9053\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2368 - accuracy: 0.9055\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2370 - accuracy: 0.9063\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2421 - accuracy: 0.9068\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 12s 40ms/step - loss: 0.2336 - accuracy: 0.9078\n",
      "152/152 [==============================] - 2s 14ms/step - loss: 0.2704 - accuracy: 0.9017\n",
      "n_conv_1 = 512, k_conv_1 = 7,  n_conv_2 = 512, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 4,\n",
      "          avepooling_pool_size = 4,  n_dense_1 = 512, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000016C6B050408>\n",
      "Epoch 1/60\n",
      "304/304 [==============================] - 6s 21ms/step - loss: 1.1032 - accuracy: 0.5389\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.8235 - accuracy: 0.6745\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.6383 - accuracy: 0.7426\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.5690 - accuracy: 0.7681\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.5395 - accuracy: 0.7801\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.5004 - accuracy: 0.7951\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.4770 - accuracy: 0.8068\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.4747 - accuracy: 0.8103\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.4423 - accuracy: 0.8233\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.4266 - accuracy: 0.8291\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.9361 - accuracy: 0.7495\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.4789 - accuracy: 0.8076\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.4092 - accuracy: 0.8366\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3830 - accuracy: 0.8484\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3671 - accuracy: 0.8550\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3532 - accuracy: 0.8615\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3581 - accuracy: 0.8601\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3369 - accuracy: 0.8674\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3331 - accuracy: 0.8698\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3350 - accuracy: 0.8694 \n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3249 - accuracy: 0.8728\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3172 - accuracy: 0.8735\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3123 - accuracy: 0.8768\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3178 - accuracy: 0.8771\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3167 - accuracy: 0.8755\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3026 - accuracy: 0.8821\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2964 - accuracy: 0.8798\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2989 - accuracy: 0.8830\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2935 - accuracy: 0.8828\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2916 - accuracy: 0.8847\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2939 - accuracy: 0.8845\n",
      "Epoch 32/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2804 - accuracy: 0.8889\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2807 - accuracy: 0.8877\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2953 - accuracy: 0.8872\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2707 - accuracy: 0.8930\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2602 - accuracy: 0.8963\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2690 - accuracy: 0.8928\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2671 - accuracy: 0.8956\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2763 - accuracy: 0.8928\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2678 - accuracy: 0.8923\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2594 - accuracy: 0.8973\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2564 - accuracy: 0.8973\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2504 - accuracy: 0.9006\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2504 - accuracy: 0.9000\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2541 - accuracy: 0.9000\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2460 - accuracy: 0.9041 0s - loss: 0.2457 - \n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2381 - accuracy: 0.9026\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2465 - accuracy: 0.9023\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2429 - accuracy: 0.9044\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2456 - accuracy: 0.9043\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2423 - accuracy: 0.9037\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2385 - accuracy: 0.9045\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2313 - accuracy: 0.9076\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2293 - accuracy: 0.9081\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2375 - accuracy: 0.9063\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2311 - accuracy: 0.9079\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2328 - accuracy: 0.9078\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2292 - accuracy: 0.9097\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2235 - accuracy: 0.9108\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2371 - accuracy: 0.9080\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.3420 - accuracy: 0.8965\n",
      "n_conv_1 = 512, k_conv_1 = 7,  n_conv_2 = 512, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 4,\n",
      "          avepooling_pool_size = 4,  n_dense_1 = 512, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000016B4BC36808>\n",
      "Epoch 1/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 1.1071 - accuracy: 0.5404\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.7889 - accuracy: 0.6858\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.6515 - accuracy: 0.7417\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.5638 - accuracy: 0.7711 \n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 1.4531 - accuracy: 0.6780\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.6167 - accuracy: 0.7557\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.5339 - accuracy: 0.7873\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.4896 - accuracy: 0.8032\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.4740 - accuracy: 0.8130\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.4514 - accuracy: 0.8209\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.4346 - accuracy: 0.8308 0s - loss: 0.4337 \n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.4233 - accuracy: 0.8353\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.4110 - accuracy: 0.8410\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.4002 - accuracy: 0.8451\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3821 - accuracy: 0.8518\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3737 - accuracy: 0.8558\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3681 - accuracy: 0.8566\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3636 - accuracy: 0.8579\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3616 - accuracy: 0.8625\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3521 - accuracy: 0.8643\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3462 - accuracy: 0.8658\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3399 - accuracy: 0.8682\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3492 - accuracy: 0.8657\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3253 - accuracy: 0.8734\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3221 - accuracy: 0.8743\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3148 - accuracy: 0.8769\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3158 - accuracy: 0.8767\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3228 - accuracy: 0.8734\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3118 - accuracy: 0.8785\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3014 - accuracy: 0.8824\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2936 - accuracy: 0.8827\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2943 - accuracy: 0.8818\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2933 - accuracy: 0.8839 0s - loss: 0.2916 - accuracy\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2907 - accuracy: 0.8844\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2923 - accuracy: 0.8822\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2773 - accuracy: 0.8891 0s -\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2753 - accuracy: 0.8900\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2843 - accuracy: 0.8862\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2748 - accuracy: 0.8918\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2762 - accuracy: 0.8926\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2680 - accuracy: 0.8927\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2720 - accuracy: 0.8921\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2594 - accuracy: 0.8953\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2615 - accuracy: 0.8949\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2622 - accuracy: 0.8969\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2547 - accuracy: 0.8983\n",
      "Epoch 47/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2482 - accuracy: 0.9007\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2501 - accuracy: 0.9010\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2686 - accuracy: 0.8949\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2527 - accuracy: 0.8994\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2458 - accuracy: 0.9012\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2427 - accuracy: 0.9038\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2413 - accuracy: 0.9047\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2364 - accuracy: 0.9046\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2376 - accuracy: 0.9069\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2443 - accuracy: 0.9042\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2384 - accuracy: 0.9054\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2510 - accuracy: 0.9031\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2257 - accuracy: 0.9110\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2282 - accuracy: 0.9081\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.3735 - accuracy: 0.8856\n",
      "n_conv_1 = 512, k_conv_1 = 7,  n_conv_2 = 512, k_conv_2 = 3, \n",
      "          n_conv_3 = 512,  k_conv_3 = 3,  maxpooling_pool_size = 4,\n",
      "          avepooling_pool_size = 4,  n_dense_1 = 512, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000016B518DF308>\n",
      "Epoch 1/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 1.1162 - accuracy: 0.5322\n",
      "Epoch 2/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.8154 - accuracy: 0.6756\n",
      "Epoch 3/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.6599 - accuracy: 0.7373\n",
      "Epoch 4/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.5736 - accuracy: 0.7686\n",
      "Epoch 5/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.5371 - accuracy: 0.7828\n",
      "Epoch 6/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.5154 - accuracy: 0.7931\n",
      "Epoch 7/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.4776 - accuracy: 0.8072\n",
      "Epoch 8/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.4580 - accuracy: 0.8183\n",
      "Epoch 9/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 1.7744 - accuracy: 0.7101\n",
      "Epoch 10/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.6519 - accuracy: 0.7419\n",
      "Epoch 11/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.5427 - accuracy: 0.7877\n",
      "Epoch 12/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.4890 - accuracy: 0.8065\n",
      "Epoch 13/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.4642 - accuracy: 0.8170\n",
      "Epoch 14/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.4427 - accuracy: 0.8263\n",
      "Epoch 15/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.4265 - accuracy: 0.8341\n",
      "Epoch 16/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.4126 - accuracy: 0.8387\n",
      "Epoch 17/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.4016 - accuracy: 0.8434\n",
      "Epoch 18/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3992 - accuracy: 0.8444\n",
      "Epoch 19/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3971 - accuracy: 0.8463\n",
      "Epoch 20/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3723 - accuracy: 0.8541\n",
      "Epoch 21/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3696 - accuracy: 0.8564\n",
      "Epoch 22/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3620 - accuracy: 0.8582\n",
      "Epoch 23/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3637 - accuracy: 0.8582\n",
      "Epoch 24/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3473 - accuracy: 0.8643\n",
      "Epoch 25/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3504 - accuracy: 0.8634\n",
      "Epoch 26/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3543 - accuracy: 0.8623\n",
      "Epoch 27/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3363 - accuracy: 0.8684\n",
      "Epoch 28/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3323 - accuracy: 0.8696\n",
      "Epoch 29/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3305 - accuracy: 0.8705\n",
      "Epoch 30/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3273 - accuracy: 0.8718\n",
      "Epoch 31/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3502 - accuracy: 0.8652\n",
      "Epoch 32/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3131 - accuracy: 0.8777\n",
      "Epoch 33/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3122 - accuracy: 0.8762\n",
      "Epoch 34/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3115 - accuracy: 0.8782\n",
      "Epoch 35/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3080 - accuracy: 0.8789\n",
      "Epoch 36/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3062 - accuracy: 0.8787\n",
      "Epoch 37/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2973 - accuracy: 0.8821\n",
      "Epoch 38/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.3004 - accuracy: 0.8809\n",
      "Epoch 39/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2928 - accuracy: 0.8847\n",
      "Epoch 40/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2986 - accuracy: 0.8843\n",
      "Epoch 41/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2872 - accuracy: 0.8879\n",
      "Epoch 42/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2806 - accuracy: 0.8889\n",
      "Epoch 43/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2838 - accuracy: 0.8890\n",
      "Epoch 44/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2782 - accuracy: 0.8898\n",
      "Epoch 45/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2775 - accuracy: 0.8907\n",
      "Epoch 46/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2726 - accuracy: 0.8931\n",
      "Epoch 47/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2713 - accuracy: 0.8949\n",
      "Epoch 48/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2643 - accuracy: 0.8958\n",
      "Epoch 49/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2650 - accuracy: 0.8965\n",
      "Epoch 50/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2733 - accuracy: 0.8948\n",
      "Epoch 51/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2613 - accuracy: 0.8955\n",
      "Epoch 52/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2654 - accuracy: 0.8956\n",
      "Epoch 53/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2521 - accuracy: 0.9002\n",
      "Epoch 54/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2532 - accuracy: 0.8990\n",
      "Epoch 55/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2560 - accuracy: 0.8983\n",
      "Epoch 56/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2517 - accuracy: 0.8998\n",
      "Epoch 57/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2497 - accuracy: 0.9018\n",
      "Epoch 58/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2439 - accuracy: 0.9024\n",
      "Epoch 59/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2483 - accuracy: 0.9026\n",
      "Epoch 60/60\n",
      "304/304 [==============================] - 6s 20ms/step - loss: 0.2431 - accuracy: 0.9040\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.3489 - accuracy: 0.8904\n",
      "n_conv_1 = 512, k_conv_1 = 3,  n_conv_2 = 512, k_conv_2 = 7, \n",
      "          n_conv_3 = 768,  k_conv_3 = 7,  maxpooling_pool_size = 2,\n",
      "          avepooling_pool_size = 2,  n_dense_1 = 768, dropout_1 = 0.22,\n",
      "          n_dense_2 = 512, dropout_2 = 0.25, activation=  relu\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2/456 [..............................] - ETA: 19s - loss: 1.7889 - accuracy: 0.1406WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0169s vs `on_train_batch_end` time: 0.0359s). Check your callbacks.\n",
      "456/456 [==============================] - 24s 53ms/step - loss: 0.8753 - accuracy: 0.6336\n",
      "Epoch 2/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.4452 - accuracy: 0.8315\n",
      "Epoch 3/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.3509 - accuracy: 0.8645\n",
      "Epoch 4/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.3034 - accuracy: 0.8833\n",
      "Epoch 5/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.2776 - accuracy: 0.8961\n",
      "Epoch 6/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.2529 - accuracy: 0.9054\n",
      "Epoch 7/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.2338 - accuracy: 0.9113\n",
      "Epoch 8/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.2202 - accuracy: 0.9163\n",
      "Epoch 9/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.2070 - accuracy: 0.9233\n",
      "Epoch 10/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.1969 - accuracy: 0.9264\n",
      "Epoch 11/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.1913 - accuracy: 0.9276\n",
      "Epoch 12/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.1789 - accuracy: 0.9329\n",
      "Epoch 13/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.1705 - accuracy: 0.9357\n",
      "Epoch 14/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.1637 - accuracy: 0.9369\n",
      "Epoch 15/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.1622 - accuracy: 0.9381\n",
      "Epoch 16/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.1537 - accuracy: 0.9409\n",
      "Epoch 17/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.1443 - accuracy: 0.9445\n",
      "Epoch 18/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.1429 - accuracy: 0.9462\n",
      "Epoch 19/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.1357 - accuracy: 0.9478\n",
      "Epoch 20/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.1368 - accuracy: 0.9471\n",
      "Epoch 21/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.1296 - accuracy: 0.9514\n",
      "Epoch 22/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.1311 - accuracy: 0.9507\n",
      "Epoch 23/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.1220 - accuracy: 0.9539\n",
      "Epoch 24/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.1219 - accuracy: 0.9535\n",
      "Epoch 25/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.1172 - accuracy: 0.9557\n",
      "Epoch 26/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.1166 - accuracy: 0.9560\n",
      "Epoch 27/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.1101 - accuracy: 0.9592\n",
      "Epoch 28/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.1032 - accuracy: 0.9609\n",
      "Epoch 29/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.1057 - accuracy: 0.9598\n",
      "Epoch 30/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.1093 - accuracy: 0.9593\n",
      "Epoch 31/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.1043 - accuracy: 0.9610\n",
      "Epoch 32/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.1013 - accuracy: 0.9622\n",
      "Epoch 33/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.1008 - accuracy: 0.9628\n",
      "Epoch 34/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.0966 - accuracy: 0.9631\n",
      "Epoch 35/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.1048 - accuracy: 0.9611\n",
      "Epoch 36/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.0997 - accuracy: 0.9633\n",
      "Epoch 37/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.0942 - accuracy: 0.9648\n",
      "Epoch 38/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.0906 - accuracy: 0.9664\n",
      "Epoch 39/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.0928 - accuracy: 0.9650\n",
      "Epoch 40/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.0928 - accuracy: 0.9662\n",
      "Epoch 41/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.0926 - accuracy: 0.9659\n",
      "Epoch 42/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.0884 - accuracy: 0.9665\n",
      "Epoch 43/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.0927 - accuracy: 0.9659\n",
      "Epoch 44/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.0882 - accuracy: 0.9659\n",
      "Epoch 45/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.0959 - accuracy: 0.9656\n",
      "Epoch 46/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.0846 - accuracy: 0.9680\n",
      "Epoch 47/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.0845 - accuracy: 0.9687\n",
      "Epoch 48/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.0820 - accuracy: 0.9692\n",
      "Epoch 49/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.0824 - accuracy: 0.9701\n",
      "Epoch 50/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.0838 - accuracy: 0.9688\n",
      "Epoch 51/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.0863 - accuracy: 0.9685\n",
      "Epoch 52/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.0816 - accuracy: 0.9697\n",
      "Epoch 53/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.0795 - accuracy: 0.9715\n",
      "Epoch 54/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.0770 - accuracy: 0.9706\n",
      "Epoch 55/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.0757 - accuracy: 0.9722\n",
      "Epoch 56/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.0717 - accuracy: 0.9724\n",
      "Epoch 57/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.0758 - accuracy: 0.9719\n",
      "Epoch 58/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.0725 - accuracy: 0.9733\n",
      "Epoch 59/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.0799 - accuracy: 0.9716\n",
      "Epoch 60/60\n",
      "456/456 [==============================] - 24s 52ms/step - loss: 0.0709 - accuracy: 0.9737\n",
      "Wall time: 10h 25min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rscv = RandomizedSearchCV(model, param_distributions=params, cv=cv, n_iter=n_tune_iter)\n",
    "rscv_results = rscv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score is: 0.9357905387878418 using {'n_dense_2': 512, 'n_dense_1': 768, 'n_conv_3': 768, 'n_conv_2': 512, 'n_conv_1': 512, 'maxpooling_pool_size': 2, 'k_conv_3': 7, 'k_conv_2': 7, 'k_conv_1': 3, 'dropout_2': 0.25, 'dropout_1': 0.22, 'avepooling_pool_size': 2, 'activation': 'relu'}\n"
     ]
    }
   ],
   "source": [
    "print('Best score is: {} using {}'.format(rscv_results.best_score_,\n",
    "rscv_results.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = rscv_results.best_estimator_\n",
    "best_param = rscv_results.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JkbhJMox8Jl9"
   },
   "source": [
    "### Test the best model based on the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZwGk5dR8JmS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dfuller\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py:241: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    }
   ],
   "source": [
    "y_hat = best_model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E_GNq-R_8JmZ",
    "outputId": "0fc16efe-7739-4394-f1a3-46da92e79584"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = to_categorical(y_hat)\n",
    "y_hat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFqRQ5XB8Jmc",
    "outputId": "a957fbed-92f8-4bdd-d24b-5df3361a8bac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FTWf6Cf-8Jme",
    "outputId": "1256b170-33dc-4171-f2ff-7a3d7ddd74e3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQiklEQVR4nO3ccayddX3H8fdnrSKbojAupGlx7ZZOKTBUOmzmtqCYUZGsLJFZt0ljWBoZbiwxmcU/psvShP3hYsgA0zhDyTaxmTo6FB0rY24RxcuG1FIZnTBo2tCKTplLWFq/++P+lp22t73ntvee4+X3fiUnz/N8z/M75/tLm0+fPs9znlQVkqQ+/Ni4G5AkjY6hL0kdMfQlqSOGviR1xNCXpI4sHncDMzn77LNr+fLl425DOtL3H59anvGa8fYhHcfDDz/87aqaOLr+Ix/6y5cvZ3JyctxtSEf6+8umlm99YJxdSMeV5D+mq3t6R5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOvIj/4vccbr1vfcfU7vhY28ZQyeSNDc80pekjhj6ktQRQ1+SOmLoS1JHvJA7Sx9551VHbL//U/eMqRNJmj2P9CWpI4a+JHXE0JekjnhOf8Du155/ZOGyW8fTiCTNk75C/8OvPGr7e+PpQ5LGxNM7ktSRvo70j3LR1ouO2N42pj4kaVQ80pekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1ZKjQT/JUkp1JHkky2WpnJbkvyRNteebA/jcl2ZPk8SRXDNQvaZ+zJ8ktSTL3U5IkHc9sjvTfXFWvq6rVbXsTsKOqVgI72jZJVgHrgQuAtcBtSRa1MbcDG4GV7bX21KcgSRrWqTyGYR1wWVvfCjwAfKDV76qqF4Ank+wBLk3yFHBGVT0IkORO4Grg3lPo4YSWb/rcEdtPvWy+vkmSFoZhj/QL+LskDyfZ2GrnVtV+gLY8p9WXAs8MjN3bakvb+tH1YyTZmGQyyeTBgweHbFGSNJNhj/TfVFX7kpwD3JfkmyfYd7rz9HWC+rHFqi3AFoDVq1dPu48kafaGOtKvqn1teQD4LHAp8GySJQBteaDtvhc4b2D4MmBfqy+bpi5JGpEZQz/JTyR5xf+tA78CfAPYDmxou20A7m7r24H1SU5LsoKpC7YPtVNAzydZ0+7auXZgjCRpBIY5vXMu8Nl2d+Vi4K+q6gtJvgZsS3Id8DRwDUBV7UqyDXgMOATcUFWH22ddD9wBnM7UBdx5u4grSTrWjKFfVd8CLp6m/hxw+XHGbAY2T1OfBC6cfZuSpLngL3IlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUkaFDP8miJP+a5J62fVaS+5I80ZZnDux7U5I9SR5PcsVA/ZIkO9t7tyTJ3E5HknQisznSvxHYPbC9CdhRVSuBHW2bJKuA9cAFwFrgtiSL2pjbgY3AyvZae0rdS5JmZajQT7IMeDvw8YHyOmBrW98KXD1Qv6uqXqiqJ4E9wKVJlgBnVNWDVVXAnQNjJEkjMOyR/keBPwB+OFA7t6r2A7TlOa2+FHhmYL+9rba0rR9dP0aSjUkmk0wePHhwyBYlSTOZMfSTXAUcqKqHh/zM6c7T1wnqxxartlTV6qpaPTExMeTXSpJmsniIfd4E/GqSK4GXAWck+Qvg2SRLqmp/O3VzoO2/FzhvYPwyYF+rL5umLkkakRmP9KvqpqpaVlXLmbpAe39V/RawHdjQdtsA3N3WtwPrk5yWZAVTF2wfaqeAnk+ypt21c+3AGEnSCAxzpH88NwPbklwHPA1cA1BVu5JsAx4DDgE3VNXhNuZ64A7gdODe9pIkjcisQr+qHgAeaOvPAZcfZ7/NwOZp6pPAhbNtUpI0N/xFriR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUkRlDP8nLkjyU5OtJdiX5o1Y/K8l9SZ5oyzMHxtyUZE+Sx5NcMVC/JMnO9t4tSTI/05IkTWeYI/0XgLdU1cXA64C1SdYAm4AdVbUS2NG2SbIKWA9cAKwFbkuyqH3W7cBGYGV7rZ27qUiSZjJj6NeU/2qbL2mvAtYBW1t9K3B1W18H3FVVL1TVk8Ae4NIkS4AzqurBqirgzoExkqQRGOqcfpJFSR4BDgD3VdVXgXOraj9AW57Tdl8KPDMwfG+rLW3rR9en+76NSSaTTB48eHAW05EknchQoV9Vh6vqdcAypo7aLzzB7tOdp68T1Kf7vi1VtbqqVk9MTAzToiRpCLO6e6eq/hN4gKlz8c+2Uza05YG2217gvIFhy4B9rb5smrokaUSGuXtnIsmr2vrpwFuBbwLbgQ1ttw3A3W19O7A+yWlJVjB1wfahdgro+SRr2l071w6MkSSNwOIh9lkCbG134PwYsK2q7knyILAtyXXA08A1AFW1K8k24DHgEHBDVR1un3U9cAdwOnBve0mSRmTG0K+qR4HXT1N/Drj8OGM2A5unqU8CJ7oeIEmaR/4iV5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR2ZMfSTnJfkH5LsTrIryY2tflaS+5I80ZZnDoy5KcmeJI8nuWKgfkmSne29W5JkfqYlSZrOMEf6h4D3V9X5wBrghiSrgE3AjqpaCexo27T31gMXAGuB25Isap91O7ARWNlea+dwLpKkGcwY+lW1v6r+pa0/D+wGlgLrgK1tt63A1W19HXBXVb1QVU8Ce4BLkywBzqiqB6uqgDsHxkiSRmBW5/STLAdeD3wVOLeq9sPUPwzAOW23pcAzA8P2ttrStn50fbrv2ZhkMsnkwYMHZ9OiJOkEhg79JC8HPg38flV9/0S7TlOrE9SPLVZtqarVVbV6YmJi2BYlSTMYKvSTvISpwP/LqvpMKz/bTtnQlgdafS9w3sDwZcC+Vl82TV2SNCLD3L0T4M+B3VX1pwNvbQc2tPUNwN0D9fVJTkuygqkLtg+1U0DPJ1nTPvPagTGSpBFYPMQ+bwLeDexM8kirfRC4GdiW5DrgaeAagKralWQb8BhTd/7cUFWH27jrgTuA04F720uSNCIzhn5V/TPTn48HuPw4YzYDm6epTwIXzqZBSdLc8Re5ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0JakjwzxPX5I0YPmmzx1Te+rmt4+hk9nzSF+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIz96RpHlw0daLjtjeuWHnmDo50oyhn+QTwFXAgaq6sNXOAj4FLAeeAn69qr7b3rsJuA44DPxeVX2x1S8B7gBOBz4P3FhVNbfTkaQx+fArj9xe8erx9DGDYU7v3AGsPaq2CdhRVSuBHW2bJKuA9cAFbcxtSRa1MbcDG4GV7XX0Z0rSi9bu155/xGtcZgz9qvoS8J2jyuuArW19K3D1QP2uqnqhqp4E9gCXJlkCnFFVD7aj+zsHxkiSRuRkL+SeW1X7AdrynFZfCjwzsN/eVlva1o+uTyvJxiSTSSYPHjx4ki1Kko4213fvZJpanaA+raraUlWrq2r1xMTEnDUnSb072bt3nk2ypKr2t1M3B1p9L3DewH7LgH2tvmyauiQJ+Mg7rzpi+/2fumdevudkj/S3Axva+gbg7oH6+iSnJVnB1AXbh9opoOeTrEkS4NqBMZKkERnmls1PApcBZyfZC3wIuBnYluQ64GngGoCq2pVkG/AYcAi4oaoOt4+6nv+/ZfPe9pIkjdCMoV9V7zrOW5cfZ//NwOZp6pPAhbPqTpI0p3wMgyR1xMcwSNIY3Pre+8fyvR7pS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6sjIQz/J2iSPJ9mTZNOov1+SejbS0E+yCLgVeBuwCnhXklWj7EGSejbqI/1LgT1V9a2q+h/gLmDdiHuQpG6lqkb3Zck7gLVV9dtt+93AG6vqfUfttxHY2DZfAzw+i685G/j2HLS70DjvvjjvvpzMvH+qqiaOLi6em36Glmlqx/yrU1VbgC0n9QXJZFWtPpmxC5nz7ovz7stcznvUp3f2AucNbC8D9o24B0nq1qhD/2vAyiQrkrwUWA9sH3EPktStkZ7eqapDSd4HfBFYBHyiqnbN8dec1GmhFwHn3Rfn3Zc5m/dIL+RKksbLX+RKUkcMfUnqyIIM/Zke5ZApt7T3H03yhnH0OR+GmPtvtjk/muTLSS4eR59zbdjHdyT5+SSH229CFrxh5p3ksiSPJNmV5B9H3eN8GOLv+SuT/G2Sr7d5v2ccfc61JJ9IciDJN47z/qlnW1UtqBdTF4D/Hfhp4KXA14FVR+1zJXAvU78LWAN8ddx9j3DuvwCc2dbf9mKY+zDzHtjvfuDzwDvG3feI/rxfBTwGvLptnzPuvkc07w8Cf9LWJ4DvAC8dd+9zMPdfBt4AfOM4759yti3EI/1hHuWwDrizpnwFeFWSJaNudB7MOPeq+nJVfbdtfoWp30IsdMM+vuN3gU8DB0bZ3DwaZt6/AXymqp4GqKoXw9yHmXcBr0gS4OVMhf6h0bY596rqS0zN5XhOOdsWYugvBZ4Z2N7barPdZyGa7byuY+qoYKGbcd5JlgK/BnxshH3Nt2H+vH8WODPJA0keTnLtyLqbP8PM+8+A85n6cedO4Maq+uFo2hurU862UT+GYS4M8yiHoR73sAANPa8kb2Yq9H9xXjsajWHm/VHgA1V1eOrg70VhmHkvBi4BLgdOBx5M8pWq+rf5bm4eDTPvK4BHgLcAPwPcl+Sfqur789zbuJ1yti3E0B/mUQ4v1sc9DDWvJD8HfBx4W1U9N6Le5tMw814N3NUC/2zgyiSHqupvRtLh/Bj27/q3q+oHwA+SfAm4GFjIoT/MvN8D3FxTJ7r3JHkSeC3w0GhaHJtTzraFeHpnmEc5bAeubVe61wDfq6r9o250Hsw49ySvBj4DvHuBH+0NmnHeVbWiqpZX1XLgr4HfWeCBD8P9Xb8b+KUki5P8OPBGYPeI+5xrw8z7aab+d0OSc5l6Gu+3RtrleJxyti24I/06zqMckry3vf8xpu7euBLYA/w3U0cFC96Qc/9D4CeB29pR76Fa4E8lHHLeLzrDzLuqdif5AvAo8EPg41U17e1+C8WQf95/DNyRZCdTpzw+UFUL/pHLST4JXAacnWQv8CHgJTB32eZjGCSpIwvx9I4k6SQZ+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakj/wuKLqb14QHCrAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_hat)\n",
    "_ = plt.axvline(x=0.5, color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "94.88882025941939"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import  accuracy_score as score\n",
    "y_pred_classes = y_hat.round()\n",
    "print(y_valid[0], y_hat[0], y_pred_classes[0])\n",
    "acc = score(y_valid, y_pred_classes) * 100\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RNljSx3v8Jmh"
   },
   "outputs": [],
   "source": [
    "pct_auc = roc_auc_score(y_valid, y_hat)*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SO_N9lJG8Jmj",
    "outputId": "20968a52-8ca0-44d3-abf6-a1382bba5cde"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'96.87'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{:0.2f}\".format(pct_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vU3hnA298Jmo"
   },
   "outputs": [],
   "source": [
    "float_y_hat = []\n",
    "for y in y_hat:\n",
    "    float_y_hat.append(y[0:6].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zrTf-aQl8Jmr"
   },
   "outputs": [],
   "source": [
    "ydf = pd.DataFrame(list(zip(float_y_hat, y_valid)), columns=['y_hat', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zAyKh_mX8Jmu",
    "outputId": "611224b8-f980-47df-d596-9aa3a5811009"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_hat</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            y_hat                               y\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
       "1  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n",
       "2  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
       "3  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "4  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
       "5  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
       "6  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
       "7  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
       "8  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "9  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ydf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_dense_2</th>\n",
       "      <th>n_dense_1</th>\n",
       "      <th>n_conv_3</th>\n",
       "      <th>n_conv_2</th>\n",
       "      <th>n_conv_1</th>\n",
       "      <th>maxpooling_pool_size</th>\n",
       "      <th>k_conv_3</th>\n",
       "      <th>k_conv_2</th>\n",
       "      <th>k_conv_1</th>\n",
       "      <th>dropout_2</th>\n",
       "      <th>dropout_1</th>\n",
       "      <th>avepooling_pool_size</th>\n",
       "      <th>activation</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>768</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.935791</td>\n",
       "      <td>0.003756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>512</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.933646</td>\n",
       "      <td>0.002797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>768</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.933045</td>\n",
       "      <td>0.002490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>768</td>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.932479</td>\n",
       "      <td>0.003557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.911716</td>\n",
       "      <td>0.006474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>768</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.909503</td>\n",
       "      <td>0.002990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.22</td>\n",
       "      <td>4</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.903497</td>\n",
       "      <td>0.002703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>768</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.901198</td>\n",
       "      <td>0.015270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>768</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.899602</td>\n",
       "      <td>0.007049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>768</td>\n",
       "      <td>768</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.898778</td>\n",
       "      <td>0.003801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.22</td>\n",
       "      <td>4</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.893322</td>\n",
       "      <td>0.024338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.22</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>0.892429</td>\n",
       "      <td>0.008359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.22</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>0.890834</td>\n",
       "      <td>0.004445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>0.889650</td>\n",
       "      <td>0.010236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>768</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>0.880418</td>\n",
       "      <td>0.006444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.22</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>0.874807</td>\n",
       "      <td>0.006995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>768</td>\n",
       "      <td>512</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.30</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;tensorflow.python.keras.layers.advanced_activ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>768</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.30</td>\n",
       "      <td>4</td>\n",
       "      <td>elu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.22</td>\n",
       "      <td>4</td>\n",
       "      <td>elu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>768</td>\n",
       "      <td>512</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.30</td>\n",
       "      <td>4</td>\n",
       "      <td>relu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_dense_2  n_dense_1  n_conv_3  n_conv_2  n_conv_1  maxpooling_pool_size  \\\n",
       "6         512        768       768       512       512                     2   \n",
       "17        512        768       512       768       512                     4   \n",
       "14        512        768       768       512       512                     4   \n",
       "2         512        768       768       512       768                     4   \n",
       "12        512        512       512       512       512                     2   \n",
       "11        512        768       512       768       768                     4   \n",
       "4         512        512       512       512       768                     2   \n",
       "7         512        768       768       512       512                     2   \n",
       "0         512        768       768       512       512                     2   \n",
       "1         512        512       768       768       768                     4   \n",
       "9         512        512       768       512       768                     4   \n",
       "16        512        512       512       768       512                     2   \n",
       "19        512        512       512       512       512                     4   \n",
       "18        512        512       512       512       768                     2   \n",
       "10        512        768       768       512       512                     4   \n",
       "13        512        768       512       768       512                     2   \n",
       "3         512        512       768       768       512                     4   \n",
       "5         512        768       512       768       768                     4   \n",
       "8         512        768       512       512       768                     4   \n",
       "15        512        512       512       768       512                     4   \n",
       "\n",
       "    k_conv_3  k_conv_2  k_conv_1  dropout_2  dropout_1  avepooling_pool_size  \\\n",
       "6          7         7         3       0.25       0.22                     2   \n",
       "17         7         7         7       0.25       0.22                     2   \n",
       "14         3         7         7       0.25       0.30                     2   \n",
       "2          3         7         7       0.25       0.22                     2   \n",
       "12         3         3         7       0.25       0.30                     2   \n",
       "11         7         3         7       0.25       0.22                     2   \n",
       "4          7         3         7       0.25       0.22                     4   \n",
       "7          7         7         7       0.25       0.30                     2   \n",
       "0          7         3         7       0.25       0.30                     2   \n",
       "1          7         7         3       0.25       0.22                     2   \n",
       "9          3         7         7       0.25       0.22                     4   \n",
       "16         7         3         3       0.25       0.22                     4   \n",
       "19         3         3         7       0.25       0.22                     4   \n",
       "18         3         3         7       0.25       0.30                     2   \n",
       "10         7         7         3       0.25       0.22                     2   \n",
       "13         7         3         7       0.25       0.22                     4   \n",
       "3          7         7         3       0.25       0.30                     4   \n",
       "5          7         3         3       0.25       0.30                     4   \n",
       "8          7         3         7       0.25       0.22                     4   \n",
       "15         7         3         3       0.25       0.30                     4   \n",
       "\n",
       "                                           activation      mean       std  \n",
       "6                                                relu  0.935791  0.003756  \n",
       "17                                               relu  0.933646  0.002797  \n",
       "14                                               relu  0.933045  0.002490  \n",
       "2                                                relu  0.932479  0.003557  \n",
       "12                                                elu  0.911716  0.006474  \n",
       "11                                                elu  0.909503  0.002990  \n",
       "4                                                 elu  0.903497  0.002703  \n",
       "7                                                 elu  0.901198  0.015270  \n",
       "0                                                 elu  0.899602  0.007049  \n",
       "1                                                 elu  0.898778  0.003801  \n",
       "9                                                 elu  0.893322  0.024338  \n",
       "16  <tensorflow.python.keras.layers.advanced_activ...  0.892429  0.008359  \n",
       "19  <tensorflow.python.keras.layers.advanced_activ...  0.890834  0.004445  \n",
       "18  <tensorflow.python.keras.layers.advanced_activ...  0.889650  0.010236  \n",
       "10  <tensorflow.python.keras.layers.advanced_activ...  0.880418  0.006444  \n",
       "13  <tensorflow.python.keras.layers.advanced_activ...  0.874807  0.006995  \n",
       "3   <tensorflow.python.keras.layers.advanced_activ...       NaN       NaN  \n",
       "5                                                 elu       NaN       NaN  \n",
       "8                                                 elu       NaN       NaN  \n",
       "15                                               relu       NaN       NaN  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(rscv_results.cv_results_['params'])\n",
    "results_df['mean'] = rscv_results.cv_results_['mean_test_score']\n",
    "results_df['std'] = rscv_results.cv_results_['std_test_score']\n",
    "results_df.sort_values('mean', ascending=False, ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating best model from the best param and train it for 60 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('n_dense_2', 512),\n",
       " ('n_dense_1', 768),\n",
       " ('n_conv_3', 768),\n",
       " ('n_conv_2', 512),\n",
       " ('n_conv_1', 512),\n",
       " ('maxpooling_pool_size', 2),\n",
       " ('k_conv_3', 7),\n",
       " ('k_conv_2', 7),\n",
       " ('k_conv_1', 3),\n",
       " ('dropout_2', 0.25),\n",
       " ('dropout_1', 0.22),\n",
       " ('avepooling_pool_size', 2),\n",
       " ('activation', 'relu')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(best_param.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_63\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_189 (Conv1D)          (None, 88, 512)           5120      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 44, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_190 (Conv1D)          (None, 38, 512)           1835520   \n",
      "_________________________________________________________________\n",
      "average_pooling1d_63 (Averag (None, 19, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_191 (Conv1D)          (None, 13, 768)           2753280   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_51  (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_153 (Dense)            (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "dropout_102 (Dropout)        (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_154 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_103 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_155 (Dense)            (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 5,581,318\n",
      "Trainable params: 5,581,318\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_dense_2= 512\n",
    "n_dense_1= 768\n",
    "n_conv_3= 768\n",
    "n_conv_2= 512\n",
    "n_conv_1= 512\n",
    "maxpooling_pool_size= 2\n",
    "k_conv_3= 7\n",
    "k_conv_2= 7\n",
    "k_conv_1= 3\n",
    "dropout_2= 0.25\n",
    "dropout_1= 0.22\n",
    "avepooling_pool_size= 2\n",
    "activation= 'relu'\n",
    "model = Sequential()\n",
    "model.add(Conv1D(n_conv_1, k_conv_1, activation=activation, input_shape=input_shape[1:]))\n",
    "model.add(MaxPool1D(pool_size = maxpooling_pool_size))\n",
    "model.add(Conv1D(n_conv_2, k_conv_2, activation=activation))\n",
    "model.add(AveragePooling1D(pool_size = avepooling_pool_size))\n",
    "model.add(Conv1D(n_conv_3, k_conv_3, activation=activation))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(n_dense_1, activation=activation))\n",
    "model.add(Dropout(dropout_1))\n",
    "model.add(Dense(n_dense_2, activation=activation))\n",
    "model.add(Dropout(dropout_2))\n",
    "model.add(Dense(n_class, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir= 'model_ouput/tune-sklearn-1'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "modelcheckpoint = ModelCheckpoint(filepath=output_dir+\n",
    "                                  \"/weights.{epoch:02d}.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "  2/456 [..............................] - ETA: 25s - loss: 1.8039 - accuracy: 0.1367WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0219s vs `on_train_batch_end` time: 0.0449s). Check your callbacks.\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.8921 - accuracy: 0.6301 - val_loss: 0.8564 - val_accuracy: 0.6901\n",
      "Epoch 2/60\n",
      "456/456 [==============================] - 25s 56ms/step - loss: 0.4614 - accuracy: 0.8204 - val_loss: 0.6183 - val_accuracy: 0.7708\n",
      "Epoch 3/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.3622 - accuracy: 0.8610 - val_loss: 0.3411 - val_accuracy: 0.8718\n",
      "Epoch 4/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.3108 - accuracy: 0.8795 - val_loss: 0.3053 - val_accuracy: 0.8834\n",
      "Epoch 5/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.2881 - accuracy: 0.8901 - val_loss: 0.2890 - val_accuracy: 0.8919\n",
      "Epoch 6/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.2614 - accuracy: 0.9016 - val_loss: 0.2637 - val_accuracy: 0.8999\n",
      "Epoch 7/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.2407 - accuracy: 0.9100 - val_loss: 0.2702 - val_accuracy: 0.9009\n",
      "Epoch 8/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.2223 - accuracy: 0.9168 - val_loss: 0.2389 - val_accuracy: 0.9138\n",
      "Epoch 9/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.2135 - accuracy: 0.9201 - val_loss: 0.2178 - val_accuracy: 0.9265\n",
      "Epoch 10/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.1958 - accuracy: 0.9271 - val_loss: 0.2348 - val_accuracy: 0.9158\n",
      "Epoch 11/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.1854 - accuracy: 0.9305 - val_loss: 0.2106 - val_accuracy: 0.9260\n",
      "Epoch 12/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.1771 - accuracy: 0.9331 - val_loss: 0.2504 - val_accuracy: 0.9046\n",
      "Epoch 13/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.1701 - accuracy: 0.9352 - val_loss: 0.2291 - val_accuracy: 0.9236\n",
      "Epoch 14/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.1633 - accuracy: 0.9385 - val_loss: 0.2194 - val_accuracy: 0.9228\n",
      "Epoch 15/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.1555 - accuracy: 0.9412 - val_loss: 0.2166 - val_accuracy: 0.9254\n",
      "Epoch 16/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.1516 - accuracy: 0.9430 - val_loss: 0.2086 - val_accuracy: 0.9302\n",
      "Epoch 17/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.1409 - accuracy: 0.9466 - val_loss: 0.2083 - val_accuracy: 0.9351\n",
      "Epoch 18/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.1401 - accuracy: 0.9474 - val_loss: 0.2107 - val_accuracy: 0.9333\n",
      "Epoch 19/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.1341 - accuracy: 0.9504 - val_loss: 0.2066 - val_accuracy: 0.9368\n",
      "Epoch 20/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.1322 - accuracy: 0.9504 - val_loss: 0.2127 - val_accuracy: 0.9385\n",
      "Epoch 21/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.1285 - accuracy: 0.9519 - val_loss: 0.2360 - val_accuracy: 0.9362\n",
      "Epoch 22/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.1278 - accuracy: 0.9528 - val_loss: 0.2212 - val_accuracy: 0.9355\n",
      "Epoch 23/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.1218 - accuracy: 0.9545 - val_loss: 0.2123 - val_accuracy: 0.9385\n",
      "Epoch 24/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.1186 - accuracy: 0.9561 - val_loss: 0.2284 - val_accuracy: 0.9307\n",
      "Epoch 25/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.1159 - accuracy: 0.9582 - val_loss: 0.2133 - val_accuracy: 0.9385\n",
      "Epoch 26/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.1117 - accuracy: 0.9577 - val_loss: 0.2364 - val_accuracy: 0.9327\n",
      "Epoch 27/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.1162 - accuracy: 0.9567 - val_loss: 0.2084 - val_accuracy: 0.9410\n",
      "Epoch 28/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.1074 - accuracy: 0.9599 - val_loss: 0.2078 - val_accuracy: 0.9430\n",
      "Epoch 29/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.1024 - accuracy: 0.9616 - val_loss: 0.2119 - val_accuracy: 0.9444\n",
      "Epoch 30/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.1095 - accuracy: 0.9614 - val_loss: 0.2018 - val_accuracy: 0.9441\n",
      "Epoch 31/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.1025 - accuracy: 0.9610 - val_loss: 0.2078 - val_accuracy: 0.9452\n",
      "Epoch 32/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.0998 - accuracy: 0.9631 - val_loss: 0.2262 - val_accuracy: 0.9439\n",
      "Epoch 33/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.0974 - accuracy: 0.9635 - val_loss: 0.2225 - val_accuracy: 0.9422\n",
      "Epoch 34/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.0909 - accuracy: 0.9654 - val_loss: 0.2330 - val_accuracy: 0.9439\n",
      "Epoch 35/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.0974 - accuracy: 0.9645 - val_loss: 0.2398 - val_accuracy: 0.9453\n",
      "Epoch 36/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.0908 - accuracy: 0.9662 - val_loss: 0.2711 - val_accuracy: 0.9387\n",
      "Epoch 37/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.0938 - accuracy: 0.9656 - val_loss: 0.2198 - val_accuracy: 0.9478\n",
      "Epoch 38/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.0892 - accuracy: 0.9671 - val_loss: 0.2341 - val_accuracy: 0.9438\n",
      "Epoch 39/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.0886 - accuracy: 0.9679 - val_loss: 0.2417 - val_accuracy: 0.9419\n",
      "Epoch 40/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.0933 - accuracy: 0.9652 - val_loss: 0.2439 - val_accuracy: 0.9427\n",
      "Epoch 41/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.0935 - accuracy: 0.9663 - val_loss: 0.2303 - val_accuracy: 0.9460\n",
      "Epoch 42/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.0843 - accuracy: 0.9693 - val_loss: 0.2576 - val_accuracy: 0.9345\n",
      "Epoch 43/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.0878 - accuracy: 0.9680 - val_loss: 0.2524 - val_accuracy: 0.9382\n",
      "Epoch 44/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.0856 - accuracy: 0.9687 - val_loss: 0.2407 - val_accuracy: 0.9433\n",
      "Epoch 45/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.0863 - accuracy: 0.9688 - val_loss: 0.2509 - val_accuracy: 0.9430\n",
      "Epoch 46/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.0825 - accuracy: 0.9700 - val_loss: 0.2921 - val_accuracy: 0.9449\n",
      "Epoch 47/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.0806 - accuracy: 0.9694 - val_loss: 0.2665 - val_accuracy: 0.9387\n",
      "Epoch 48/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.0830 - accuracy: 0.9689 - val_loss: 0.2357 - val_accuracy: 0.9444\n",
      "Epoch 49/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.0803 - accuracy: 0.9709 - val_loss: 0.2689 - val_accuracy: 0.9422\n",
      "Epoch 50/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.0792 - accuracy: 0.9712 - val_loss: 0.2341 - val_accuracy: 0.9477\n",
      "Epoch 51/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.0744 - accuracy: 0.9728 - val_loss: 0.2383 - val_accuracy: 0.9489\n",
      "Epoch 52/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.0792 - accuracy: 0.9708 - val_loss: 0.2529 - val_accuracy: 0.9441\n",
      "Epoch 53/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.0765 - accuracy: 0.9727 - val_loss: 0.2434 - val_accuracy: 0.9456\n",
      "Epoch 54/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.0763 - accuracy: 0.9726 - val_loss: 0.2461 - val_accuracy: 0.9467\n",
      "Epoch 55/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.0738 - accuracy: 0.9734 - val_loss: 0.2447 - val_accuracy: 0.9470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.0756 - accuracy: 0.9731 - val_loss: 0.2689 - val_accuracy: 0.9424\n",
      "Epoch 57/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.0804 - accuracy: 0.9708 - val_loss: 0.2487 - val_accuracy: 0.9469\n",
      "Epoch 58/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.0734 - accuracy: 0.9728 - val_loss: 0.2505 - val_accuracy: 0.9455\n",
      "Epoch 59/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.0704 - accuracy: 0.9745 - val_loss: 0.2513 - val_accuracy: 0.9467\n",
      "Epoch 60/60\n",
      "456/456 [==============================] - 25s 55ms/step - loss: 0.0755 - accuracy: 0.9728 - val_loss: 0.2325 - val_accuracy: 0.9480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16c72bc1208>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, \n",
    "         batch_size=batch_size, epochs=epochs, verbose=1, \n",
    "         validation_data=(X_valid, y_valid), \n",
    "         callbacks=[modelcheckpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z8t0nVCw8JmP"
   },
   "outputs": [],
   "source": [
    "# model.load_weights(output_dir+\"/weights.49.hdf5\") # 94.22 val accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZwGk5dR8JmS"
   },
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EPc5_h6K8JmW",
    "outputId": "cad99da9-9f89-437f-854b-a315616ed50f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6476"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E_GNq-R_8JmZ",
    "outputId": "0fc16efe-7739-4394-f1a3-46da92e79584"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.9517664e-05, 1.2823284e-11, 1.5929148e-08, 5.1007139e-11,\n",
       "       4.3575806e-08, 9.9993050e-01], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFqRQ5XB8Jmc",
    "outputId": "a957fbed-92f8-4bdd-d24b-5df3361a8bac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FTWf6Cf-8Jme",
    "outputId": "1256b170-33dc-4171-f2ff-7a3d7ddd74e3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_hat)\n",
    "_ = plt.axvline(x=0.5, color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RNljSx3v8Jmh"
   },
   "outputs": [],
   "source": [
    "pct_auc = roc_auc_score(y_valid, y_hat)*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SO_N9lJG8Jmj",
    "outputId": "20968a52-8ca0-44d3-abf6-a1382bba5cde"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'99.60'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{:0.2f}\".format(pct_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vU3hnA298Jmo"
   },
   "outputs": [],
   "source": [
    "float_y_hat = []\n",
    "for y in y_hat:\n",
    "    float_y_hat.append(y[0:6].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zrTf-aQl8Jmr"
   },
   "outputs": [],
   "source": [
    "ydf = pd.DataFrame(list(zip(float_y_hat, y_valid)), columns=['y_hat', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zAyKh_mX8Jmu",
    "outputId": "611224b8-f980-47df-d596-9aa3a5811009"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_hat</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.007, 0.0, 0.0, 0.0, 0.0, 0.993]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.001, 0.987, 0.0, 0.0, 0.012, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  y_hat                               y\n",
       "0        [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
       "1        [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n",
       "2        [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
       "3        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "4        [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
       "5    [0.007, 0.0, 0.0, 0.0, 0.0, 0.993]  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
       "6        [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
       "7        [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
       "8        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "9  [0.001, 0.987, 0.0, 0.0, 0.012, 0.0]  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ydf.head(10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "dense_sentiment_classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
